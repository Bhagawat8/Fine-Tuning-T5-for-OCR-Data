{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-21T12:12:01.523197Z",
     "iopub.status.busy": "2024-10-21T12:12:01.522798Z",
     "iopub.status.idle": "2024-10-21T12:12:01.892555Z",
     "shell.execute_reply": "2024-10-21T12:12:01.891597Z",
     "shell.execute_reply.started": "2024-10-21T12:12:01.523159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bhagawat-training-data-ploy-features/training_data_ploy_features.csv\n",
      "/kaggle/input/final/transformers/default/1/config.json\n",
      "/kaggle/input/final/transformers/default/1/spiece.model\n",
      "/kaggle/input/final/transformers/default/1/tokenizer_config.json\n",
      "/kaggle/input/final/transformers/default/1/model.safetensors\n",
      "/kaggle/input/final/transformers/default/1/special_tokens_map.json\n",
      "/kaggle/input/final/transformers/default/1/added_tokens.json\n",
      "/kaggle/input/final/transformers/default/1/generation_config.json\n",
      "/kaggle/input/prediction/data_for_prediction_ploy_features.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T08:48:45.711711Z",
     "iopub.status.busy": "2024-10-21T08:48:45.711285Z",
     "iopub.status.idle": "2024-10-21T08:48:45.716092Z",
     "shell.execute_reply": "2024-10-21T08:48:45.715151Z",
     "shell.execute_reply.started": "2024-10-21T08:48:45.711673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here I used google-t5/t5-base model from hugging face\n",
    "# training for 2 epochs on data training_data_ploy_features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T17:36:32.190456Z",
     "iopub.status.busy": "2024-10-20T17:36:32.189943Z",
     "iopub.status.idle": "2024-10-20T22:08:52.690379Z",
     "shell.execute_reply": "2024-10-20T22:08:52.689341Z",
     "shell.execute_reply.started": "2024-10-20T17:36:32.190417Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "# Define a dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Construct input text using all columns except 'field'\n",
    "        input_columns = [col for col in self.data.columns if col != 'field']\n",
    "        input_text = ' '.join([f\"{col}: {row[col]}\" for col in input_columns])  # Example input construction\n",
    "\n",
    "        # Target text is the 'field' column\n",
    "        target_text = row['field']  # Output is the 'field' column\n",
    "\n",
    "        # Tokenize inputs and targets\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        targets = self.tokenizer.encode_plus(\n",
    "            target_text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "        labels = targets[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Load your dataset using pandas\n",
    "data = pd.read_csv(\"/kaggle/input/training-data-ploy-infrrd/training_data_ploy_features.csv\")\n",
    "\n",
    "# Load the tokenizer and model for conditional generation\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Create dataset and dataloader with reduced batch size\n",
    "train_dataset = CustomDataset(tokenizer, data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Reduced batch size\n",
    "\n",
    "# Optimizer and training settings\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)  # Learning rate adjusted\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Mixed precision training setup\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Gradient accumulation settings\n",
    "gradient_accumulation_steps = 2  # Accumulate gradients over 2 batches\n",
    "accumulated_loss = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Ignore padding tokens in the labels\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        with autocast():  # Mixed precision context\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / gradient_accumulation_steps  # Divide by accumulation steps\n",
    "            accumulated_loss += loss.item()\n",
    "        \n",
    "        # Check for NaN loss and skip the batch if encountered\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Skipping batch {batch_idx + 1} due to NaN loss\")\n",
    "            continue\n",
    "\n",
    "        # Scale the loss and call backward\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscale the gradients and apply gradient clipping after accumulation\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # Clear the CUDA cache periodically to free up memory\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Batch {batch_idx + 1}/{num_batches}, Loss: {accumulated_loss}\")\n",
    "        accumulated_loss = 0\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1} Average Loss: {avg_loss}\")\n",
    "\n",
    "    # Save the model and optimizer state after each epoch\n",
    "    model.save_pretrained(f\"trained_t5_model_epoch_{epoch + 1}\")\n",
    "    tokenizer.save_pretrained(f\"trained_t5_model_epoch_{epoch + 1}\")\n",
    "    torch.save(optimizer.state_dict(), f\"optimizer_state_epoch_{epoch + 1}.pth\")\n",
    "    print(f\"Model and optimizer state saved after epoch {epoch + 1}\")\n",
    "\n",
    "# Final save if all epochs complete\n",
    "model.save_pretrained(\"trained_t5_model_final\")\n",
    "tokenizer.save_pretrained(\"trained_t5_model_final\")\n",
    "torch.save(optimizer.state_dict(), \"optimizer_state_final.pth\")\n",
    "print(\"Final model and optimizer state saved.\")\n",
    "end = time.time()\n",
    "print(f\"Training time: {end - st} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result of this is present in eval_metrics_for_2epochs_ploy_features_model.csv this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is prediction code for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "st = time.time()\n",
    "# Load the trained model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/kaggle/input/model_v4/transformers/default/1\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/kaggle/input/model_v4/transformers/default/1\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(\"/kaggle/input/testing-data-ploy/testing_data_ploy_features.csv\")\n",
    "\n",
    "# Prepare a function for prediction\n",
    "def predict_field(row, tokenizer, model, max_len=128):\n",
    "    # Construct input text using all columns except 'field'\n",
    "    input_columns = [col for col in test_data.columns if col != 'field']\n",
    "    input_text = ' '.join([f\"{col}: {row[col]}\" for col in input_columns])\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_len)\n",
    "    \n",
    "    # Decode the generated prediction\n",
    "    predicted_field = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return predicted_field\n",
    "\n",
    "# Apply the function to each row of the test data and create a new column for the predictions\n",
    "test_data['predicted_field'] = test_data.apply(lambda row: predict_field(row, tokenizer, model), axis=1)\n",
    "\n",
    "# Save the result to a new CSV file (optional)\n",
    "test_data.to_csv(\"test_data_with_ploy_predictions.csv\", index=False)\n",
    "\n",
    "end = time.time()\n",
    "print(end-st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T04:14:42.076354Z",
     "iopub.status.busy": "2024-10-21T04:14:42.075631Z",
     "iopub.status.idle": "2024-10-21T04:14:42.579606Z",
     "shell.execute_reply": "2024-10-21T04:14:42.578791Z",
     "shell.execute_reply.started": "2024-10-21T04:14:42.076308Z"
    }
   },
   "outputs": [],
   "source": [
    "df11 = pd.read_csv(\"/kaggle/working/test_data_with_ploy_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T04:14:42.581059Z",
     "iopub.status.busy": "2024-10-21T04:14:42.580742Z",
     "iopub.status.idle": "2024-10-21T04:14:42.61014Z",
     "shell.execute_reply": "2024-10-21T04:14:42.609148Z",
     "shell.execute_reply.started": "2024-10-21T04:14:42.581027Z"
    }
   },
   "outputs": [],
   "source": [
    "df11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that I again trained my already train model for 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T08:50:42.852935Z",
     "iopub.status.busy": "2024-10-21T08:50:42.852264Z",
     "iopub.status.idle": "2024-10-21T08:50:42.857515Z",
     "shell.execute_reply": "2024-10-21T08:50:42.856606Z",
     "shell.execute_reply.started": "2024-10-21T08:50:42.852887Z"
    }
   },
   "outputs": [],
   "source": [
    "# training for 3rd epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T08:54:12.378132Z",
     "iopub.status.busy": "2024-10-21T08:54:12.377807Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_30/3588094712.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_30/3588094712.py:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Batch 1/14851, Loss: 0.004114020615816116\n",
      "Batch 2/14851, Loss: 0.0600995272397995\n",
      "Batch 3/14851, Loss: 0.0009119971073232591\n",
      "Batch 4/14851, Loss: 0.001110713928937912\n",
      "Batch 5/14851, Loss: 0.005726331379264593\n",
      "Batch 6/14851, Loss: 0.007934951223433018\n",
      "Batch 7/14851, Loss: 0.001703263376839459\n",
      "Batch 8/14851, Loss: 0.0009576889569871128\n",
      "Batch 9/14851, Loss: 0.003997235093265772\n",
      "Batch 10/14851, Loss: 5.385403710533865e-05\n",
      "Batch 11/14851, Loss: 0.0013376415008679032\n",
      "Batch 12/14851, Loss: 0.006538925226777792\n",
      "Batch 13/14851, Loss: 0.03696290776133537\n",
      "Batch 14/14851, Loss: 0.007530923932790756\n",
      "Batch 15/14851, Loss: 0.008077082224190235\n",
      "Batch 16/14851, Loss: 0.0008829523576423526\n",
      "Batch 17/14851, Loss: 0.03663649782538414\n",
      "Batch 18/14851, Loss: 0.013496718369424343\n",
      "Batch 19/14851, Loss: 0.025220854207873344\n",
      "Batch 20/14851, Loss: 0.011424766853451729\n",
      "Batch 21/14851, Loss: 0.0006425242172554135\n",
      "Batch 22/14851, Loss: 0.003217418910935521\n",
      "Batch 23/14851, Loss: 0.002208355814218521\n",
      "Batch 24/14851, Loss: 0.0022615790367126465\n",
      "Batch 25/14851, Loss: 0.004101825412362814\n",
      "Batch 26/14851, Loss: 0.033535826951265335\n",
      "Batch 27/14851, Loss: 0.018348271027207375\n",
      "Batch 28/14851, Loss: 0.0011691153049468994\n",
      "Batch 29/14851, Loss: 0.00022331252694129944\n",
      "Batch 30/14851, Loss: 0.003925895318388939\n",
      "Batch 31/14851, Loss: 0.0010642074048519135\n",
      "Batch 32/14851, Loss: 0.018378986045718193\n",
      "Batch 33/14851, Loss: 0.002112246584147215\n",
      "Batch 34/14851, Loss: 0.002184482989832759\n",
      "Batch 35/14851, Loss: 0.052606552839279175\n",
      "Batch 36/14851, Loss: 0.0007028206018730998\n",
      "Batch 37/14851, Loss: 0.011589289642870426\n",
      "Batch 38/14851, Loss: 0.0005441729445010424\n",
      "Batch 39/14851, Loss: 0.023731518536806107\n",
      "Batch 40/14851, Loss: 0.00010277206456521526\n",
      "Batch 41/14851, Loss: 0.0005432317848317325\n",
      "Batch 42/14851, Loss: 0.0001498026103945449\n",
      "Batch 43/14851, Loss: 0.00016599521040916443\n",
      "Batch 44/14851, Loss: 0.02485489659011364\n",
      "Batch 45/14851, Loss: 0.0007931378786452115\n",
      "Batch 46/14851, Loss: 0.00033495575189590454\n",
      "Batch 47/14851, Loss: 0.0010539591312408447\n",
      "Batch 48/14851, Loss: 0.0018888148479163647\n",
      "Batch 49/14851, Loss: 0.026755833998322487\n",
      "Batch 50/14851, Loss: 0.07391546666622162\n",
      "Batch 51/14851, Loss: 0.00734492065384984\n",
      "Batch 52/14851, Loss: 3.492708128760569e-05\n",
      "Batch 53/14851, Loss: 0.005071838852018118\n",
      "Batch 54/14851, Loss: 0.0013915440067648888\n",
      "Batch 55/14851, Loss: 0.00017373014998156577\n",
      "Batch 56/14851, Loss: 0.0010663612047210336\n",
      "Batch 57/14851, Loss: 0.0015128491213545203\n",
      "Batch 58/14851, Loss: 0.006794830318540335\n",
      "Batch 59/14851, Loss: 0.038910627365112305\n",
      "Batch 60/14851, Loss: 0.0665464773774147\n",
      "Batch 61/14851, Loss: 0.00014096002269070596\n",
      "Batch 62/14851, Loss: 0.0008602539892308414\n",
      "Batch 63/14851, Loss: 0.005274048540741205\n",
      "Batch 64/14851, Loss: 0.01645393669605255\n",
      "Batch 65/14851, Loss: 0.004038588609546423\n",
      "Batch 66/14851, Loss: 0.01764451339840889\n",
      "Batch 67/14851, Loss: 0.00037346905446611345\n",
      "Batch 68/14851, Loss: 0.031338393688201904\n",
      "Batch 69/14851, Loss: 0.0021388183813542128\n",
      "Batch 70/14851, Loss: 0.0006674627657048404\n",
      "Batch 71/14851, Loss: 0.0011020442470908165\n",
      "Batch 72/14851, Loss: 0.000542864203453064\n",
      "Batch 73/14851, Loss: 0.009829936549067497\n",
      "Batch 74/14851, Loss: 0.0026241366285830736\n",
      "Batch 75/14851, Loss: 0.04368292912840843\n",
      "Batch 76/14851, Loss: 0.0009349402971565723\n",
      "Batch 77/14851, Loss: 0.055493470281362534\n",
      "Batch 78/14851, Loss: 0.00036672005080617964\n",
      "Batch 79/14851, Loss: 0.009659255854785442\n",
      "Batch 80/14851, Loss: 0.0061174663715064526\n",
      "Batch 81/14851, Loss: 0.00010916093742707744\n",
      "Batch 82/14851, Loss: 0.0043931505642831326\n",
      "Batch 83/14851, Loss: 0.006410967092961073\n",
      "Batch 84/14851, Loss: 0.01034602802246809\n",
      "Batch 85/14851, Loss: 0.015507317148149014\n",
      "Batch 86/14851, Loss: 0.002717796713113785\n",
      "Batch 87/14851, Loss: 0.04877099022269249\n",
      "Batch 88/14851, Loss: 0.07471788674592972\n",
      "Batch 89/14851, Loss: 0.002169570652768016\n",
      "Batch 90/14851, Loss: 0.008860867470502853\n",
      "Batch 91/14851, Loss: 0.03324303403496742\n",
      "Batch 92/14851, Loss: 0.0015002290019765496\n",
      "Batch 93/14851, Loss: 0.005304386373609304\n",
      "Batch 94/14851, Loss: 0.00012405838060658425\n",
      "Batch 95/14851, Loss: 0.0001946402044268325\n",
      "Batch 96/14851, Loss: 0.027679461985826492\n",
      "Batch 97/14851, Loss: 0.002400545170530677\n",
      "Batch 98/14851, Loss: 4.137928408454172e-05\n",
      "Batch 99/14851, Loss: 0.0061228531412780285\n",
      "Batch 100/14851, Loss: 0.003581983270123601\n",
      "Batch 101/14851, Loss: 0.003020587144419551\n",
      "Batch 102/14851, Loss: 0.00017279386520385742\n",
      "Batch 103/14851, Loss: 0.016694365069270134\n",
      "Batch 104/14851, Loss: 0.0007845585350878537\n",
      "Batch 105/14851, Loss: 0.023498354479670525\n",
      "Batch 106/14851, Loss: 0.07204625755548477\n",
      "Batch 107/14851, Loss: 0.013974729925394058\n",
      "Batch 108/14851, Loss: 0.013879511505365372\n",
      "Batch 109/14851, Loss: 0.00015137965965550393\n",
      "Batch 110/14851, Loss: 0.06950503587722778\n",
      "Batch 111/14851, Loss: 0.027644667774438858\n",
      "Batch 112/14851, Loss: 0.02259598858654499\n",
      "Batch 113/14851, Loss: 0.0031751850619912148\n",
      "Batch 114/14851, Loss: 0.00045638656592927873\n",
      "Batch 115/14851, Loss: 0.0008647193317301571\n",
      "Batch 116/14851, Loss: 0.0045353383757174015\n",
      "Batch 117/14851, Loss: 0.004226207733154297\n",
      "Batch 118/14851, Loss: 0.001056022010743618\n",
      "Batch 119/14851, Loss: 0.042152851819992065\n",
      "Batch 120/14851, Loss: 0.010955470614135265\n",
      "Batch 121/14851, Loss: 0.021271012723445892\n",
      "Batch 122/14851, Loss: 0.0007223921711556613\n",
      "Batch 123/14851, Loss: 0.0702582597732544\n",
      "Batch 124/14851, Loss: 0.0003690893354360014\n",
      "Batch 125/14851, Loss: 0.04105444252490997\n",
      "Batch 126/14851, Loss: 0.01099351141601801\n",
      "Batch 127/14851, Loss: 0.0017266497015953064\n",
      "Batch 128/14851, Loss: 0.0003854272363241762\n",
      "Batch 129/14851, Loss: 0.03531229868531227\n",
      "Batch 130/14851, Loss: 0.0008528170292265713\n",
      "Batch 131/14851, Loss: 0.03653858229517937\n",
      "Batch 132/14851, Loss: 0.01018568966537714\n",
      "Batch 133/14851, Loss: 0.01327523309737444\n",
      "Batch 134/14851, Loss: 0.0004467529652174562\n",
      "Batch 135/14851, Loss: 0.014186238870024681\n",
      "Batch 136/14851, Loss: 0.010915201157331467\n",
      "Batch 137/14851, Loss: 0.01124418992549181\n",
      "Batch 138/14851, Loss: 0.0017830647993832827\n",
      "Batch 139/14851, Loss: 0.010332818143069744\n",
      "Batch 140/14851, Loss: 0.002166974125429988\n",
      "Batch 141/14851, Loss: 0.002018609317019582\n",
      "Batch 142/14851, Loss: 0.0003846449253614992\n",
      "Batch 143/14851, Loss: 0.05777248740196228\n",
      "Batch 144/14851, Loss: 0.010598688386380672\n",
      "Batch 145/14851, Loss: 5.6978315114974976e-05\n",
      "Batch 146/14851, Loss: 0.00034923269413411617\n",
      "Batch 147/14851, Loss: 0.003828272921964526\n",
      "Batch 148/14851, Loss: 0.01579049602150917\n",
      "Batch 149/14851, Loss: 0.002214444801211357\n",
      "Batch 150/14851, Loss: 0.0005538224941119552\n",
      "Batch 151/14851, Loss: 0.0011680833995342255\n",
      "Batch 152/14851, Loss: 0.005723691079765558\n",
      "Batch 153/14851, Loss: 0.0006310600438155234\n",
      "Batch 154/14851, Loss: 0.008678066544234753\n",
      "Batch 155/14851, Loss: 0.02921496331691742\n",
      "Batch 156/14851, Loss: 0.009209600277245045\n",
      "Batch 157/14851, Loss: 0.014433454722166061\n",
      "Batch 158/14851, Loss: 0.00010478869080543518\n",
      "Batch 159/14851, Loss: 0.0002987062034662813\n",
      "Batch 160/14851, Loss: 0.00776583980768919\n",
      "Batch 161/14851, Loss: 0.0032188729383051395\n",
      "Batch 162/14851, Loss: 0.0033275592140853405\n",
      "Batch 163/14851, Loss: 0.03511568158864975\n",
      "Batch 164/14851, Loss: 0.0004500994982663542\n",
      "Batch 165/14851, Loss: 0.0013614296913146973\n",
      "Batch 166/14851, Loss: 0.00040461868047714233\n",
      "Batch 167/14851, Loss: 0.0027712658047676086\n",
      "Batch 168/14851, Loss: 0.0004472239234019071\n",
      "Batch 169/14851, Loss: 0.0003572565910872072\n",
      "Batch 170/14851, Loss: 0.0006348502938635647\n",
      "Batch 171/14851, Loss: 0.009768611751496792\n",
      "Batch 172/14851, Loss: 0.030904633924365044\n",
      "Batch 173/14851, Loss: 0.010042130947113037\n",
      "Batch 174/14851, Loss: 0.00019824877381324768\n",
      "Batch 175/14851, Loss: 0.013997284695506096\n",
      "Batch 176/14851, Loss: 0.004385311622172594\n",
      "Batch 177/14851, Loss: 0.03478293493390083\n",
      "Batch 178/14851, Loss: 0.013455059379339218\n",
      "Batch 179/14851, Loss: 0.010209421627223492\n",
      "Batch 180/14851, Loss: 0.0001540315424790606\n",
      "Batch 181/14851, Loss: 0.0012527108192443848\n",
      "Batch 182/14851, Loss: 0.0008020599489100277\n",
      "Batch 183/14851, Loss: 0.007955186069011688\n",
      "Batch 184/14851, Loss: 0.0002770464343484491\n",
      "Batch 185/14851, Loss: 0.005786276888102293\n",
      "Batch 186/14851, Loss: 0.01005668006837368\n",
      "Batch 187/14851, Loss: 6.109227979322895e-05\n",
      "Batch 188/14851, Loss: 0.009303783066570759\n",
      "Batch 189/14851, Loss: 0.00706717511638999\n",
      "Batch 190/14851, Loss: 0.03101697936654091\n",
      "Batch 191/14851, Loss: 0.04352733865380287\n",
      "Batch 192/14851, Loss: 0.02923218347132206\n",
      "Batch 193/14851, Loss: 0.002762551186606288\n",
      "Batch 194/14851, Loss: 0.0002254943101434037\n",
      "Batch 195/14851, Loss: 0.029522016644477844\n",
      "Batch 196/14851, Loss: 0.033741313964128494\n",
      "Batch 197/14851, Loss: 0.01399488840252161\n",
      "Batch 198/14851, Loss: 0.00539412209764123\n",
      "Batch 199/14851, Loss: 0.025546595454216003\n",
      "Batch 200/14851, Loss: 0.0006453394889831543\n",
      "Batch 201/14851, Loss: 0.022495493292808533\n",
      "Batch 202/14851, Loss: 0.0007148057920858264\n",
      "Batch 203/14851, Loss: 0.0007515159668400884\n",
      "Batch 204/14851, Loss: 0.0014515029033645988\n",
      "Batch 205/14851, Loss: 0.04015915468335152\n",
      "Batch 206/14851, Loss: 0.009823368862271309\n",
      "Batch 207/14851, Loss: 0.004054066259413958\n",
      "Batch 208/14851, Loss: 0.00042567617492750287\n",
      "Batch 209/14851, Loss: 0.014420388266444206\n",
      "Batch 210/14851, Loss: 0.004949583206325769\n",
      "Batch 211/14851, Loss: 0.04809103161096573\n",
      "Batch 212/14851, Loss: 0.013475370593369007\n",
      "Batch 213/14851, Loss: 0.024142757058143616\n",
      "Batch 214/14851, Loss: 0.005906983278691769\n",
      "Batch 215/14851, Loss: 0.0036150701344013214\n",
      "Batch 216/14851, Loss: 0.034415557980537415\n",
      "Batch 217/14851, Loss: 0.002074817894026637\n",
      "Batch 218/14851, Loss: 0.007000113371759653\n",
      "Batch 219/14851, Loss: 0.006486872676759958\n",
      "Batch 220/14851, Loss: 0.007007422391325235\n",
      "Batch 221/14851, Loss: 0.022690847516059875\n",
      "Batch 222/14851, Loss: 0.002942258259281516\n",
      "Batch 223/14851, Loss: 0.0007600968820042908\n",
      "Batch 224/14851, Loss: 0.0010721096768975258\n",
      "Batch 225/14851, Loss: 0.016760671511292458\n",
      "Batch 226/14851, Loss: 0.0014491061447188258\n",
      "Batch 227/14851, Loss: 0.0036706903483718634\n",
      "Batch 228/14851, Loss: 0.003243474056944251\n",
      "Batch 229/14851, Loss: 0.020748155191540718\n",
      "Batch 230/14851, Loss: 0.022032497450709343\n",
      "Batch 231/14851, Loss: 0.004830099642276764\n",
      "Batch 232/14851, Loss: 0.0016303743468597531\n",
      "Batch 233/14851, Loss: 0.0001274198293685913\n",
      "Batch 234/14851, Loss: 0.00715534808114171\n",
      "Batch 235/14851, Loss: 0.0063667306676507\n",
      "Batch 236/14851, Loss: 0.0020395268220454454\n",
      "Batch 237/14851, Loss: 0.0012893921229988337\n",
      "Batch 238/14851, Loss: 0.00960287544876337\n",
      "Batch 239/14851, Loss: 0.0024271048605442047\n",
      "Batch 240/14851, Loss: 0.008613902144134045\n",
      "Batch 241/14851, Loss: 0.002650611335411668\n",
      "Batch 242/14851, Loss: 0.0010267073521390557\n",
      "Batch 243/14851, Loss: 0.002373444614931941\n",
      "Batch 244/14851, Loss: 0.00270530441775918\n",
      "Batch 245/14851, Loss: 0.016193196177482605\n",
      "Batch 246/14851, Loss: 0.002783402567729354\n",
      "Batch 247/14851, Loss: 0.000636837154161185\n",
      "Batch 248/14851, Loss: 0.0021498475689440966\n",
      "Batch 249/14851, Loss: 0.007620035205036402\n",
      "Batch 250/14851, Loss: 0.02209559641778469\n",
      "Batch 251/14851, Loss: 0.0006184858502820134\n",
      "Batch 252/14851, Loss: 0.00013441841292660683\n",
      "Batch 253/14851, Loss: 0.01512343343347311\n",
      "Batch 254/14851, Loss: 0.0010564655531197786\n",
      "Batch 255/14851, Loss: 0.025404414162039757\n",
      "Batch 256/14851, Loss: 5.679081004927866e-05\n",
      "Batch 257/14851, Loss: 0.04336975887417793\n",
      "Batch 258/14851, Loss: 0.00033640614128671587\n",
      "Batch 259/14851, Loss: 0.001853756490163505\n",
      "Batch 260/14851, Loss: 0.0003805049345828593\n",
      "Batch 261/14851, Loss: 0.03457878902554512\n",
      "Batch 262/14851, Loss: 0.00437651202082634\n",
      "Batch 263/14851, Loss: 0.002828930737450719\n",
      "Batch 264/14851, Loss: 0.0026667725760489702\n",
      "Batch 265/14851, Loss: 0.00011957287642871961\n",
      "Batch 266/14851, Loss: 0.0008894409402273595\n",
      "Batch 267/14851, Loss: 0.009794049896299839\n",
      "Batch 268/14851, Loss: 0.0002384980471106246\n",
      "Batch 269/14851, Loss: 0.0020427966956049204\n",
      "Batch 270/14851, Loss: 0.013023504056036472\n",
      "Batch 271/14851, Loss: 0.009308298118412495\n",
      "Batch 272/14851, Loss: 0.057313840836286545\n",
      "Batch 273/14851, Loss: 0.027819810435175896\n",
      "Batch 274/14851, Loss: 0.04701517894864082\n",
      "Batch 275/14851, Loss: 0.00013496726751327515\n",
      "Batch 276/14851, Loss: 0.02757496014237404\n",
      "Batch 277/14851, Loss: 0.0006133136339485645\n",
      "Batch 278/14851, Loss: 0.008942416869103909\n",
      "Batch 279/14851, Loss: 0.00021477539849001914\n",
      "Batch 280/14851, Loss: 0.049735475331544876\n",
      "Batch 281/14851, Loss: 0.008230755105614662\n",
      "Batch 282/14851, Loss: 0.03934262692928314\n",
      "Batch 283/14851, Loss: 0.0007678368128836155\n",
      "Batch 284/14851, Loss: 0.027131978422403336\n",
      "Batch 285/14851, Loss: 0.0032070328015834093\n",
      "Batch 286/14851, Loss: 0.007915184833109379\n",
      "Batch 287/14851, Loss: 0.0005506314337253571\n",
      "Batch 288/14851, Loss: 0.0008766137179918587\n",
      "Batch 289/14851, Loss: 0.0033924977760761976\n",
      "Batch 290/14851, Loss: 0.009860369376838207\n",
      "Batch 291/14851, Loss: 0.013970930129289627\n",
      "Batch 292/14851, Loss: 0.017736557871103287\n",
      "Batch 293/14851, Loss: 0.019748909398913383\n",
      "Batch 294/14851, Loss: 0.005529114510864019\n",
      "Batch 295/14851, Loss: 0.001475391793064773\n",
      "Batch 296/14851, Loss: 0.003600897965952754\n",
      "Batch 297/14851, Loss: 0.013324056752026081\n",
      "Batch 298/14851, Loss: 0.04424069821834564\n",
      "Batch 299/14851, Loss: 0.0007470336859114468\n",
      "Batch 300/14851, Loss: 0.00856848806142807\n",
      "Batch 301/14851, Loss: 0.006884882226586342\n",
      "Batch 302/14851, Loss: 0.031577229499816895\n",
      "Batch 303/14851, Loss: 0.011642598547041416\n",
      "Batch 304/14851, Loss: 0.005931635852903128\n",
      "Batch 305/14851, Loss: 0.013314674608409405\n",
      "Batch 306/14851, Loss: 0.0005823783576488495\n",
      "Batch 307/14851, Loss: 0.016145939007401466\n",
      "Batch 308/14851, Loss: 0.005756003316491842\n",
      "Batch 309/14851, Loss: 0.007435698993504047\n",
      "Batch 310/14851, Loss: 0.006876272615045309\n",
      "Batch 311/14851, Loss: 0.04848061129450798\n",
      "Batch 312/14851, Loss: 0.02130219154059887\n",
      "Batch 313/14851, Loss: 0.014037993736565113\n",
      "Batch 314/14851, Loss: 0.0024527860805392265\n",
      "Batch 315/14851, Loss: 0.0005184672772884369\n",
      "Batch 316/14851, Loss: 0.002927972935140133\n",
      "Batch 317/14851, Loss: 0.001278693787753582\n",
      "Batch 318/14851, Loss: 0.0036084193270653486\n",
      "Batch 319/14851, Loss: 0.02868962660431862\n",
      "Batch 320/14851, Loss: 0.051440272480249405\n",
      "Batch 321/14851, Loss: 0.022924602031707764\n",
      "Batch 322/14851, Loss: 0.005660639610141516\n",
      "Batch 323/14851, Loss: 0.0022432238329201937\n",
      "Batch 324/14851, Loss: 0.002125499537214637\n",
      "Batch 325/14851, Loss: 8.770575368544087e-05\n",
      "Batch 326/14851, Loss: 0.028474414721131325\n",
      "Batch 327/14851, Loss: 0.004096226301044226\n",
      "Batch 328/14851, Loss: 0.02337789535522461\n",
      "Batch 329/14851, Loss: 0.008423548191785812\n",
      "Batch 330/14851, Loss: 0.007250575348734856\n",
      "Batch 331/14851, Loss: 0.005642245057970285\n",
      "Batch 332/14851, Loss: 0.005410303827375174\n",
      "Batch 333/14851, Loss: 0.002696539042517543\n",
      "Batch 334/14851, Loss: 0.00922464206814766\n",
      "Batch 335/14851, Loss: 0.0025051545817404985\n",
      "Batch 336/14851, Loss: 0.009169306606054306\n",
      "Batch 337/14851, Loss: 0.02587413229048252\n",
      "Batch 338/14851, Loss: 0.0011159839341416955\n",
      "Batch 339/14851, Loss: 0.0007372709806077182\n",
      "Batch 340/14851, Loss: 0.004031530115753412\n",
      "Batch 341/14851, Loss: 0.001967044547200203\n",
      "Batch 342/14851, Loss: 0.001263645594008267\n",
      "Batch 343/14851, Loss: 9.93683934211731e-05\n",
      "Batch 344/14851, Loss: 0.018127815797924995\n",
      "Batch 345/14851, Loss: 0.0004278098640497774\n",
      "Batch 346/14851, Loss: 0.0023389572743326426\n",
      "Batch 347/14851, Loss: 0.020477833226323128\n",
      "Batch 348/14851, Loss: 0.005455767270177603\n",
      "Batch 349/14851, Loss: 0.0024973039980977774\n",
      "Batch 350/14851, Loss: 0.009337727911770344\n",
      "Batch 351/14851, Loss: 0.01912635937333107\n",
      "Batch 352/14851, Loss: 0.002568949945271015\n",
      "Batch 353/14851, Loss: 0.0002576237020548433\n",
      "Batch 354/14851, Loss: 0.039304811507463455\n",
      "Batch 355/14851, Loss: 0.0003967334923800081\n",
      "Batch 356/14851, Loss: 0.013982418924570084\n",
      "Batch 357/14851, Loss: 0.0028047412633895874\n",
      "Batch 358/14851, Loss: 0.0009118269081227481\n",
      "Batch 359/14851, Loss: 0.03706718981266022\n",
      "Batch 360/14851, Loss: 0.00021853049111086875\n",
      "Batch 361/14851, Loss: 0.0013849828392267227\n",
      "Batch 362/14851, Loss: 0.0002766735851764679\n",
      "Batch 363/14851, Loss: 0.02051335945725441\n",
      "Batch 364/14851, Loss: 0.04540351778268814\n",
      "Batch 365/14851, Loss: 0.0005353552405722439\n",
      "Batch 366/14851, Loss: 0.008205891586840153\n",
      "Batch 367/14851, Loss: 0.008301992900669575\n",
      "Batch 368/14851, Loss: 0.0005962350405752659\n",
      "Batch 369/14851, Loss: 0.003493787022307515\n",
      "Batch 370/14851, Loss: 0.015527807176113129\n",
      "Batch 371/14851, Loss: 0.053718820214271545\n",
      "Batch 372/14851, Loss: 0.024875055998563766\n",
      "Batch 373/14851, Loss: 0.0005318981711752713\n",
      "Batch 374/14851, Loss: 0.01277318224310875\n",
      "Batch 375/14851, Loss: 0.0539216622710228\n",
      "Batch 376/14851, Loss: 5.4135918617248535e-05\n",
      "Batch 377/14851, Loss: 0.0005644572083838284\n",
      "Batch 378/14851, Loss: 0.039214618504047394\n",
      "Batch 379/14851, Loss: 0.007881675846874714\n",
      "Batch 380/14851, Loss: 0.0008317381143569946\n",
      "Batch 381/14851, Loss: 0.009865631349384785\n",
      "Batch 382/14851, Loss: 0.014179354533553123\n",
      "Batch 383/14851, Loss: 0.020395411178469658\n",
      "Batch 384/14851, Loss: 0.025062674656510353\n",
      "Batch 385/14851, Loss: 0.0032262864988297224\n",
      "Batch 386/14851, Loss: 0.0011237493017688394\n",
      "Batch 387/14851, Loss: 0.00016724194574635476\n",
      "Batch 388/14851, Loss: 0.05630688741803169\n",
      "Batch 389/14851, Loss: 0.0013202416012063622\n",
      "Batch 390/14851, Loss: 0.009988588280975819\n",
      "Batch 391/14851, Loss: 0.04834318533539772\n",
      "Batch 392/14851, Loss: 0.0020435452461242676\n",
      "Batch 393/14851, Loss: 0.009243729524314404\n",
      "Batch 394/14851, Loss: 0.001093971193768084\n",
      "Batch 395/14851, Loss: 0.00405774125829339\n",
      "Batch 396/14851, Loss: 0.0024740882217884064\n",
      "Batch 397/14851, Loss: 0.0011288512032479048\n",
      "Batch 398/14851, Loss: 0.0126795694231987\n",
      "Batch 399/14851, Loss: 0.016845840960741043\n",
      "Batch 400/14851, Loss: 0.007856777869164944\n",
      "Batch 401/14851, Loss: 0.010040531866252422\n",
      "Batch 402/14851, Loss: 2.5657316655269824e-05\n",
      "Batch 403/14851, Loss: 0.004073059652000666\n",
      "Batch 404/14851, Loss: 0.0016190422466024756\n",
      "Batch 405/14851, Loss: 0.00027542063617147505\n",
      "Batch 406/14851, Loss: 0.00024751946330070496\n",
      "Batch 407/14851, Loss: 0.0016259050462394953\n",
      "Batch 408/14851, Loss: 0.00011100868141511455\n",
      "Batch 409/14851, Loss: 0.00122673565056175\n",
      "Batch 410/14851, Loss: 0.004891856107860804\n",
      "Batch 411/14851, Loss: 0.00028374046087265015\n",
      "Batch 412/14851, Loss: 0.00582481175661087\n",
      "Batch 413/14851, Loss: 0.03423593193292618\n",
      "Batch 414/14851, Loss: 0.024954788386821747\n",
      "Batch 415/14851, Loss: 0.0016262432327494025\n",
      "Batch 416/14851, Loss: 0.053450293838977814\n",
      "Batch 417/14851, Loss: 5.278736352920532e-05\n",
      "Batch 418/14851, Loss: 0.0024222582578659058\n",
      "Batch 419/14851, Loss: 0.0009300808305852115\n",
      "Batch 420/14851, Loss: 0.04022763669490814\n",
      "Batch 421/14851, Loss: 0.0033797307405620813\n",
      "Batch 422/14851, Loss: 0.0028835434932261705\n",
      "Batch 423/14851, Loss: 0.000874376273714006\n",
      "Batch 424/14851, Loss: 0.02760465256869793\n",
      "Batch 425/14851, Loss: 0.009601587429642677\n",
      "Batch 426/14851, Loss: 0.0032469071447849274\n",
      "Batch 427/14851, Loss: 0.02824614755809307\n",
      "Batch 428/14851, Loss: 0.0007783090695738792\n",
      "Batch 429/14851, Loss: 0.017323467880487442\n",
      "Batch 430/14851, Loss: 0.013936256058514118\n",
      "Batch 431/14851, Loss: 0.02723219059407711\n",
      "Batch 432/14851, Loss: 0.0013760129222646356\n",
      "Batch 433/14851, Loss: 0.040877606719732285\n",
      "Batch 434/14851, Loss: 0.0014262886252254248\n",
      "Batch 435/14851, Loss: 0.001672289683483541\n",
      "Batch 436/14851, Loss: 0.009906007908284664\n",
      "Batch 437/14851, Loss: 0.008766698651015759\n",
      "Batch 438/14851, Loss: 0.00015254780009854585\n",
      "Batch 439/14851, Loss: 0.07440204173326492\n",
      "Batch 440/14851, Loss: 0.0032162375282496214\n",
      "Batch 441/14851, Loss: 0.009977414272725582\n",
      "Batch 442/14851, Loss: 0.00658798310905695\n",
      "Batch 443/14851, Loss: 0.026016782969236374\n",
      "Batch 444/14851, Loss: 0.025692442432045937\n",
      "Batch 445/14851, Loss: 0.00023338035680353642\n",
      "Batch 446/14851, Loss: 0.0028720784466713667\n",
      "Batch 447/14851, Loss: 0.0117630111053586\n",
      "Batch 448/14851, Loss: 0.0013743253657594323\n",
      "Batch 449/14851, Loss: 0.008313123136758804\n",
      "Batch 450/14851, Loss: 0.0036556969862431288\n",
      "Batch 451/14851, Loss: 0.022066811099648476\n",
      "Batch 452/14851, Loss: 0.03481149673461914\n",
      "Batch 453/14851, Loss: 0.013339529745280743\n",
      "Batch 454/14851, Loss: 0.032413821667432785\n",
      "Batch 455/14851, Loss: 0.0010499244090169668\n",
      "Batch 456/14851, Loss: 0.002271105768159032\n",
      "Batch 457/14851, Loss: 0.006452260538935661\n",
      "Batch 458/14851, Loss: 0.0044219763949513435\n",
      "Batch 459/14851, Loss: 0.010424147360026836\n",
      "Batch 460/14851, Loss: 0.006709201727062464\n",
      "Batch 461/14851, Loss: 0.0077055394649505615\n",
      "Batch 462/14851, Loss: 0.002564169466495514\n",
      "Batch 463/14851, Loss: 0.009321870282292366\n",
      "Batch 464/14851, Loss: 0.015546446666121483\n",
      "Batch 465/14851, Loss: 0.004271271172910929\n",
      "Batch 466/14851, Loss: 0.02580440416932106\n",
      "Batch 467/14851, Loss: 0.0006466750637628138\n",
      "Batch 468/14851, Loss: 0.0050566187128424644\n",
      "Batch 469/14851, Loss: 0.03692911937832832\n",
      "Batch 470/14851, Loss: 0.039721425622701645\n",
      "Batch 471/14851, Loss: 0.008073586039245129\n",
      "Batch 472/14851, Loss: 0.0033120799344033003\n",
      "Batch 473/14851, Loss: 0.010553741827607155\n",
      "Batch 474/14851, Loss: 0.004130174871534109\n",
      "Batch 475/14851, Loss: 0.002075960161164403\n",
      "Batch 476/14851, Loss: 0.006176789756864309\n",
      "Batch 477/14851, Loss: 0.020054807886481285\n",
      "Batch 478/14851, Loss: 0.0008900041575543582\n",
      "Batch 479/14851, Loss: 0.00402243435382843\n",
      "Batch 480/14851, Loss: 0.0011783689260482788\n",
      "Batch 481/14851, Loss: 0.0008463822305202484\n",
      "Batch 482/14851, Loss: 0.010140744969248772\n",
      "Batch 483/14851, Loss: 7.394453132292256e-05\n",
      "Batch 484/14851, Loss: 0.004786662291735411\n",
      "Batch 485/14851, Loss: 0.01571282371878624\n",
      "Batch 486/14851, Loss: 0.0008166502229869366\n",
      "Batch 487/14851, Loss: 0.00832146406173706\n",
      "Batch 488/14851, Loss: 0.0002022633998421952\n",
      "Batch 489/14851, Loss: 0.0039293221198022366\n",
      "Batch 490/14851, Loss: 0.027977777644991875\n",
      "Batch 491/14851, Loss: 0.0014097801176831126\n",
      "Batch 492/14851, Loss: 0.0017436497146263719\n",
      "Batch 493/14851, Loss: 0.027365947142243385\n",
      "Batch 494/14851, Loss: 0.0037368673365563154\n",
      "Batch 495/14851, Loss: 0.03274538740515709\n",
      "Batch 496/14851, Loss: 0.002451207023113966\n",
      "Batch 497/14851, Loss: 0.018349099904298782\n",
      "Batch 498/14851, Loss: 0.05546971410512924\n",
      "Batch 499/14851, Loss: 0.04065829515457153\n",
      "Batch 500/14851, Loss: 0.0036589016672223806\n",
      "Batch 501/14851, Loss: 0.025407740846276283\n",
      "Batch 502/14851, Loss: 0.005023961886763573\n",
      "Batch 503/14851, Loss: 0.018401583656668663\n",
      "Batch 504/14851, Loss: 0.007671769242733717\n",
      "Batch 505/14851, Loss: 0.00023020182561594993\n",
      "Batch 506/14851, Loss: 0.009833313524723053\n",
      "Batch 507/14851, Loss: 0.052103396505117416\n",
      "Batch 508/14851, Loss: 0.00029861429356969893\n",
      "Batch 509/14851, Loss: 0.012342333793640137\n",
      "Batch 510/14851, Loss: 0.015341409482061863\n",
      "Batch 511/14851, Loss: 0.00393754243850708\n",
      "Batch 512/14851, Loss: 0.002134285867214203\n",
      "Batch 513/14851, Loss: 0.0016844855854287744\n",
      "Batch 514/14851, Loss: 0.026559503749012947\n",
      "Batch 515/14851, Loss: 0.003318802686408162\n",
      "Batch 516/14851, Loss: 0.004585088696330786\n",
      "Batch 517/14851, Loss: 9.931375825544819e-05\n",
      "Batch 518/14851, Loss: 0.009958903305232525\n",
      "Batch 519/14851, Loss: 0.024017445743083954\n",
      "Batch 520/14851, Loss: 0.0027574796695262194\n",
      "Batch 521/14851, Loss: 0.0024757024366408587\n",
      "Batch 522/14851, Loss: 0.018999170511960983\n",
      "Batch 523/14851, Loss: 0.06192755326628685\n",
      "Batch 524/14851, Loss: 0.001931090489961207\n",
      "Batch 525/14851, Loss: 6.397068500518799e-05\n",
      "Batch 526/14851, Loss: 0.0006844704621471465\n",
      "Batch 527/14851, Loss: 0.03288964927196503\n",
      "Batch 528/14851, Loss: 0.005464727524667978\n",
      "Batch 529/14851, Loss: 0.004741921089589596\n",
      "Batch 530/14851, Loss: 0.00922298151999712\n",
      "Batch 531/14851, Loss: 0.001876140828244388\n",
      "Batch 532/14851, Loss: 0.0013966858386993408\n",
      "Batch 533/14851, Loss: 0.0017427168786525726\n",
      "Batch 534/14851, Loss: 0.0005486831068992615\n",
      "Batch 535/14851, Loss: 0.0779191330075264\n",
      "Batch 536/14851, Loss: 0.007892140187323093\n",
      "Batch 537/14851, Loss: 0.0013029774418100715\n",
      "Batch 538/14851, Loss: 0.00031656274222768843\n",
      "Batch 539/14851, Loss: 0.018076807260513306\n",
      "Batch 540/14851, Loss: 0.0004765242338180542\n",
      "Batch 541/14851, Loss: 0.006599497515708208\n",
      "Batch 542/14851, Loss: 0.0007656502421014011\n",
      "Batch 543/14851, Loss: 0.03180216997861862\n",
      "Batch 544/14851, Loss: 0.05836452543735504\n",
      "Batch 545/14851, Loss: 0.0011440193047747016\n",
      "Batch 546/14851, Loss: 0.00014356151223182678\n",
      "Batch 547/14851, Loss: 0.0006065976922400296\n",
      "Batch 548/14851, Loss: 0.0006390165071934462\n",
      "Batch 549/14851, Loss: 0.02752246893942356\n",
      "Batch 550/14851, Loss: 0.0024263437371701\n",
      "Batch 551/14851, Loss: 0.004539308603852987\n",
      "Batch 552/14851, Loss: 0.00038900101208128035\n",
      "Batch 553/14851, Loss: 0.02484811283648014\n",
      "Batch 554/14851, Loss: 0.014240059070289135\n",
      "Batch 555/14851, Loss: 0.023213621228933334\n",
      "Batch 556/14851, Loss: 0.008902308531105518\n",
      "Batch 557/14851, Loss: 0.001676861196756363\n",
      "Batch 558/14851, Loss: 0.0038428243715316057\n",
      "Batch 559/14851, Loss: 0.0039009489119052887\n",
      "Batch 560/14851, Loss: 0.016695870086550713\n",
      "Batch 561/14851, Loss: 0.00719404686242342\n",
      "Batch 562/14851, Loss: 0.009453381411731243\n",
      "Batch 563/14851, Loss: 0.0006993934512138367\n",
      "Batch 564/14851, Loss: 0.04482295364141464\n",
      "Batch 565/14851, Loss: 0.01698419824242592\n",
      "Batch 566/14851, Loss: 0.0005279108881950378\n",
      "Batch 567/14851, Loss: 0.00132722535636276\n",
      "Batch 568/14851, Loss: 9.391953790327534e-05\n",
      "Batch 569/14851, Loss: 0.00018012647342402488\n",
      "Batch 570/14851, Loss: 0.00890170969069004\n",
      "Batch 571/14851, Loss: 0.001505946391262114\n",
      "Batch 572/14851, Loss: 0.03736677020788193\n",
      "Batch 573/14851, Loss: 0.0037901911418884993\n",
      "Batch 574/14851, Loss: 0.07187443971633911\n",
      "Batch 575/14851, Loss: 0.005199773702770472\n",
      "Batch 576/14851, Loss: 0.07681199908256531\n",
      "Batch 577/14851, Loss: 0.022201992571353912\n",
      "Batch 578/14851, Loss: 0.006661952938884497\n",
      "Batch 579/14851, Loss: 0.022500410676002502\n",
      "Batch 580/14851, Loss: 0.010976096615195274\n",
      "Batch 581/14851, Loss: 0.06981172412633896\n",
      "Batch 582/14851, Loss: 0.0003644848766271025\n",
      "Batch 583/14851, Loss: 0.008231540210545063\n",
      "Batch 584/14851, Loss: 0.005103702191263437\n",
      "Batch 585/14851, Loss: 0.0010046599199995399\n",
      "Batch 586/14851, Loss: 0.013171049766242504\n",
      "Batch 587/14851, Loss: 0.02476830780506134\n",
      "Batch 588/14851, Loss: 0.07426570355892181\n",
      "Batch 589/14851, Loss: 0.0015080961165949702\n",
      "Batch 590/14851, Loss: 0.0002433396875858307\n",
      "Batch 591/14851, Loss: 0.0009697340428829193\n",
      "Batch 592/14851, Loss: 0.004819020628929138\n",
      "Batch 593/14851, Loss: 0.00016390408563893288\n",
      "Batch 594/14851, Loss: 0.011413496918976307\n",
      "Batch 595/14851, Loss: 0.010951618663966656\n",
      "Batch 596/14851, Loss: 0.017316142097115517\n",
      "Batch 597/14851, Loss: 0.01877562329173088\n",
      "Batch 598/14851, Loss: 0.0010432722046971321\n",
      "Batch 599/14851, Loss: 0.0013919462217018008\n",
      "Batch 600/14851, Loss: 0.005299056880176067\n",
      "Batch 601/14851, Loss: 0.04244253784418106\n",
      "Batch 602/14851, Loss: 0.001755442121066153\n",
      "Batch 603/14851, Loss: 0.0003782734274864197\n",
      "Batch 604/14851, Loss: 0.0002965355524793267\n",
      "Batch 605/14851, Loss: 0.0011193914106115699\n",
      "Batch 606/14851, Loss: 0.0010340389562770724\n",
      "Batch 607/14851, Loss: 0.028941243886947632\n",
      "Batch 608/14851, Loss: 0.04308793693780899\n",
      "Batch 609/14851, Loss: 0.0012121647596359253\n",
      "Batch 610/14851, Loss: 0.005273766815662384\n",
      "Batch 611/14851, Loss: 0.013184914365410805\n",
      "Batch 612/14851, Loss: 0.022899052128195763\n",
      "Batch 613/14851, Loss: 0.0007144746487028897\n",
      "Batch 614/14851, Loss: 0.010897667147219181\n",
      "Batch 615/14851, Loss: 0.017770327627658844\n",
      "Batch 616/14851, Loss: 0.03988180309534073\n",
      "Batch 617/14851, Loss: 0.024726325646042824\n",
      "Batch 618/14851, Loss: 0.00016555313777644187\n",
      "Batch 619/14851, Loss: 0.0011653266847133636\n",
      "Batch 620/14851, Loss: 0.0345957949757576\n",
      "Batch 621/14851, Loss: 0.008980119600892067\n",
      "Batch 622/14851, Loss: 0.021429436281323433\n",
      "Batch 623/14851, Loss: 0.002860519103705883\n",
      "Batch 624/14851, Loss: 0.0016442595515400171\n",
      "Batch 625/14851, Loss: 0.005588883068412542\n",
      "Batch 626/14851, Loss: 0.00012389570474624634\n",
      "Batch 627/14851, Loss: 0.008556832559406757\n",
      "Batch 628/14851, Loss: 0.0009068648214451969\n",
      "Batch 629/14851, Loss: 0.0008508878527209163\n",
      "Batch 630/14851, Loss: 0.002735322806984186\n",
      "Batch 631/14851, Loss: 0.006394599098712206\n",
      "Batch 632/14851, Loss: 0.02153804339468479\n",
      "Batch 633/14851, Loss: 0.0015731664607301354\n",
      "Batch 634/14851, Loss: 0.01413324847817421\n",
      "Batch 635/14851, Loss: 0.095024973154068\n",
      "Batch 636/14851, Loss: 0.01895020715892315\n",
      "Batch 637/14851, Loss: 0.0010496577015146613\n",
      "Batch 638/14851, Loss: 0.0036705813836306334\n",
      "Batch 639/14851, Loss: 0.0003159373882226646\n",
      "Batch 640/14851, Loss: 0.06386533379554749\n",
      "Batch 641/14851, Loss: 0.013247784227132797\n",
      "Batch 642/14851, Loss: 0.005078556481748819\n",
      "Batch 643/14851, Loss: 0.005199556704610586\n",
      "Batch 644/14851, Loss: 0.027915962040424347\n",
      "Batch 645/14851, Loss: 0.008169475942850113\n",
      "Batch 646/14851, Loss: 0.014359574764966965\n",
      "Batch 647/14851, Loss: 0.0008180141448974609\n",
      "Batch 648/14851, Loss: 0.0022644747514277697\n",
      "Batch 649/14851, Loss: 0.060629211366176605\n",
      "Batch 650/14851, Loss: 0.06021348387002945\n",
      "Batch 651/14851, Loss: 0.004742268472909927\n",
      "Batch 652/14851, Loss: 0.011689255945384502\n",
      "Batch 653/14851, Loss: 0.050837986171245575\n",
      "Batch 654/14851, Loss: 0.07357438653707504\n",
      "Batch 655/14851, Loss: 0.003598298877477646\n",
      "Batch 656/14851, Loss: 0.009273452684283257\n",
      "Batch 657/14851, Loss: 0.024735981598496437\n",
      "Batch 658/14851, Loss: 0.0012082546018064022\n",
      "Batch 659/14851, Loss: 0.01270652562379837\n",
      "Batch 660/14851, Loss: 0.002208329737186432\n",
      "Batch 661/14851, Loss: 0.02504107356071472\n",
      "Batch 662/14851, Loss: 0.038184184581041336\n",
      "Batch 663/14851, Loss: 0.00024318571377079934\n",
      "Batch 664/14851, Loss: 0.008744676597416401\n",
      "Batch 665/14851, Loss: 0.061796825379133224\n",
      "Batch 666/14851, Loss: 0.013014731928706169\n",
      "Batch 667/14851, Loss: 0.004623289220035076\n",
      "Batch 668/14851, Loss: 0.00707970280200243\n",
      "Batch 669/14851, Loss: 0.0061576212756335735\n",
      "Batch 670/14851, Loss: 0.007485884707421064\n",
      "Batch 671/14851, Loss: 0.06344879418611526\n",
      "Batch 672/14851, Loss: 0.013393530622124672\n",
      "Batch 673/14851, Loss: 0.0027443054132163525\n",
      "Batch 674/14851, Loss: 0.005566297098994255\n",
      "Batch 675/14851, Loss: 0.019418835639953613\n",
      "Batch 676/14851, Loss: 0.020589979365468025\n",
      "Batch 677/14851, Loss: 0.0010147355496883392\n",
      "Batch 678/14851, Loss: 0.002246742369607091\n",
      "Batch 679/14851, Loss: 0.001957477070391178\n",
      "Batch 680/14851, Loss: 0.01630803756415844\n",
      "Batch 681/14851, Loss: 0.009895260445773602\n",
      "Batch 682/14851, Loss: 0.0021827842574566603\n",
      "Batch 683/14851, Loss: 0.03580109402537346\n",
      "Batch 684/14851, Loss: 0.015659591183066368\n",
      "Batch 685/14851, Loss: 0.0017168211052194238\n",
      "Batch 686/14851, Loss: 0.0032816666644066572\n",
      "Batch 687/14851, Loss: 0.002687985310330987\n",
      "Batch 688/14851, Loss: 0.00037224218249320984\n",
      "Batch 689/14851, Loss: 0.0031418118160218\n",
      "Batch 690/14851, Loss: 0.007998009212315083\n",
      "Batch 691/14851, Loss: 0.0005728552932851017\n",
      "Batch 692/14851, Loss: 0.026705265045166016\n",
      "Batch 693/14851, Loss: 0.0053517939522862434\n",
      "Batch 694/14851, Loss: 6.826346361776814e-05\n",
      "Batch 695/14851, Loss: 0.0006105552311055362\n",
      "Batch 696/14851, Loss: 0.003892686916515231\n",
      "Batch 697/14851, Loss: 0.0007138202781789005\n",
      "Batch 698/14851, Loss: 0.002779913367703557\n",
      "Batch 699/14851, Loss: 0.00031301751732826233\n",
      "Batch 700/14851, Loss: 0.0021675366442650557\n",
      "Batch 701/14851, Loss: 0.05184478685259819\n",
      "Batch 702/14851, Loss: 0.032747089862823486\n",
      "Batch 703/14851, Loss: 0.10400845855474472\n",
      "Batch 704/14851, Loss: 0.02498667500913143\n",
      "Batch 705/14851, Loss: 0.04291856288909912\n",
      "Batch 706/14851, Loss: 0.011096817441284657\n",
      "Batch 707/14851, Loss: 0.005476702004671097\n",
      "Batch 708/14851, Loss: 0.01046281959861517\n",
      "Batch 709/14851, Loss: 0.0021214650478214025\n",
      "Batch 710/14851, Loss: 0.06175092235207558\n",
      "Batch 711/14851, Loss: 0.039632391184568405\n",
      "Batch 712/14851, Loss: 0.0458814911544323\n",
      "Batch 713/14851, Loss: 0.0006958407466299832\n",
      "Batch 714/14851, Loss: 0.008742224425077438\n",
      "Batch 715/14851, Loss: 0.0001110894008888863\n",
      "Batch 716/14851, Loss: 0.015534128062427044\n",
      "Batch 717/14851, Loss: 0.00029635181999765337\n",
      "Batch 718/14851, Loss: 0.04054730013012886\n",
      "Batch 719/14851, Loss: 0.014128847047686577\n",
      "Batch 720/14851, Loss: 7.880976045271382e-05\n",
      "Batch 721/14851, Loss: 0.003411961253732443\n",
      "Batch 722/14851, Loss: 0.008263982832431793\n",
      "Batch 723/14851, Loss: 0.006339533720165491\n",
      "Batch 724/14851, Loss: 0.0010059106862172484\n",
      "Batch 725/14851, Loss: 0.002282737521454692\n",
      "Batch 726/14851, Loss: 0.0006714343908242881\n",
      "Batch 727/14851, Loss: 0.0012347839074209332\n",
      "Batch 728/14851, Loss: 0.009271071292459965\n",
      "Batch 729/14851, Loss: 0.000694194168318063\n",
      "Batch 730/14851, Loss: 0.0008031942415982485\n",
      "Batch 731/14851, Loss: 0.016344107687473297\n",
      "Batch 732/14851, Loss: 0.025328239426016808\n",
      "Batch 733/14851, Loss: 0.016832806169986725\n",
      "Batch 734/14851, Loss: 0.0134650319814682\n",
      "Batch 735/14851, Loss: 0.004015443380922079\n",
      "Batch 736/14851, Loss: 0.02177506871521473\n",
      "Batch 737/14851, Loss: 0.0012434720993041992\n",
      "Batch 738/14851, Loss: 0.00044029331183992326\n",
      "Batch 739/14851, Loss: 0.003601367585361004\n",
      "Batch 740/14851, Loss: 0.006324984598904848\n",
      "Batch 741/14851, Loss: 0.02644776925444603\n",
      "Batch 742/14851, Loss: 0.03568334877490997\n",
      "Batch 743/14851, Loss: 0.0004314109683036804\n",
      "Batch 744/14851, Loss: 0.0014838427305221558\n",
      "Batch 745/14851, Loss: 0.002969615627080202\n",
      "Batch 746/14851, Loss: 0.0018003156874328852\n",
      "Batch 747/14851, Loss: 0.004444072488695383\n",
      "Batch 748/14851, Loss: 0.007767719682306051\n",
      "Batch 749/14851, Loss: 0.016730761155486107\n",
      "Batch 750/14851, Loss: 0.0010867019882425666\n",
      "Batch 751/14851, Loss: 0.003252583323046565\n",
      "Batch 752/14851, Loss: 0.0231789443641901\n",
      "Batch 753/14851, Loss: 0.002638342557474971\n",
      "Batch 754/14851, Loss: 0.004248087760061026\n",
      "Batch 755/14851, Loss: 0.0013917610049247742\n",
      "Batch 756/14851, Loss: 0.002786496886983514\n",
      "Batch 757/14851, Loss: 0.0006806763703934848\n",
      "Batch 758/14851, Loss: 0.0012738866498693824\n",
      "Batch 759/14851, Loss: 0.03786316141486168\n",
      "Batch 760/14851, Loss: 0.021164242178201675\n",
      "Batch 761/14851, Loss: 0.06212813779711723\n",
      "Batch 762/14851, Loss: 0.02383803389966488\n",
      "Batch 763/14851, Loss: 0.00046144425868988037\n",
      "Batch 764/14851, Loss: 0.0015396898379549384\n",
      "Batch 765/14851, Loss: 0.0038237052503973246\n",
      "Batch 766/14851, Loss: 0.007186666131019592\n",
      "Batch 767/14851, Loss: 0.004932028707116842\n",
      "Batch 768/14851, Loss: 0.0012130106333643198\n",
      "Batch 769/14851, Loss: 0.01242784596979618\n",
      "Batch 770/14851, Loss: 0.012503352016210556\n",
      "Batch 771/14851, Loss: 0.010341541841626167\n",
      "Batch 772/14851, Loss: 0.01624305732548237\n",
      "Batch 773/14851, Loss: 0.04356847330927849\n",
      "Batch 774/14851, Loss: 0.0008415298070758581\n",
      "Batch 775/14851, Loss: 0.008122035302221775\n",
      "Batch 776/14851, Loss: 0.005778597202152014\n",
      "Batch 777/14851, Loss: 0.016259996220469475\n",
      "Batch 778/14851, Loss: 0.0325210802257061\n",
      "Batch 779/14851, Loss: 0.0007412585546262562\n",
      "Batch 780/14851, Loss: 0.01894422620534897\n",
      "Batch 781/14851, Loss: 0.0011294558644294739\n",
      "Batch 782/14851, Loss: 0.017153527587652206\n",
      "Batch 783/14851, Loss: 0.001561976969242096\n",
      "Batch 784/14851, Loss: 0.0024224549997597933\n",
      "Batch 785/14851, Loss: 0.006842047907412052\n",
      "Batch 786/14851, Loss: 0.050569798797369\n",
      "Batch 787/14851, Loss: 0.0026331122498959303\n",
      "Batch 788/14851, Loss: 0.016937101259827614\n",
      "Batch 789/14851, Loss: 0.0032976206857711077\n",
      "Batch 790/14851, Loss: 0.013477623462677002\n",
      "Batch 791/14851, Loss: 0.0009947344660758972\n",
      "Batch 792/14851, Loss: 0.008066107518970966\n",
      "Batch 793/14851, Loss: 0.0010999649530276656\n",
      "Batch 794/14851, Loss: 0.0011487776646390557\n",
      "Batch 795/14851, Loss: 0.002132795751094818\n",
      "Batch 796/14851, Loss: 0.0013787371572107077\n",
      "Batch 797/14851, Loss: 0.01788470149040222\n",
      "Batch 798/14851, Loss: 0.024716882035136223\n",
      "Batch 799/14851, Loss: 0.00022605930280406028\n",
      "Batch 800/14851, Loss: 0.00034323459840379655\n",
      "Batch 801/14851, Loss: 0.01134990993887186\n",
      "Batch 802/14851, Loss: 0.0024241069331765175\n",
      "Batch 803/14851, Loss: 0.0014178529381752014\n",
      "Batch 804/14851, Loss: 0.02001373842358589\n",
      "Batch 805/14851, Loss: 0.04079258814454079\n",
      "Batch 806/14851, Loss: 0.007770795375108719\n",
      "Batch 807/14851, Loss: 0.0034281339030712843\n",
      "Batch 808/14851, Loss: 0.0007010680274106562\n",
      "Batch 809/14851, Loss: 0.00950690172612667\n",
      "Batch 810/14851, Loss: 0.0014639244182035327\n",
      "Batch 811/14851, Loss: 0.004503188189119101\n",
      "Batch 812/14851, Loss: 0.002058238722383976\n",
      "Batch 813/14851, Loss: 0.0024426181335002184\n",
      "Batch 814/14851, Loss: 0.004457222763448954\n",
      "Batch 815/14851, Loss: 0.0009250893490388989\n",
      "Batch 816/14851, Loss: 0.037760213017463684\n",
      "Batch 817/14851, Loss: 0.001616599503904581\n",
      "Batch 818/14851, Loss: 0.010671800933778286\n",
      "Batch 819/14851, Loss: 0.0657716616988182\n",
      "Batch 820/14851, Loss: 0.003620221046730876\n",
      "Batch 821/14851, Loss: 0.016037819907069206\n",
      "Batch 822/14851, Loss: 0.0033575459383428097\n",
      "Batch 823/14851, Loss: 0.052950385957956314\n",
      "Batch 824/14851, Loss: 0.008070643059909344\n",
      "Batch 825/14851, Loss: 0.0005043404526077211\n",
      "Batch 826/14851, Loss: 0.0012205458479002118\n",
      "Batch 827/14851, Loss: 0.023294588550925255\n",
      "Batch 828/14851, Loss: 0.0018915904220193624\n",
      "Batch 829/14851, Loss: 0.001090915291570127\n",
      "Batch 830/14851, Loss: 0.004198096692562103\n",
      "Batch 831/14851, Loss: 0.024997036904096603\n",
      "Batch 832/14851, Loss: 0.003786712419241667\n",
      "Batch 833/14851, Loss: 0.010732068680226803\n",
      "Batch 834/14851, Loss: 0.007255684584379196\n",
      "Batch 835/14851, Loss: 0.013283062726259232\n",
      "Batch 836/14851, Loss: 0.0017877680948004127\n",
      "Batch 837/14851, Loss: 0.011163853108882904\n",
      "Batch 838/14851, Loss: 0.01613994501531124\n",
      "Batch 839/14851, Loss: 0.009915133938193321\n",
      "Batch 840/14851, Loss: 0.0002742434444371611\n",
      "Batch 841/14851, Loss: 0.010870206169784069\n",
      "Batch 842/14851, Loss: 0.0008658245205879211\n",
      "Batch 843/14851, Loss: 0.00047745928168296814\n",
      "Batch 844/14851, Loss: 0.03459271043539047\n",
      "Batch 845/14851, Loss: 0.005481939762830734\n",
      "Batch 846/14851, Loss: 0.11129365861415863\n",
      "Batch 847/14851, Loss: 0.003077450441196561\n",
      "Batch 848/14851, Loss: 0.0028817893471568823\n",
      "Batch 849/14851, Loss: 0.004075566306710243\n",
      "Batch 850/14851, Loss: 0.001735910540446639\n",
      "Batch 851/14851, Loss: 0.00011803582310676575\n",
      "Batch 852/14851, Loss: 0.00032497753272764385\n",
      "Batch 853/14851, Loss: 0.023591933771967888\n",
      "Batch 854/14851, Loss: 0.011894186958670616\n",
      "Batch 855/14851, Loss: 0.0030068408232182264\n",
      "Batch 856/14851, Loss: 0.0034199466463178396\n",
      "Batch 857/14851, Loss: 0.018260227516293526\n",
      "Batch 858/14851, Loss: 0.00037753209471702576\n",
      "Batch 859/14851, Loss: 0.0021443164441734552\n",
      "Batch 860/14851, Loss: 0.0133009422570467\n",
      "Batch 861/14851, Loss: 0.0007440848858095706\n",
      "Batch 862/14851, Loss: 0.009978672489523888\n",
      "Batch 863/14851, Loss: 0.0016397112049162388\n",
      "Batch 864/14851, Loss: 0.01022249087691307\n",
      "Batch 865/14851, Loss: 0.00013043110084254295\n",
      "Batch 866/14851, Loss: 0.06879238784313202\n",
      "Batch 867/14851, Loss: 0.0005325265228748322\n",
      "Batch 868/14851, Loss: 0.0018439566483721137\n",
      "Batch 869/14851, Loss: 0.036681052297353745\n",
      "Batch 870/14851, Loss: 0.019753417000174522\n",
      "Batch 871/14851, Loss: 0.006190210580825806\n",
      "Batch 872/14851, Loss: 0.012225257232785225\n",
      "Batch 873/14851, Loss: 0.023231813684105873\n",
      "Batch 874/14851, Loss: 0.023022279143333435\n",
      "Batch 875/14851, Loss: 0.00041196495294570923\n",
      "Batch 876/14851, Loss: 0.001985655864700675\n",
      "Batch 877/14851, Loss: 0.00611337274312973\n",
      "Batch 878/14851, Loss: 0.012325560674071312\n",
      "Batch 879/14851, Loss: 0.003147970885038376\n",
      "Batch 880/14851, Loss: 0.003399189794436097\n",
      "Batch 881/14851, Loss: 0.005378922913223505\n",
      "Batch 882/14851, Loss: 0.02394925057888031\n",
      "Batch 883/14851, Loss: 0.0013466054806485772\n",
      "Batch 884/14851, Loss: 0.023234844207763672\n",
      "Batch 885/14851, Loss: 0.0016181583050638437\n",
      "Batch 886/14851, Loss: 0.0005732832942157984\n",
      "Batch 887/14851, Loss: 0.005896605085581541\n",
      "Batch 888/14851, Loss: 0.0023729729000478983\n",
      "Batch 889/14851, Loss: 0.0015296079218387604\n",
      "Batch 890/14851, Loss: 0.015354945324361324\n",
      "Batch 891/14851, Loss: 0.013781716115772724\n",
      "Batch 892/14851, Loss: 0.0019891897682100534\n",
      "Batch 893/14851, Loss: 0.0016302354633808136\n",
      "Batch 894/14851, Loss: 0.007245009299367666\n",
      "Batch 895/14851, Loss: 0.03439253568649292\n",
      "Batch 896/14851, Loss: 0.003982486668974161\n",
      "Batch 897/14851, Loss: 0.0026107020676136017\n",
      "Batch 898/14851, Loss: 0.0839613527059555\n",
      "Batch 899/14851, Loss: 0.001471343101002276\n",
      "Batch 900/14851, Loss: 0.03343938663601875\n",
      "Batch 901/14851, Loss: 0.005125019699335098\n",
      "Batch 902/14851, Loss: 0.0017474828055128455\n",
      "Batch 903/14851, Loss: 0.026617661118507385\n",
      "Batch 904/14851, Loss: 0.000912769406568259\n",
      "Batch 905/14851, Loss: 0.00020099058747291565\n",
      "Batch 906/14851, Loss: 0.016199316829442978\n",
      "Batch 907/14851, Loss: 0.043870534747838974\n",
      "Batch 908/14851, Loss: 0.010298802517354488\n",
      "Batch 909/14851, Loss: 0.004299915861338377\n",
      "Batch 910/14851, Loss: 0.0025604162365198135\n",
      "Batch 911/14851, Loss: 0.00395376980304718\n",
      "Batch 912/14851, Loss: 0.02276228368282318\n",
      "Batch 913/14851, Loss: 0.0012610811972990632\n",
      "Batch 914/14851, Loss: 0.0007499295170418918\n",
      "Batch 915/14851, Loss: 0.031186861917376518\n",
      "Batch 916/14851, Loss: 0.00020474568009376526\n",
      "Batch 917/14851, Loss: 0.0013817432336509228\n",
      "Batch 918/14851, Loss: 0.005920193623751402\n",
      "Batch 919/14851, Loss: 0.029174936935305595\n",
      "Batch 920/14851, Loss: 0.0014287900412455201\n",
      "Batch 921/14851, Loss: 0.032211896032094955\n",
      "Batch 922/14851, Loss: 0.009444279596209526\n",
      "Batch 923/14851, Loss: 0.001218395889736712\n",
      "Batch 924/14851, Loss: 0.01749850995838642\n",
      "Batch 925/14851, Loss: 0.0006297131185419858\n",
      "Batch 926/14851, Loss: 0.05359795689582825\n",
      "Batch 927/14851, Loss: 0.006905718240886927\n",
      "Batch 928/14851, Loss: 0.0022342305164784193\n",
      "Batch 929/14851, Loss: 0.004942080471664667\n",
      "Batch 930/14851, Loss: 0.014271617867052555\n",
      "Batch 931/14851, Loss: 0.007721251342445612\n",
      "Batch 932/14851, Loss: 0.0019226567819714546\n",
      "Batch 933/14851, Loss: 0.00019890938710886985\n",
      "Batch 934/14851, Loss: 0.06766735762357712\n",
      "Batch 935/14851, Loss: 0.01966964453458786\n",
      "Batch 936/14851, Loss: 0.005748497787863016\n",
      "Batch 937/14851, Loss: 0.008016111329197884\n",
      "Batch 938/14851, Loss: 0.023875514045357704\n",
      "Batch 939/14851, Loss: 3.0389675885089673e-05\n",
      "Batch 940/14851, Loss: 0.002243436872959137\n",
      "Batch 941/14851, Loss: 0.042034827172756195\n",
      "Batch 942/14851, Loss: 0.0014842336531728506\n",
      "Batch 943/14851, Loss: 0.0022689655888825655\n",
      "Batch 944/14851, Loss: 0.0037321578711271286\n",
      "Batch 945/14851, Loss: 0.00040816268301568925\n",
      "Batch 946/14851, Loss: 0.00518818711861968\n",
      "Batch 947/14851, Loss: 0.000774900137912482\n",
      "Batch 948/14851, Loss: 0.0016321813454851508\n",
      "Batch 949/14851, Loss: 0.05003191903233528\n",
      "Batch 950/14851, Loss: 0.0012903088936582208\n",
      "Batch 951/14851, Loss: 0.0014890381135046482\n",
      "Batch 952/14851, Loss: 0.0025053780991584063\n",
      "Batch 953/14851, Loss: 0.02043120563030243\n",
      "Batch 954/14851, Loss: 0.015229657292366028\n",
      "Batch 955/14851, Loss: 0.013387770392000675\n",
      "Batch 956/14851, Loss: 0.001786592067219317\n",
      "Batch 957/14851, Loss: 0.0384734645485878\n",
      "Batch 958/14851, Loss: 0.013690211810171604\n",
      "Batch 959/14851, Loss: 0.00615103542804718\n",
      "Batch 960/14851, Loss: 0.011378699913620949\n",
      "Batch 961/14851, Loss: 0.001138476189225912\n",
      "Batch 962/14851, Loss: 0.01452193409204483\n",
      "Batch 963/14851, Loss: 0.011835635639727116\n",
      "Batch 964/14851, Loss: 0.030043797567486763\n",
      "Batch 965/14851, Loss: 0.0035936343483626842\n",
      "Batch 966/14851, Loss: 0.04344778507947922\n",
      "Batch 967/14851, Loss: 0.04907180368900299\n",
      "Batch 968/14851, Loss: 0.023946303874254227\n",
      "Batch 969/14851, Loss: 0.10447324067354202\n",
      "Batch 970/14851, Loss: 0.017023369669914246\n",
      "Batch 971/14851, Loss: 0.013675663620233536\n",
      "Batch 972/14851, Loss: 0.022082407027482986\n",
      "Batch 973/14851, Loss: 0.045153092592954636\n",
      "Batch 974/14851, Loss: 0.022641824558377266\n",
      "Batch 975/14851, Loss: 0.07034372538328171\n",
      "Batch 976/14851, Loss: 0.0015181887429207563\n",
      "Batch 977/14851, Loss: 0.001888661296106875\n",
      "Batch 978/14851, Loss: 0.006609950214624405\n",
      "Batch 979/14851, Loss: 0.07332099974155426\n",
      "Batch 980/14851, Loss: 0.0036258697509765625\n",
      "Batch 981/14851, Loss: 0.0004433120193425566\n",
      "Batch 982/14851, Loss: 0.011352155357599258\n",
      "Batch 983/14851, Loss: 0.013227168470621109\n",
      "Batch 984/14851, Loss: 0.00656194007024169\n",
      "Batch 985/14851, Loss: 0.004531981889158487\n",
      "Batch 986/14851, Loss: 0.03759889677166939\n",
      "Batch 987/14851, Loss: 0.021325480192899704\n",
      "Batch 988/14851, Loss: 0.001231405884027481\n",
      "Batch 989/14851, Loss: 0.0021199907641857862\n",
      "Batch 990/14851, Loss: 0.0025692619383335114\n",
      "Batch 991/14851, Loss: 0.01495955791324377\n",
      "Batch 992/14851, Loss: 0.0020433738827705383\n",
      "Batch 993/14851, Loss: 0.0007657669484615326\n",
      "Batch 994/14851, Loss: 0.00788834784179926\n",
      "Batch 995/14851, Loss: 0.005718641448765993\n",
      "Batch 996/14851, Loss: 0.02018660493195057\n",
      "Batch 997/14851, Loss: 0.00314077315852046\n",
      "Batch 998/14851, Loss: 0.012861853465437889\n",
      "Batch 999/14851, Loss: 0.0009257246856577694\n",
      "Batch 1000/14851, Loss: 0.014564603567123413\n",
      "Batch 1001/14851, Loss: 0.001874767243862152\n",
      "Batch 1002/14851, Loss: 0.005231708753854036\n",
      "Batch 1003/14851, Loss: 0.04264217987656593\n",
      "Batch 1004/14851, Loss: 0.0003210852446500212\n",
      "Batch 1005/14851, Loss: 0.024298299103975296\n",
      "Batch 1006/14851, Loss: 0.00690807169303298\n",
      "Batch 1007/14851, Loss: 0.00037363669252954423\n",
      "Batch 1008/14851, Loss: 0.016200155019760132\n",
      "Batch 1009/14851, Loss: 0.005444089416414499\n",
      "Batch 1010/14851, Loss: 0.0005464355344884098\n",
      "Batch 1011/14851, Loss: 0.0010523219825699925\n",
      "Batch 1012/14851, Loss: 0.006985285319387913\n",
      "Batch 1013/14851, Loss: 0.0012489421060308814\n",
      "Batch 1014/14851, Loss: 0.06674293428659439\n",
      "Batch 1015/14851, Loss: 0.005560858640819788\n",
      "Batch 1016/14851, Loss: 0.013872861862182617\n",
      "Batch 1017/14851, Loss: 0.006428520195186138\n",
      "Batch 1018/14851, Loss: 0.004571587312966585\n",
      "Batch 1019/14851, Loss: 0.0028215881902724504\n",
      "Batch 1020/14851, Loss: 0.010008509270846844\n",
      "Batch 1021/14851, Loss: 0.006120019592344761\n",
      "Batch 1022/14851, Loss: 0.029247159138321877\n",
      "Batch 1023/14851, Loss: 0.018874583765864372\n",
      "Batch 1024/14851, Loss: 0.00034085908555425704\n",
      "Batch 1025/14851, Loss: 0.0004243167641106993\n",
      "Batch 1026/14851, Loss: 0.02486700564622879\n",
      "Batch 1027/14851, Loss: 0.00127337034791708\n",
      "Batch 1028/14851, Loss: 0.004077104385942221\n",
      "Batch 1029/14851, Loss: 0.03649075701832771\n",
      "Batch 1030/14851, Loss: 0.012223616242408752\n",
      "Batch 1031/14851, Loss: 0.0018468560883775353\n",
      "Batch 1032/14851, Loss: 0.003990929573774338\n",
      "Batch 1033/14851, Loss: 0.020791811868548393\n",
      "Batch 1034/14851, Loss: 0.061340443789958954\n",
      "Batch 1035/14851, Loss: 0.01970217376947403\n",
      "Batch 1036/14851, Loss: 0.014371762052178383\n",
      "Batch 1037/14851, Loss: 0.0001611684710951522\n",
      "Batch 1038/14851, Loss: 0.05206362158060074\n",
      "Batch 1039/14851, Loss: 0.00727394875138998\n",
      "Batch 1040/14851, Loss: 0.0007943746750243008\n",
      "Batch 1041/14851, Loss: 0.024517100304365158\n",
      "Batch 1042/14851, Loss: 0.0403364934027195\n",
      "Batch 1043/14851, Loss: 0.013349857181310654\n",
      "Batch 1044/14851, Loss: 0.00045392042375169694\n",
      "Batch 1045/14851, Loss: 0.0009467589552514255\n",
      "Batch 1046/14851, Loss: 0.0016044348012655973\n",
      "Batch 1047/14851, Loss: 0.02991073578596115\n",
      "Batch 1048/14851, Loss: 0.0016977190971374512\n",
      "Batch 1049/14851, Loss: 0.014850878156721592\n",
      "Batch 1050/14851, Loss: 0.00010889147961279377\n",
      "Batch 1051/14851, Loss: 0.0025211311876773834\n",
      "Batch 1052/14851, Loss: 0.00524436729028821\n",
      "Batch 1053/14851, Loss: 0.005348727107048035\n",
      "Batch 1054/14851, Loss: 0.023577673360705376\n",
      "Batch 1055/14851, Loss: 0.007355568930506706\n",
      "Batch 1056/14851, Loss: 0.001013080240227282\n",
      "Batch 1057/14851, Loss: 0.014764636754989624\n",
      "Batch 1058/14851, Loss: 0.0018523186445236206\n",
      "Batch 1059/14851, Loss: 0.02098401077091694\n",
      "Batch 1060/14851, Loss: 0.03786979988217354\n",
      "Batch 1061/14851, Loss: 0.0016827484359964728\n",
      "Batch 1062/14851, Loss: 0.010441457852721214\n",
      "Batch 1063/14851, Loss: 0.002051711082458496\n",
      "Batch 1064/14851, Loss: 0.006415245588868856\n",
      "Batch 1065/14851, Loss: 0.0004995626513846219\n",
      "Batch 1066/14851, Loss: 0.004876430612057447\n",
      "Batch 1067/14851, Loss: 0.017895491793751717\n",
      "Batch 1068/14851, Loss: 0.001603181124664843\n",
      "Batch 1069/14851, Loss: 0.02892749011516571\n",
      "Batch 1070/14851, Loss: 0.025306038558483124\n",
      "Batch 1071/14851, Loss: 0.01667015813291073\n",
      "Batch 1072/14851, Loss: 0.00681682862341404\n",
      "Batch 1073/14851, Loss: 0.03501546382904053\n",
      "Batch 1074/14851, Loss: 0.0012102400651201606\n",
      "Batch 1075/14851, Loss: 0.022906756028532982\n",
      "Batch 1076/14851, Loss: 0.0909862220287323\n",
      "Batch 1077/14851, Loss: 0.002799318404868245\n",
      "Batch 1078/14851, Loss: 0.00032629320048727095\n",
      "Batch 1079/14851, Loss: 0.020182935521006584\n",
      "Batch 1080/14851, Loss: 0.0021099750883877277\n",
      "Batch 1081/14851, Loss: 0.001455834717489779\n",
      "Batch 1082/14851, Loss: 0.004923435393720865\n",
      "Batch 1083/14851, Loss: 0.0010564984986558557\n",
      "Batch 1084/14851, Loss: 0.00276695704087615\n",
      "Batch 1085/14851, Loss: 0.014170045033097267\n",
      "Batch 1086/14851, Loss: 0.004514496307820082\n",
      "Batch 1087/14851, Loss: 0.0023545597214251757\n",
      "Batch 1088/14851, Loss: 0.004136982373893261\n",
      "Batch 1089/14851, Loss: 0.04305485263466835\n",
      "Batch 1090/14851, Loss: 0.004724803846329451\n",
      "Batch 1091/14851, Loss: 0.015996109694242477\n",
      "Batch 1092/14851, Loss: 0.025725126266479492\n",
      "Batch 1093/14851, Loss: 0.0008842560346238315\n",
      "Batch 1094/14851, Loss: 0.016600847244262695\n",
      "Batch 1095/14851, Loss: 0.0019704217556864023\n",
      "Batch 1096/14851, Loss: 0.007567645516246557\n",
      "Batch 1097/14851, Loss: 0.0008756232564337552\n",
      "Batch 1098/14851, Loss: 0.003851111978292465\n",
      "Batch 1099/14851, Loss: 0.007455433718860149\n",
      "Batch 1100/14851, Loss: 0.05776498466730118\n",
      "Batch 1101/14851, Loss: 0.006177638657391071\n",
      "Batch 1102/14851, Loss: 0.01404755748808384\n",
      "Batch 1103/14851, Loss: 0.032086726278066635\n",
      "Batch 1104/14851, Loss: 0.0012110049137845635\n",
      "Batch 1105/14851, Loss: 0.0032580727711319923\n",
      "Batch 1106/14851, Loss: 0.0016095085302367806\n",
      "Batch 1107/14851, Loss: 0.0006630520219914615\n",
      "Batch 1108/14851, Loss: 0.014105039648711681\n",
      "Batch 1109/14851, Loss: 0.03821749985218048\n",
      "Batch 1110/14851, Loss: 0.006225879769772291\n",
      "Batch 1111/14851, Loss: 0.05398106202483177\n",
      "Batch 1112/14851, Loss: 0.0055739847011864185\n",
      "Batch 1113/14851, Loss: 0.01731412298977375\n",
      "Batch 1114/14851, Loss: 0.003270115237683058\n",
      "Batch 1115/14851, Loss: 0.0014826046535745263\n",
      "Batch 1116/14851, Loss: 0.0007047044928185642\n",
      "Batch 1117/14851, Loss: 0.007508414331823587\n",
      "Batch 1118/14851, Loss: 0.025171127170324326\n",
      "Batch 1119/14851, Loss: 0.015262703411281109\n",
      "Batch 1120/14851, Loss: 0.003916742745786905\n",
      "Batch 1121/14851, Loss: 0.0010236738016828895\n",
      "Batch 1122/14851, Loss: 0.0041990443132817745\n",
      "Batch 1123/14851, Loss: 0.0006907376227900386\n",
      "Batch 1124/14851, Loss: 0.0018201654311269522\n",
      "Batch 1125/14851, Loss: 0.007997683249413967\n",
      "Batch 1126/14851, Loss: 0.0015753431944176555\n",
      "Batch 1127/14851, Loss: 0.0021335184574127197\n",
      "Batch 1128/14851, Loss: 0.002784453332424164\n",
      "Batch 1129/14851, Loss: 0.0003102533519268036\n",
      "Batch 1130/14851, Loss: 0.0011716908775269985\n",
      "Batch 1131/14851, Loss: 0.0057480717077851295\n",
      "Batch 1132/14851, Loss: 0.022507494315505028\n",
      "Batch 1133/14851, Loss: 0.05089401826262474\n",
      "Batch 1134/14851, Loss: 8.357191836694255e-05\n",
      "Batch 1135/14851, Loss: 0.0007501256186515093\n",
      "Batch 1136/14851, Loss: 2.035250327026006e-05\n",
      "Batch 1137/14851, Loss: 0.0008099141414277256\n",
      "Batch 1138/14851, Loss: 0.0016177978832274675\n",
      "Batch 1139/14851, Loss: 0.0026314929127693176\n",
      "Batch 1140/14851, Loss: 0.004197476897388697\n",
      "Batch 1141/14851, Loss: 0.03786803409457207\n",
      "Batch 1142/14851, Loss: 0.000491686281748116\n",
      "Batch 1143/14851, Loss: 0.0029684442561119795\n",
      "Batch 1144/14851, Loss: 0.0016310909995809197\n",
      "Batch 1145/14851, Loss: 0.04231368005275726\n",
      "Batch 1146/14851, Loss: 0.00046945115900598466\n",
      "Batch 1147/14851, Loss: 0.00349755329079926\n",
      "Batch 1148/14851, Loss: 0.0010931810829788446\n",
      "Batch 1149/14851, Loss: 0.0017492460319772363\n",
      "Batch 1150/14851, Loss: 0.006556197535246611\n",
      "Batch 1151/14851, Loss: 0.01192412432283163\n",
      "Batch 1152/14851, Loss: 0.0014935806393623352\n",
      "Batch 1153/14851, Loss: 2.5956580429919995e-05\n",
      "Batch 1154/14851, Loss: 3.1460076570510864e-05\n",
      "Batch 1155/14851, Loss: 0.03455423191189766\n",
      "Batch 1156/14851, Loss: 0.0031701885163784027\n",
      "Batch 1157/14851, Loss: 0.006498789880424738\n",
      "Batch 1158/14851, Loss: 0.001677683205343783\n",
      "Batch 1159/14851, Loss: 0.032471783459186554\n",
      "Batch 1160/14851, Loss: 0.0035211907234042883\n",
      "Batch 1161/14851, Loss: 0.007134633604437113\n",
      "Batch 1162/14851, Loss: 0.0043557994067668915\n",
      "Batch 1163/14851, Loss: 6.158152973512188e-05\n",
      "Batch 1164/14851, Loss: 4.892548076895764e-06\n",
      "Batch 1165/14851, Loss: 0.004547571763396263\n",
      "Batch 1166/14851, Loss: 0.0018703639507293701\n",
      "Batch 1167/14851, Loss: 0.0015521608293056488\n",
      "Batch 1168/14851, Loss: 0.000683984428178519\n",
      "Batch 1169/14851, Loss: 1.436596085113706e-05\n",
      "Batch 1170/14851, Loss: 0.001209026901051402\n",
      "Batch 1171/14851, Loss: 0.00619914336130023\n",
      "Batch 1172/14851, Loss: 0.00445789797231555\n",
      "Batch 1173/14851, Loss: 4.1894614696502686e-05\n",
      "Batch 1174/14851, Loss: 3.629053753684275e-05\n",
      "Batch 1175/14851, Loss: 6.745880091330037e-05\n",
      "Batch 1176/14851, Loss: 0.0005820629303343594\n",
      "Batch 1177/14851, Loss: 0.0002454295754432678\n",
      "Batch 1178/14851, Loss: 0.0006661842344328761\n",
      "Batch 1179/14851, Loss: 0.0005885293358005583\n",
      "Batch 1180/14851, Loss: 0.010778875090181828\n",
      "Batch 1181/14851, Loss: 0.0023614210076630116\n",
      "Batch 1182/14851, Loss: 5.187590795685537e-05\n",
      "Batch 1183/14851, Loss: 1.3089428648527246e-05\n",
      "Batch 1184/14851, Loss: 0.0011851367307826877\n",
      "Batch 1185/14851, Loss: 0.05876089632511139\n",
      "Batch 1186/14851, Loss: 0.0004781559109687805\n",
      "Batch 1187/14851, Loss: 0.02058243565261364\n",
      "Batch 1188/14851, Loss: 0.00035435918834991753\n",
      "Batch 1189/14851, Loss: 0.031433966010808945\n",
      "Batch 1190/14851, Loss: 0.00020323097123764455\n",
      "Batch 1191/14851, Loss: 5.251655602478422e-05\n",
      "Batch 1192/14851, Loss: 0.023414766415953636\n",
      "Batch 1193/14851, Loss: 0.0021073599345982075\n",
      "Batch 1194/14851, Loss: 6.765251600882038e-05\n",
      "Batch 1195/14851, Loss: 0.022729510441422462\n",
      "Batch 1196/14851, Loss: 0.0343395434319973\n",
      "Batch 1197/14851, Loss: 0.015802541747689247\n",
      "Batch 1198/14851, Loss: 0.003878336399793625\n",
      "Batch 1199/14851, Loss: 0.0017976502422243357\n",
      "Batch 1200/14851, Loss: 0.01297986414283514\n",
      "Batch 1201/14851, Loss: 0.006400102283805609\n",
      "Batch 1202/14851, Loss: 0.0003156334860250354\n",
      "Batch 1203/14851, Loss: 0.0013113729655742645\n",
      "Batch 1204/14851, Loss: 0.007367176003754139\n",
      "Batch 1205/14851, Loss: 0.0034358028788119555\n",
      "Batch 1206/14851, Loss: 5.258868259261362e-05\n",
      "Batch 1207/14851, Loss: 0.019496306777000427\n",
      "Batch 1208/14851, Loss: 0.00023414194583892822\n",
      "Batch 1209/14851, Loss: 0.003990786615759134\n",
      "Batch 1210/14851, Loss: 0.03768526762723923\n",
      "Batch 1211/14851, Loss: 0.018126728013157845\n",
      "Batch 1212/14851, Loss: 0.04288974776864052\n",
      "Batch 1213/14851, Loss: 0.000897590653039515\n",
      "Batch 1214/14851, Loss: 0.0001752744137775153\n",
      "Batch 1215/14851, Loss: 0.0005072435596957803\n",
      "Batch 1216/14851, Loss: 0.03500233218073845\n",
      "Batch 1217/14851, Loss: 3.307809311081655e-05\n",
      "Batch 1218/14851, Loss: 0.007256506476551294\n",
      "Batch 1219/14851, Loss: 0.00015234202146530151\n",
      "Batch 1220/14851, Loss: 0.07720401138067245\n",
      "Batch 1221/14851, Loss: 0.008378577418625355\n",
      "Batch 1222/14851, Loss: 0.005826530512422323\n",
      "Batch 1223/14851, Loss: 0.018549639731645584\n",
      "Batch 1224/14851, Loss: 0.008022519759833813\n",
      "Batch 1225/14851, Loss: 0.015059052966535091\n",
      "Batch 1226/14851, Loss: 0.00016643355775158852\n",
      "Batch 1227/14851, Loss: 0.005540976766496897\n",
      "Batch 1228/14851, Loss: 0.00018710439326241612\n",
      "Batch 1229/14851, Loss: 0.00048617273569107056\n",
      "Batch 1230/14851, Loss: 0.12417884916067123\n",
      "Batch 1231/14851, Loss: 0.018611591309309006\n",
      "Batch 1232/14851, Loss: 0.0009059607982635498\n",
      "Batch 1233/14851, Loss: 0.006554866675287485\n",
      "Batch 1234/14851, Loss: 0.01582138054072857\n",
      "Batch 1235/14851, Loss: 0.029005149379372597\n",
      "Batch 1236/14851, Loss: 0.00841576512902975\n",
      "Batch 1237/14851, Loss: 0.001673231483437121\n",
      "Batch 1238/14851, Loss: 0.008678465150296688\n",
      "Batch 1239/14851, Loss: 0.00070989626692608\n",
      "Batch 1240/14851, Loss: 0.025768840685486794\n",
      "Batch 1241/14851, Loss: 0.0002640994789544493\n",
      "Batch 1242/14851, Loss: 0.0056367116048932076\n",
      "Batch 1243/14851, Loss: 0.03853315860033035\n",
      "Batch 1244/14851, Loss: 0.00530640734359622\n",
      "Batch 1245/14851, Loss: 0.0004896372556686401\n",
      "Batch 1246/14851, Loss: 0.0005429349839687347\n",
      "Batch 1247/14851, Loss: 0.0031315814703702927\n",
      "Batch 1248/14851, Loss: 0.0034526751842349768\n",
      "Batch 1249/14851, Loss: 0.00017707918595988303\n",
      "Batch 1250/14851, Loss: 0.05287870764732361\n",
      "Batch 1251/14851, Loss: 0.021621868014335632\n",
      "Batch 1252/14851, Loss: 0.06267928332090378\n",
      "Batch 1253/14851, Loss: 0.02303251251578331\n",
      "Batch 1254/14851, Loss: 0.016664914786815643\n",
      "Batch 1255/14851, Loss: 0.041857052594423294\n",
      "Batch 1256/14851, Loss: 0.006400623824447393\n",
      "Batch 1257/14851, Loss: 0.0004437615571077913\n",
      "Batch 1258/14851, Loss: 0.011026143096387386\n",
      "Batch 1259/14851, Loss: 0.009314763359725475\n",
      "Batch 1260/14851, Loss: 0.0002600124862510711\n",
      "Batch 1261/14851, Loss: 0.0003084056079387665\n",
      "Batch 1262/14851, Loss: 0.004385945852845907\n",
      "Batch 1263/14851, Loss: 0.00015301381063181907\n",
      "Batch 1264/14851, Loss: 0.0015860364073887467\n",
      "Batch 1265/14851, Loss: 9.214293822878972e-05\n",
      "Batch 1266/14851, Loss: 0.02844499796628952\n",
      "Batch 1267/14851, Loss: 0.0011174437822774053\n",
      "Batch 1268/14851, Loss: 0.004594702273607254\n",
      "Batch 1269/14851, Loss: 0.006679164711385965\n",
      "Batch 1270/14851, Loss: 0.0010776740964502096\n",
      "Batch 1271/14851, Loss: 0.0015991851687431335\n",
      "Batch 1272/14851, Loss: 0.0026563305873423815\n",
      "Batch 1273/14851, Loss: 0.013411013409495354\n",
      "Batch 1274/14851, Loss: 0.001363579765893519\n",
      "Batch 1275/14851, Loss: 0.023710137233138084\n",
      "Batch 1276/14851, Loss: 0.028698012232780457\n",
      "Batch 1277/14851, Loss: 0.004345552530139685\n",
      "Batch 1278/14851, Loss: 0.032563094049692154\n",
      "Batch 1279/14851, Loss: 0.00495877442881465\n",
      "Batch 1280/14851, Loss: 0.0023319381289184093\n",
      "Batch 1281/14851, Loss: 0.028265342116355896\n",
      "Batch 1282/14851, Loss: 0.03859073296189308\n",
      "Batch 1283/14851, Loss: 0.0012256596237421036\n",
      "Batch 1284/14851, Loss: 0.019708875566720963\n",
      "Batch 1285/14851, Loss: 0.00016278401017189026\n",
      "Batch 1286/14851, Loss: 0.00020427269919309765\n",
      "Batch 1287/14851, Loss: 0.0004212732019368559\n",
      "Batch 1288/14851, Loss: 0.021990632638335228\n",
      "Batch 1289/14851, Loss: 0.006621122360229492\n",
      "Batch 1290/14851, Loss: 0.0027447380125522614\n",
      "Batch 1291/14851, Loss: 0.0037524583749473095\n",
      "Batch 1292/14851, Loss: 0.0012172677088528872\n",
      "Batch 1293/14851, Loss: 0.01203390583395958\n",
      "Batch 1294/14851, Loss: 0.0006061115418560803\n",
      "Batch 1295/14851, Loss: 0.0037291396874934435\n",
      "Batch 1296/14851, Loss: 0.0011506716255098581\n",
      "Batch 1297/14851, Loss: 0.005369392689317465\n",
      "Batch 1298/14851, Loss: 0.04989292100071907\n",
      "Batch 1299/14851, Loss: 0.001088254153728485\n",
      "Batch 1300/14851, Loss: 0.00020098313689231873\n",
      "Batch 1301/14851, Loss: 0.006748674903064966\n",
      "Batch 1302/14851, Loss: 0.0001599138486199081\n",
      "Batch 1303/14851, Loss: 0.018810059875249863\n",
      "Batch 1304/14851, Loss: 0.012809311039745808\n",
      "Batch 1305/14851, Loss: 0.04669306427240372\n",
      "Batch 1306/14851, Loss: 0.00013436500739771873\n",
      "Batch 1307/14851, Loss: 0.002875586273148656\n",
      "Batch 1308/14851, Loss: 0.003872375935316086\n",
      "Batch 1309/14851, Loss: 0.0599994882941246\n",
      "Batch 1310/14851, Loss: 9.810924530029297e-05\n",
      "Batch 1311/14851, Loss: 0.009786056354641914\n",
      "Batch 1312/14851, Loss: 0.008066359907388687\n",
      "Batch 1313/14851, Loss: 0.020767850801348686\n",
      "Batch 1314/14851, Loss: 0.011935796588659286\n",
      "Batch 1315/14851, Loss: 7.837017619749531e-05\n",
      "Batch 1316/14851, Loss: 0.0026183067820966244\n",
      "Batch 1317/14851, Loss: 0.0026602260768413544\n",
      "Batch 1318/14851, Loss: 0.006231511943042278\n",
      "Batch 1319/14851, Loss: 0.004406656138598919\n",
      "Batch 1320/14851, Loss: 0.005277507938444614\n",
      "Batch 1321/14851, Loss: 0.0008067438029684126\n",
      "Batch 1322/14851, Loss: 0.003273952752351761\n",
      "Batch 1323/14851, Loss: 0.007714784238487482\n",
      "Batch 1324/14851, Loss: 0.0017320154001936316\n",
      "Batch 1325/14851, Loss: 0.00024888417101465166\n",
      "Batch 1326/14851, Loss: 0.00044320712913759053\n",
      "Batch 1327/14851, Loss: 0.0014495551586151123\n",
      "Batch 1328/14851, Loss: 8.990243077278137e-05\n",
      "Batch 1329/14851, Loss: 0.0018220847705379128\n",
      "Batch 1330/14851, Loss: 0.01057970616966486\n",
      "Batch 1331/14851, Loss: 0.08568896353244781\n",
      "Batch 1332/14851, Loss: 0.0002355206524953246\n",
      "Batch 1333/14851, Loss: 0.033325377851724625\n",
      "Batch 1334/14851, Loss: 0.04025350883603096\n",
      "Batch 1335/14851, Loss: 0.001778431236743927\n",
      "Batch 1336/14851, Loss: 0.001844797283411026\n",
      "Batch 1337/14851, Loss: 0.0012654587626457214\n",
      "Batch 1338/14851, Loss: 0.003065250813961029\n",
      "Batch 1339/14851, Loss: 0.003863665508106351\n",
      "Batch 1340/14851, Loss: 0.013949618674814701\n",
      "Batch 1341/14851, Loss: 0.00634142616763711\n",
      "Batch 1342/14851, Loss: 0.0011744837975129485\n",
      "Batch 1343/14851, Loss: 0.001806635526008904\n",
      "Batch 1344/14851, Loss: 0.015821315348148346\n",
      "Batch 1345/14851, Loss: 0.007989903911948204\n",
      "Batch 1346/14851, Loss: 0.0017004678957164288\n",
      "Batch 1347/14851, Loss: 0.04194112494587898\n",
      "Batch 1348/14851, Loss: 0.0002203322947025299\n",
      "Batch 1349/14851, Loss: 0.04083113744854927\n",
      "Batch 1350/14851, Loss: 3.058090806007385e-05\n",
      "Batch 1351/14851, Loss: 0.04270961508154869\n",
      "Batch 1352/14851, Loss: 9.386862075189129e-05\n",
      "Batch 1353/14851, Loss: 0.014664653688669205\n",
      "Batch 1354/14851, Loss: 0.005419685039669275\n",
      "Batch 1355/14851, Loss: 9.288141882279888e-05\n",
      "Batch 1356/14851, Loss: 0.005399263463914394\n",
      "Batch 1357/14851, Loss: 0.002676970325410366\n",
      "Batch 1358/14851, Loss: 0.0022655606735497713\n",
      "Batch 1359/14851, Loss: 5.634998160530813e-05\n",
      "Batch 1360/14851, Loss: 0.0037980079650878906\n",
      "Batch 1361/14851, Loss: 0.0031084008514881134\n",
      "Batch 1362/14851, Loss: 0.014493048191070557\n",
      "Batch 1363/14851, Loss: 0.0001033159569487907\n",
      "Batch 1364/14851, Loss: 0.002745436504483223\n",
      "Batch 1365/14851, Loss: 0.0010693470248952508\n",
      "Batch 1366/14851, Loss: 0.00046219429350458086\n",
      "Batch 1367/14851, Loss: 0.004296461585909128\n",
      "Batch 1368/14851, Loss: 0.04360640048980713\n",
      "Batch 1369/14851, Loss: 0.008619026280939579\n",
      "Batch 1370/14851, Loss: 8.295724546769634e-05\n",
      "Batch 1371/14851, Loss: 0.015092910267412663\n",
      "Batch 1372/14851, Loss: 0.0008524380973540246\n",
      "Batch 1373/14851, Loss: 0.0014185048639774323\n",
      "Batch 1374/14851, Loss: 0.0031303842552006245\n",
      "Batch 1375/14851, Loss: 0.03570124879479408\n",
      "Batch 1376/14851, Loss: 0.036330446600914\n",
      "Batch 1377/14851, Loss: 0.0008238653535954654\n",
      "Batch 1378/14851, Loss: 0.00036409744643606246\n",
      "Batch 1379/14851, Loss: 0.007669365499168634\n",
      "Batch 1380/14851, Loss: 0.00965034682303667\n",
      "Batch 1381/14851, Loss: 0.001502770814113319\n",
      "Batch 1382/14851, Loss: 2.2113323211669922e-05\n",
      "Batch 1383/14851, Loss: 0.001914030290208757\n",
      "Batch 1384/14851, Loss: 0.0003095386200584471\n",
      "Batch 1385/14851, Loss: 0.020751863718032837\n",
      "Batch 1386/14851, Loss: 0.0007994066691026092\n",
      "Batch 1387/14851, Loss: 7.610892498632893e-05\n",
      "Batch 1388/14851, Loss: 0.0010547898709774017\n",
      "Batch 1389/14851, Loss: 0.05458778515458107\n",
      "Batch 1390/14851, Loss: 1.3124197721481323e-05\n",
      "Batch 1391/14851, Loss: 0.020763246342539787\n",
      "Batch 1392/14851, Loss: 0.02242995798587799\n",
      "Batch 1393/14851, Loss: 0.003483641892671585\n",
      "Batch 1394/14851, Loss: 0.0687890499830246\n",
      "Batch 1395/14851, Loss: 0.001171621261164546\n",
      "Batch 1396/14851, Loss: 0.0007999713416211307\n",
      "Batch 1397/14851, Loss: 0.00015921021986287087\n",
      "Batch 1398/14851, Loss: 0.0022650468163192272\n",
      "Batch 1399/14851, Loss: 6.579110777238384e-05\n",
      "Batch 1400/14851, Loss: 0.01126838568598032\n",
      "Batch 1401/14851, Loss: 6.588424002984539e-05\n",
      "Batch 1402/14851, Loss: 0.00010422244668006897\n",
      "Batch 1403/14851, Loss: 0.001741122454404831\n",
      "Batch 1404/14851, Loss: 0.024538733065128326\n",
      "Batch 1405/14851, Loss: 0.04580402374267578\n",
      "Batch 1406/14851, Loss: 0.0003161235654260963\n",
      "Batch 1407/14851, Loss: 0.0008084650035016239\n",
      "Batch 1408/14851, Loss: 0.037352554500103\n",
      "Batch 1409/14851, Loss: 0.051703307777643204\n",
      "Batch 1410/14851, Loss: 0.00024661794304847717\n",
      "Batch 1411/14851, Loss: 0.04316699132323265\n",
      "Batch 1412/14851, Loss: 0.07428230345249176\n",
      "Batch 1413/14851, Loss: 0.0008765670354478061\n",
      "Batch 1414/14851, Loss: 0.035243626683950424\n",
      "Batch 1415/14851, Loss: 0.019776662811636925\n",
      "Batch 1416/14851, Loss: 7.990995800355449e-05\n",
      "Batch 1417/14851, Loss: 0.012110695242881775\n",
      "Batch 1418/14851, Loss: 0.004275809973478317\n",
      "Batch 1419/14851, Loss: 0.005030822474509478\n",
      "Batch 1420/14851, Loss: 0.0011599274585023522\n",
      "Batch 1421/14851, Loss: 0.052134521305561066\n",
      "Batch 1422/14851, Loss: 0.0002524802985135466\n",
      "Batch 1423/14851, Loss: 0.02961484156548977\n",
      "Batch 1424/14851, Loss: 0.003107228549197316\n",
      "Batch 1425/14851, Loss: 0.010744270868599415\n",
      "Batch 1426/14851, Loss: 0.0006739000673405826\n",
      "Batch 1427/14851, Loss: 0.009750325232744217\n",
      "Batch 1428/14851, Loss: 0.007280492689460516\n",
      "Batch 1429/14851, Loss: 0.0020646564662456512\n",
      "Batch 1430/14851, Loss: 0.0018507055938243866\n",
      "Batch 1431/14851, Loss: 0.04118039086461067\n",
      "Batch 1432/14851, Loss: 0.007930382154881954\n",
      "Batch 1433/14851, Loss: 0.0009755119681358337\n",
      "Batch 1434/14851, Loss: 0.004071990959346294\n",
      "Batch 1435/14851, Loss: 0.00018381451081950217\n",
      "Batch 1436/14851, Loss: 0.03896036371588707\n",
      "Batch 1437/14851, Loss: 0.006977892015129328\n",
      "Batch 1438/14851, Loss: 0.0005015854258090258\n",
      "Batch 1439/14851, Loss: 0.0051848930306732655\n",
      "Batch 1440/14851, Loss: 0.06953924894332886\n",
      "Batch 1441/14851, Loss: 0.007190066855400801\n",
      "Batch 1442/14851, Loss: 0.06699884682893753\n",
      "Batch 1443/14851, Loss: 0.051166076213121414\n",
      "Batch 1444/14851, Loss: 0.0003574639558792114\n",
      "Batch 1445/14851, Loss: 0.003946057055145502\n",
      "Batch 1446/14851, Loss: 0.007831309922039509\n",
      "Batch 1447/14851, Loss: 0.0012915320694446564\n",
      "Batch 1448/14851, Loss: 0.044367533177137375\n",
      "Batch 1449/14851, Loss: 0.004597117658704519\n",
      "Batch 1450/14851, Loss: 0.0007968805730342865\n",
      "Batch 1451/14851, Loss: 0.0029466396663337946\n",
      "Batch 1452/14851, Loss: 0.0019607667345553637\n",
      "Batch 1453/14851, Loss: 0.007124366704374552\n",
      "Batch 1454/14851, Loss: 0.022890692576766014\n",
      "Batch 1455/14851, Loss: 0.006590370088815689\n",
      "Batch 1456/14851, Loss: 0.0468737818300724\n",
      "Batch 1457/14851, Loss: 0.00010053813457489014\n",
      "Batch 1458/14851, Loss: 0.0031344026792794466\n",
      "Batch 1459/14851, Loss: 0.0022556823678314686\n",
      "Batch 1460/14851, Loss: 0.02378116175532341\n",
      "Batch 1461/14851, Loss: 0.00037814551615156233\n",
      "Batch 1462/14851, Loss: 0.003477036952972412\n",
      "Batch 1463/14851, Loss: 0.009042074903845787\n",
      "Batch 1464/14851, Loss: 0.00044394948054105043\n",
      "Batch 1465/14851, Loss: 0.006353157572448254\n",
      "Batch 1466/14851, Loss: 0.0016275495290756226\n",
      "Batch 1467/14851, Loss: 0.00023569042969029397\n",
      "Batch 1468/14851, Loss: 0.00010005136573454365\n",
      "Batch 1469/14851, Loss: 0.007100476883351803\n",
      "Batch 1470/14851, Loss: 0.03429998829960823\n",
      "Batch 1471/14851, Loss: 0.11962271481752396\n",
      "Batch 1472/14851, Loss: 0.0015060831792652607\n",
      "Batch 1473/14851, Loss: 0.005373232066631317\n",
      "Batch 1474/14851, Loss: 0.026194842532277107\n",
      "Batch 1475/14851, Loss: 0.004341540392488241\n",
      "Batch 1476/14851, Loss: 0.003119541797786951\n",
      "Batch 1477/14851, Loss: 0.019360940903425217\n",
      "Batch 1478/14851, Loss: 0.030268119648098946\n",
      "Batch 1479/14851, Loss: 0.06137809902429581\n",
      "Batch 1480/14851, Loss: 0.04518306255340576\n",
      "Batch 1481/14851, Loss: 0.00021506349730771035\n",
      "Batch 1482/14851, Loss: 5.5113185226218775e-05\n",
      "Batch 1483/14851, Loss: 0.0036749292630702257\n",
      "Batch 1484/14851, Loss: 0.0006706401472911239\n",
      "Batch 1485/14851, Loss: 0.0004624811408575624\n",
      "Batch 1486/14851, Loss: 0.0007695415406487882\n",
      "Batch 1487/14851, Loss: 0.0004758313298225403\n",
      "Batch 1488/14851, Loss: 0.0005399485235102475\n",
      "Batch 1489/14851, Loss: 0.00125705951359123\n",
      "Batch 1490/14851, Loss: 0.0027002717833966017\n",
      "Batch 1491/14851, Loss: 0.02110871858894825\n",
      "Batch 1492/14851, Loss: 0.0010079555213451385\n",
      "Batch 1493/14851, Loss: 0.0002476970257703215\n",
      "Batch 1494/14851, Loss: 0.005708161275833845\n",
      "Batch 1495/14851, Loss: 0.023223107680678368\n",
      "Batch 1496/14851, Loss: 0.0007399357855319977\n",
      "Batch 1497/14851, Loss: 0.0030873145442456007\n",
      "Batch 1498/14851, Loss: 0.025603938847780228\n",
      "Batch 1499/14851, Loss: 0.00021791458129882812\n",
      "Batch 1500/14851, Loss: 0.006408839486539364\n",
      "Batch 1501/14851, Loss: 0.004004348069429398\n",
      "Batch 1502/14851, Loss: 0.002524635521695018\n",
      "Batch 1503/14851, Loss: 0.036332860589027405\n",
      "Batch 1504/14851, Loss: 0.058410242199897766\n",
      "Batch 1505/14851, Loss: 0.0015339121455326676\n",
      "Batch 1506/14851, Loss: 0.11606518179178238\n",
      "Batch 1507/14851, Loss: 0.0581238679587841\n",
      "Batch 1508/14851, Loss: 0.001034798682667315\n",
      "Batch 1509/14851, Loss: 0.0027256645262241364\n",
      "Batch 1510/14851, Loss: 0.0030215680599212646\n",
      "Batch 1511/14851, Loss: 0.037897564470767975\n",
      "Batch 1512/14851, Loss: 0.001761124818585813\n",
      "Batch 1513/14851, Loss: 0.005709526129066944\n",
      "Batch 1514/14851, Loss: 0.00756583409383893\n",
      "Batch 1515/14851, Loss: 0.0005658765439875424\n",
      "Batch 1516/14851, Loss: 0.0022350114304572344\n",
      "Batch 1517/14851, Loss: 0.005010218825191259\n",
      "Batch 1518/14851, Loss: 0.015106688253581524\n",
      "Batch 1519/14851, Loss: 0.0374179370701313\n",
      "Batch 1520/14851, Loss: 0.00026634582900442183\n",
      "Batch 1521/14851, Loss: 0.05023931339383125\n",
      "Batch 1522/14851, Loss: 0.004727926105260849\n",
      "Batch 1523/14851, Loss: 0.03067641146481037\n",
      "Batch 1524/14851, Loss: 0.001921002403832972\n",
      "Batch 1525/14851, Loss: 0.016258737072348595\n",
      "Batch 1526/14851, Loss: 0.0047046891413629055\n",
      "Batch 1527/14851, Loss: 0.00524143548682332\n",
      "Batch 1528/14851, Loss: 0.033786918967962265\n",
      "Batch 1529/14851, Loss: 0.0514165423810482\n",
      "Batch 1530/14851, Loss: 0.0047686295583844185\n",
      "Batch 1531/14851, Loss: 0.06402582675218582\n",
      "Batch 1532/14851, Loss: 0.004182548727840185\n",
      "Batch 1533/14851, Loss: 0.01643608883023262\n",
      "Batch 1534/14851, Loss: 0.006316902581602335\n",
      "Batch 1535/14851, Loss: 0.04107070714235306\n",
      "Batch 1536/14851, Loss: 0.00042346763075329363\n",
      "Batch 1537/14851, Loss: 0.002766798250377178\n",
      "Batch 1538/14851, Loss: 0.00011520460247993469\n",
      "Batch 1539/14851, Loss: 0.004624370019882917\n",
      "Batch 1540/14851, Loss: 0.0014415796613320708\n",
      "Batch 1541/14851, Loss: 0.025578653439879417\n",
      "Batch 1542/14851, Loss: 0.027441222220659256\n",
      "Batch 1543/14851, Loss: 0.0007882590289227664\n",
      "Batch 1544/14851, Loss: 0.001967215444892645\n",
      "Batch 1545/14851, Loss: 0.0014393242308869958\n",
      "Batch 1546/14851, Loss: 0.00028291219496168196\n",
      "Batch 1547/14851, Loss: 0.040310490876436234\n",
      "Batch 1548/14851, Loss: 0.01868138648569584\n",
      "Batch 1549/14851, Loss: 0.019154345616698265\n",
      "Batch 1550/14851, Loss: 0.06027054786682129\n",
      "Batch 1551/14851, Loss: 0.028977608308196068\n",
      "Batch 1552/14851, Loss: 0.0026502597611397505\n",
      "Batch 1553/14851, Loss: 0.017313621938228607\n",
      "Batch 1554/14851, Loss: 0.04267844557762146\n",
      "Batch 1555/14851, Loss: 0.014301393181085587\n",
      "Batch 1556/14851, Loss: 0.0051261321641504765\n",
      "Batch 1557/14851, Loss: 0.020113876089453697\n",
      "Batch 1558/14851, Loss: 0.007530284579843283\n",
      "Batch 1559/14851, Loss: 0.005727850366383791\n",
      "Batch 1560/14851, Loss: 0.0015932302922010422\n",
      "Batch 1561/14851, Loss: 0.012153581716120243\n",
      "Batch 1562/14851, Loss: 0.011838635429739952\n",
      "Batch 1563/14851, Loss: 0.01793505996465683\n",
      "Batch 1564/14851, Loss: 0.0038306887727230787\n",
      "Batch 1565/14851, Loss: 0.0007937301998026669\n",
      "Batch 1566/14851, Loss: 0.028664536774158478\n",
      "Batch 1567/14851, Loss: 0.00038763508200645447\n",
      "Batch 1568/14851, Loss: 0.001811755821108818\n",
      "Batch 1569/14851, Loss: 0.019665714353322983\n",
      "Batch 1570/14851, Loss: 0.0019439061870798469\n",
      "Batch 1571/14851, Loss: 0.013402455486357212\n",
      "Batch 1572/14851, Loss: 0.0018976157298311591\n",
      "Batch 1573/14851, Loss: 0.009036188945174217\n",
      "Batch 1574/14851, Loss: 0.00014324784569907933\n",
      "Batch 1575/14851, Loss: 0.02391292341053486\n",
      "Batch 1576/14851, Loss: 0.029960034415125847\n",
      "Batch 1577/14851, Loss: 0.0036763320676982403\n",
      "Batch 1578/14851, Loss: 0.005409630481153727\n",
      "Batch 1579/14851, Loss: 0.0008375358884222806\n",
      "Batch 1580/14851, Loss: 0.0023031868040561676\n",
      "Batch 1581/14851, Loss: 0.0013098703930154443\n",
      "Batch 1582/14851, Loss: 0.008393530733883381\n",
      "Batch 1583/14851, Loss: 0.04732209071516991\n",
      "Batch 1584/14851, Loss: 0.0014423668617382646\n",
      "Batch 1585/14851, Loss: 0.0064729200676083565\n",
      "Batch 1586/14851, Loss: 0.020882021635770798\n",
      "Batch 1587/14851, Loss: 0.0030952240340411663\n",
      "Batch 1588/14851, Loss: 0.017983440309762955\n",
      "Batch 1589/14851, Loss: 0.016810627654194832\n",
      "Batch 1590/14851, Loss: 0.01880170777440071\n",
      "Batch 1591/14851, Loss: 0.045837998390197754\n",
      "Batch 1592/14851, Loss: 0.002638604026287794\n",
      "Batch 1593/14851, Loss: 0.06110454723238945\n",
      "Batch 1594/14851, Loss: 0.023762930184602737\n",
      "Batch 1595/14851, Loss: 0.03604045510292053\n",
      "Batch 1596/14851, Loss: 0.0026145235169678926\n",
      "Batch 1597/14851, Loss: 0.006737449672073126\n",
      "Batch 1598/14851, Loss: 0.0012175192823633552\n",
      "Batch 1599/14851, Loss: 0.0024625808000564575\n",
      "Batch 1600/14851, Loss: 0.0012053685495629907\n",
      "Batch 1601/14851, Loss: 0.007279888726770878\n",
      "Batch 1602/14851, Loss: 0.013194262981414795\n",
      "Batch 1603/14851, Loss: 0.0015601679915562272\n",
      "Batch 1604/14851, Loss: 0.0001055064276442863\n",
      "Batch 1605/14851, Loss: 0.002791714621707797\n",
      "Batch 1606/14851, Loss: 0.0005779813509434462\n",
      "Batch 1607/14851, Loss: 0.010397474281489849\n",
      "Batch 1608/14851, Loss: 0.03321079537272453\n",
      "Batch 1609/14851, Loss: 0.0005244587664492428\n",
      "Batch 1610/14851, Loss: 0.005689781624823809\n",
      "Batch 1611/14851, Loss: 0.001984480768442154\n",
      "Batch 1612/14851, Loss: 0.0022030379623174667\n",
      "Batch 1613/14851, Loss: 0.023470697924494743\n",
      "Batch 1614/14851, Loss: 0.02383407950401306\n",
      "Batch 1615/14851, Loss: 0.0009049922227859497\n",
      "Batch 1616/14851, Loss: 0.010604288429021835\n",
      "Batch 1617/14851, Loss: 0.007259921636432409\n",
      "Batch 1618/14851, Loss: 0.01702694036066532\n",
      "Batch 1619/14851, Loss: 0.026321319863200188\n",
      "Batch 1620/14851, Loss: 0.001046558958478272\n",
      "Batch 1621/14851, Loss: 0.007579994387924671\n",
      "Batch 1622/14851, Loss: 0.06705738604068756\n",
      "Batch 1623/14851, Loss: 0.002760230330750346\n",
      "Batch 1624/14851, Loss: 0.009611831046640873\n",
      "Batch 1625/14851, Loss: 0.013705580495297909\n",
      "Batch 1626/14851, Loss: 0.0052360245026648045\n",
      "Batch 1627/14851, Loss: 0.024987269192934036\n",
      "Batch 1628/14851, Loss: 0.002752188593149185\n",
      "Batch 1629/14851, Loss: 0.00016965344548225403\n",
      "Batch 1630/14851, Loss: 0.02873043715953827\n",
      "Batch 1631/14851, Loss: 0.0330682173371315\n",
      "Batch 1632/14851, Loss: 0.009332836605608463\n",
      "Batch 1633/14851, Loss: 0.03772877901792526\n",
      "Batch 1634/14851, Loss: 0.004037655424326658\n",
      "Batch 1635/14851, Loss: 0.011416626162827015\n",
      "Batch 1636/14851, Loss: 0.03681688755750656\n",
      "Batch 1637/14851, Loss: 0.007389265112578869\n",
      "Batch 1638/14851, Loss: 0.005663577001541853\n",
      "Batch 1639/14851, Loss: 0.013222983106970787\n",
      "Batch 1640/14851, Loss: 0.011858264915645123\n",
      "Batch 1641/14851, Loss: 0.05053676664829254\n",
      "Batch 1642/14851, Loss: 0.0012432547518983483\n",
      "Batch 1643/14851, Loss: 0.020359614863991737\n",
      "Batch 1644/14851, Loss: 0.025125183165073395\n",
      "Batch 1645/14851, Loss: 0.009733954444527626\n",
      "Batch 1646/14851, Loss: 0.032850056886672974\n",
      "Batch 1647/14851, Loss: 0.00018069198995362967\n",
      "Batch 1648/14851, Loss: 0.0006377038662321866\n",
      "Batch 1649/14851, Loss: 0.0006528322701342404\n",
      "Batch 1650/14851, Loss: 0.0006418513949029148\n",
      "Batch 1651/14851, Loss: 0.003300182521343231\n",
      "Batch 1652/14851, Loss: 0.0021324034314602613\n",
      "Batch 1653/14851, Loss: 0.0002475768269505352\n",
      "Batch 1654/14851, Loss: 0.00024258096527773887\n",
      "Batch 1655/14851, Loss: 0.0005566676263697445\n",
      "Batch 1656/14851, Loss: 0.00016683216381352395\n",
      "Batch 1657/14851, Loss: 0.0033587899524718523\n",
      "Batch 1658/14851, Loss: 0.013794837519526482\n",
      "Batch 1659/14851, Loss: 0.0038696080446243286\n",
      "Batch 1660/14851, Loss: 0.001336283516138792\n",
      "Batch 1661/14851, Loss: 0.0021492429077625275\n",
      "Batch 1662/14851, Loss: 0.04052658751606941\n",
      "Batch 1663/14851, Loss: 0.006016155239194632\n",
      "Batch 1664/14851, Loss: 0.0007470585405826569\n",
      "Batch 1665/14851, Loss: 0.003293244168162346\n",
      "Batch 1666/14851, Loss: 0.010361937806010246\n",
      "Batch 1667/14851, Loss: 0.006468161940574646\n",
      "Batch 1668/14851, Loss: 0.0006030068034306169\n",
      "Batch 1669/14851, Loss: 0.007671774830669165\n",
      "Batch 1670/14851, Loss: 0.005303016398102045\n",
      "Batch 1671/14851, Loss: 0.014479245990514755\n",
      "Batch 1672/14851, Loss: 0.028606759384274483\n",
      "Batch 1673/14851, Loss: 0.008848830126225948\n",
      "Batch 1674/14851, Loss: 0.0004388391971588135\n",
      "Batch 1675/14851, Loss: 0.0003718125226441771\n",
      "Batch 1676/14851, Loss: 0.0008103276486508548\n",
      "Batch 1677/14851, Loss: 0.06486236304044724\n",
      "Batch 1678/14851, Loss: 0.0005855399067513645\n",
      "Batch 1679/14851, Loss: 0.07377412170171738\n",
      "Batch 1680/14851, Loss: 0.02239314280450344\n",
      "Batch 1681/14851, Loss: 0.0028567612171173096\n",
      "Batch 1682/14851, Loss: 0.018268469721078873\n",
      "Batch 1683/14851, Loss: 0.01343741174787283\n",
      "Batch 1684/14851, Loss: 0.0016286037862300873\n",
      "Batch 1685/14851, Loss: 0.041344743221998215\n",
      "Batch 1686/14851, Loss: 0.0011746548116207123\n",
      "Batch 1687/14851, Loss: 0.009679075330495834\n",
      "Batch 1688/14851, Loss: 0.016994042322039604\n",
      "Batch 1689/14851, Loss: 0.006184963043779135\n",
      "Batch 1690/14851, Loss: 0.0002970360219478607\n",
      "Batch 1691/14851, Loss: 0.0009744216804392636\n",
      "Batch 1692/14851, Loss: 3.140171247650869e-05\n",
      "Batch 1693/14851, Loss: 0.01905110664665699\n",
      "Batch 1694/14851, Loss: 0.051849763840436935\n",
      "Batch 1695/14851, Loss: 0.0037788066547363997\n",
      "Batch 1696/14851, Loss: 0.012174360454082489\n",
      "Batch 1697/14851, Loss: 0.002695966511964798\n",
      "Batch 1698/14851, Loss: 0.00025731822825036943\n",
      "Batch 1699/14851, Loss: 0.011981704272329807\n",
      "Batch 1700/14851, Loss: 0.004353079944849014\n",
      "Batch 1701/14851, Loss: 0.010222355835139751\n",
      "Batch 1702/14851, Loss: 0.021075734868645668\n",
      "Batch 1703/14851, Loss: 0.006565230432897806\n",
      "Batch 1704/14851, Loss: 0.0009392251376993954\n",
      "Batch 1705/14851, Loss: 0.0003528619708959013\n",
      "Batch 1706/14851, Loss: 0.019950663670897484\n",
      "Batch 1707/14851, Loss: 0.02544211968779564\n",
      "Batch 1708/14851, Loss: 2.0679086446762085e-05\n",
      "Batch 1709/14851, Loss: 0.006115075666457415\n",
      "Batch 1710/14851, Loss: 0.02691764011979103\n",
      "Batch 1711/14851, Loss: 0.0070652589201927185\n",
      "Batch 1712/14851, Loss: 0.0020350096747279167\n",
      "Batch 1713/14851, Loss: 0.0029787432868033648\n",
      "Batch 1714/14851, Loss: 0.006931429263204336\n",
      "Batch 1715/14851, Loss: 0.012485607527196407\n",
      "Batch 1716/14851, Loss: 0.00023733451962471008\n",
      "Batch 1717/14851, Loss: 0.005961921997368336\n",
      "Batch 1718/14851, Loss: 0.0002678707242012024\n",
      "Batch 1719/14851, Loss: 0.0010168775916099548\n",
      "Batch 1720/14851, Loss: 0.002155910013243556\n",
      "Batch 1721/14851, Loss: 0.005785095505416393\n",
      "Batch 1722/14851, Loss: 0.01082734577357769\n",
      "Batch 1723/14851, Loss: 0.0004144906997680664\n",
      "Batch 1724/14851, Loss: 4.9224745453102514e-05\n",
      "Batch 1725/14851, Loss: 0.0026302668265998363\n",
      "Batch 1726/14851, Loss: 0.006863009184598923\n",
      "Batch 1727/14851, Loss: 0.004459654912352562\n",
      "Batch 1728/14851, Loss: 0.0005642634932883084\n",
      "Batch 1729/14851, Loss: 0.007946058176457882\n",
      "Batch 1730/14851, Loss: 0.0008301089401356876\n",
      "Batch 1731/14851, Loss: 0.015950972214341164\n",
      "Batch 1732/14851, Loss: 0.031040674075484276\n",
      "Batch 1733/14851, Loss: 0.0020103473216295242\n",
      "Batch 1734/14851, Loss: 0.008225949481129646\n",
      "Batch 1735/14851, Loss: 0.003131563775241375\n",
      "Batch 1736/14851, Loss: 0.00864978227764368\n",
      "Batch 1737/14851, Loss: 0.0017475956119596958\n",
      "Batch 1738/14851, Loss: 0.003769222181290388\n",
      "Batch 1739/14851, Loss: 0.0010295422980561852\n",
      "Batch 1740/14851, Loss: 9.854386007646099e-05\n",
      "Batch 1741/14851, Loss: 0.003896893234923482\n",
      "Batch 1742/14851, Loss: 0.016049031168222427\n",
      "Batch 1743/14851, Loss: 0.047004129737615585\n",
      "Batch 1744/14851, Loss: 0.03702908381819725\n",
      "Batch 1745/14851, Loss: 0.017170393839478493\n",
      "Batch 1746/14851, Loss: 0.00419007521122694\n",
      "Batch 1747/14851, Loss: 0.010338688269257545\n",
      "Batch 1748/14851, Loss: 0.00011804824316641316\n",
      "Batch 1749/14851, Loss: 0.003869591513648629\n",
      "Batch 1750/14851, Loss: 0.001421373337507248\n",
      "Batch 1751/14851, Loss: 0.006107459310442209\n",
      "Batch 1752/14851, Loss: 0.003326238365843892\n",
      "Batch 1753/14851, Loss: 0.0009901151061058044\n",
      "Batch 1754/14851, Loss: 0.04386410489678383\n",
      "Batch 1755/14851, Loss: 0.021434692665934563\n",
      "Batch 1756/14851, Loss: 0.0013954518362879753\n",
      "Batch 1757/14851, Loss: 4.404410719871521e-05\n",
      "Batch 1758/14851, Loss: 0.008233098313212395\n",
      "Batch 1759/14851, Loss: 0.007625836879014969\n",
      "Batch 1760/14851, Loss: 0.01873762719333172\n",
      "Batch 1761/14851, Loss: 0.012325958348810673\n",
      "Batch 1762/14851, Loss: 0.0008668862283229828\n",
      "Batch 1763/14851, Loss: 0.004081443417817354\n",
      "Batch 1764/14851, Loss: 0.03929157555103302\n",
      "Batch 1765/14851, Loss: 0.012393414042890072\n",
      "Batch 1766/14851, Loss: 0.0017884051194414496\n",
      "Batch 1767/14851, Loss: 0.0016804680926725268\n",
      "Batch 1768/14851, Loss: 0.0006474889814853668\n",
      "Batch 1769/14851, Loss: 0.00855825562030077\n",
      "Batch 1770/14851, Loss: 0.008244967088103294\n",
      "Batch 1771/14851, Loss: 0.00014247496437747031\n",
      "Batch 1772/14851, Loss: 0.006093858275562525\n",
      "Batch 1773/14851, Loss: 0.00041750248055905104\n",
      "Batch 1774/14851, Loss: 0.0026845585089176893\n",
      "Batch 1775/14851, Loss: 0.0003080243768636137\n",
      "Batch 1776/14851, Loss: 0.017207114025950432\n",
      "Batch 1777/14851, Loss: 0.0058534773997962475\n",
      "Batch 1778/14851, Loss: 0.00457673380151391\n",
      "Batch 1779/14851, Loss: 3.4463901101844385e-05\n",
      "Batch 1780/14851, Loss: 0.0003887377679347992\n",
      "Batch 1781/14851, Loss: 0.008695165626704693\n",
      "Batch 1782/14851, Loss: 0.03331748768687248\n",
      "Batch 1783/14851, Loss: 0.0008200432057492435\n",
      "Batch 1784/14851, Loss: 0.0014518072130158544\n",
      "Batch 1785/14851, Loss: 0.0006575224106200039\n",
      "Batch 1786/14851, Loss: 0.006248163059353828\n",
      "Batch 1787/14851, Loss: 0.009584193117916584\n",
      "Batch 1788/14851, Loss: 0.00036312639713287354\n",
      "Batch 1789/14851, Loss: 0.008652951568365097\n",
      "Batch 1790/14851, Loss: 0.035557009279727936\n",
      "Batch 1791/14851, Loss: 0.0005107124452479184\n",
      "Batch 1792/14851, Loss: 0.0027179548051208258\n",
      "Batch 1793/14851, Loss: 0.00022330631327349693\n",
      "Batch 1794/14851, Loss: 0.0027405607979744673\n",
      "Batch 1795/14851, Loss: 0.03788556158542633\n",
      "Batch 1796/14851, Loss: 0.006155786570161581\n",
      "Batch 1797/14851, Loss: 0.019453391432762146\n",
      "Batch 1798/14851, Loss: 0.006533927284181118\n",
      "Batch 1799/14851, Loss: 0.004833436571061611\n",
      "Batch 1800/14851, Loss: 0.007025163620710373\n",
      "Batch 1801/14851, Loss: 0.0005665330681949854\n",
      "Batch 1802/14851, Loss: 0.00038877400220371783\n",
      "Batch 1803/14851, Loss: 0.027158508077263832\n",
      "Batch 1804/14851, Loss: 0.022314757108688354\n",
      "Batch 1805/14851, Loss: 0.02053076960146427\n",
      "Batch 1806/14851, Loss: 0.016258180141448975\n",
      "Batch 1807/14851, Loss: 0.005662435665726662\n",
      "Batch 1808/14851, Loss: 0.002815284999087453\n",
      "Batch 1809/14851, Loss: 0.0016481628408655524\n",
      "Batch 1810/14851, Loss: 0.002269105287268758\n",
      "Batch 1811/14851, Loss: 0.002024098066613078\n",
      "Batch 1812/14851, Loss: 0.028094930574297905\n",
      "Batch 1813/14851, Loss: 0.00478352140635252\n",
      "Batch 1814/14851, Loss: 0.06072695925831795\n",
      "Batch 1815/14851, Loss: 0.004111082758754492\n",
      "Batch 1816/14851, Loss: 0.021583711728453636\n",
      "Batch 1817/14851, Loss: 0.012564189732074738\n",
      "Batch 1818/14851, Loss: 0.004993073642253876\n",
      "Batch 1819/14851, Loss: 0.00032426416873931885\n",
      "Batch 1820/14851, Loss: 0.03462711721658707\n",
      "Batch 1821/14851, Loss: 0.016487719491124153\n",
      "Batch 1822/14851, Loss: 0.0008330022101290524\n",
      "Batch 1823/14851, Loss: 0.0024924043100327253\n",
      "Batch 1824/14851, Loss: 0.001038235961459577\n",
      "Batch 1825/14851, Loss: 0.00044301897287368774\n",
      "Batch 1826/14851, Loss: 0.0010906843235716224\n",
      "Batch 1827/14851, Loss: 0.001923789968714118\n",
      "Batch 1828/14851, Loss: 0.009901649318635464\n",
      "Batch 1829/14851, Loss: 0.006761622615158558\n",
      "Batch 1830/14851, Loss: 0.004721426870673895\n",
      "Batch 1831/14851, Loss: 0.021416064351797104\n",
      "Batch 1832/14851, Loss: 0.0010290398495271802\n",
      "Batch 1833/14851, Loss: 0.031146446242928505\n",
      "Batch 1834/14851, Loss: 0.03690130263566971\n",
      "Batch 1835/14851, Loss: 0.00046971687697805464\n",
      "Batch 1836/14851, Loss: 0.0007244138396345079\n",
      "Batch 1837/14851, Loss: 0.02704625390470028\n",
      "Batch 1838/14851, Loss: 0.01288060937076807\n",
      "Batch 1839/14851, Loss: 0.008269108831882477\n",
      "Batch 1840/14851, Loss: 0.004815157037228346\n",
      "Batch 1841/14851, Loss: 0.0032519870437681675\n",
      "Batch 1842/14851, Loss: 0.041546780616045\n",
      "Batch 1843/14851, Loss: 0.06694259494543076\n",
      "Batch 1844/14851, Loss: 0.007589377462863922\n",
      "Batch 1845/14851, Loss: 0.007171800825744867\n",
      "Batch 1846/14851, Loss: 0.006772755179554224\n",
      "Batch 1847/14851, Loss: 0.028235802426934242\n",
      "Batch 1848/14851, Loss: 0.003270647255703807\n",
      "Batch 1849/14851, Loss: 9.089583909371868e-05\n",
      "Batch 1850/14851, Loss: 0.001305455924011767\n",
      "Batch 1851/14851, Loss: 0.0019533431623131037\n",
      "Batch 1852/14851, Loss: 0.0007859848556108773\n",
      "Batch 1853/14851, Loss: 0.01863575540482998\n",
      "Batch 1854/14851, Loss: 0.004954025149345398\n",
      "Batch 1855/14851, Loss: 0.0013677170500159264\n",
      "Batch 1856/14851, Loss: 0.025570258498191833\n",
      "Batch 1857/14851, Loss: 0.0029020991642028093\n",
      "Batch 1858/14851, Loss: 4.0687620639801025e-05\n",
      "Batch 1859/14851, Loss: 0.00866745039820671\n",
      "Batch 1860/14851, Loss: 0.0029856176115572453\n",
      "Batch 1861/14851, Loss: 0.00026483088731765747\n",
      "Batch 1862/14851, Loss: 0.009697743691504002\n",
      "Batch 1863/14851, Loss: 0.00041253119707107544\n",
      "Batch 1864/14851, Loss: 0.03267648443579674\n",
      "Batch 1865/14851, Loss: 0.016347499564290047\n",
      "Batch 1866/14851, Loss: 0.0022380102891474962\n",
      "Batch 1867/14851, Loss: 0.025025390088558197\n",
      "Batch 1868/14851, Loss: 0.0031600433867424726\n",
      "Batch 1869/14851, Loss: 0.020703619346022606\n",
      "Batch 1870/14851, Loss: 0.009007968939840794\n",
      "Batch 1871/14851, Loss: 0.0028074842412024736\n",
      "Batch 1872/14851, Loss: 0.08101274073123932\n",
      "Batch 1873/14851, Loss: 0.0024546633940190077\n",
      "Batch 1874/14851, Loss: 0.014649847522377968\n",
      "Batch 1875/14851, Loss: 0.0031447866931557655\n",
      "Batch 1876/14851, Loss: 0.0021966814529150724\n",
      "Batch 1877/14851, Loss: 0.012676204554736614\n",
      "Batch 1878/14851, Loss: 0.0017489790916442871\n",
      "Batch 1879/14851, Loss: 0.0049204640090465546\n",
      "Batch 1880/14851, Loss: 0.0014175265096127987\n",
      "Batch 1881/14851, Loss: 0.01748591475188732\n",
      "Batch 1882/14851, Loss: 0.032345354557037354\n",
      "Batch 1883/14851, Loss: 0.0014498071977868676\n",
      "Batch 1884/14851, Loss: 0.009460140950977802\n",
      "Batch 1885/14851, Loss: 0.023807020857930183\n",
      "Batch 1886/14851, Loss: 0.00039889290928840637\n",
      "Batch 1887/14851, Loss: 0.0060410588048398495\n",
      "Batch 1888/14851, Loss: 0.011297579854726791\n",
      "Batch 1889/14851, Loss: 0.03664373233914375\n",
      "Batch 1890/14851, Loss: 0.025871489197015762\n",
      "Batch 1891/14851, Loss: 0.004080027807503939\n",
      "Batch 1892/14851, Loss: 0.0013626328436657786\n",
      "Batch 1893/14851, Loss: 0.0018834210932254791\n",
      "Batch 1894/14851, Loss: 0.0007924522506073117\n",
      "Batch 1895/14851, Loss: 0.0003093332052230835\n",
      "Batch 1896/14851, Loss: 9.341537952423096e-05\n",
      "Batch 1897/14851, Loss: 0.000593993638176471\n",
      "Batch 1898/14851, Loss: 0.001049267710186541\n",
      "Batch 1899/14851, Loss: 0.001091310172341764\n",
      "Batch 1900/14851, Loss: 0.00964453537017107\n",
      "Batch 1901/14851, Loss: 0.004841987043619156\n",
      "Batch 1902/14851, Loss: 0.004439159762114286\n",
      "Batch 1903/14851, Loss: 0.019485292956233025\n",
      "Batch 1904/14851, Loss: 0.007169779390096664\n",
      "Batch 1905/14851, Loss: 0.03296486288309097\n",
      "Batch 1906/14851, Loss: 0.003660284448415041\n",
      "Batch 1907/14851, Loss: 0.026989612728357315\n",
      "Batch 1908/14851, Loss: 0.07377392053604126\n",
      "Batch 1909/14851, Loss: 0.0004411078989505768\n",
      "Batch 1910/14851, Loss: 0.003079137997701764\n",
      "Batch 1911/14851, Loss: 0.019580837339162827\n",
      "Batch 1912/14851, Loss: 0.004751591477543116\n",
      "Batch 1913/14851, Loss: 0.0007614158093929291\n",
      "Batch 1914/14851, Loss: 0.015871962532401085\n",
      "Batch 1915/14851, Loss: 0.0007035546004772186\n",
      "Batch 1916/14851, Loss: 0.0007428738754242659\n",
      "Batch 1917/14851, Loss: 0.0051380787044763565\n",
      "Batch 1918/14851, Loss: 0.00010655075311660767\n",
      "Batch 1919/14851, Loss: 0.0004411861300468445\n",
      "Batch 1920/14851, Loss: 0.038093697279691696\n",
      "Batch 1921/14851, Loss: 0.06830640882253647\n",
      "Batch 1922/14851, Loss: 0.011188746429979801\n",
      "Batch 1923/14851, Loss: 0.0011171996593475342\n",
      "Batch 1924/14851, Loss: 0.03925011307001114\n",
      "Batch 1925/14851, Loss: 0.0008960386621765792\n",
      "Batch 1926/14851, Loss: 0.023341819643974304\n",
      "Batch 1927/14851, Loss: 0.000446202524472028\n",
      "Batch 1928/14851, Loss: 0.0007177015650086105\n",
      "Batch 1929/14851, Loss: 0.0034690373577177525\n",
      "Batch 1930/14851, Loss: 0.006992722861468792\n",
      "Batch 1931/14851, Loss: 0.0004139617085456848\n",
      "Batch 1932/14851, Loss: 0.004056330304592848\n",
      "Batch 1933/14851, Loss: 0.0002492256462574005\n",
      "Batch 1934/14851, Loss: 0.003697347128763795\n",
      "Batch 1935/14851, Loss: 0.021004263311624527\n",
      "Batch 1936/14851, Loss: 0.0047998889349401\n",
      "Batch 1937/14851, Loss: 0.005149132572114468\n",
      "Batch 1938/14851, Loss: 0.0041084312833845615\n",
      "Batch 1939/14851, Loss: 0.0013062016805633903\n",
      "Batch 1940/14851, Loss: 0.006569372024387121\n",
      "Batch 1941/14851, Loss: 0.005060870200395584\n",
      "Batch 1942/14851, Loss: 0.025754928588867188\n",
      "Batch 1943/14851, Loss: 0.015546604990959167\n",
      "Batch 1944/14851, Loss: 0.00044446811079978943\n",
      "Batch 1945/14851, Loss: 0.011414057575166225\n",
      "Batch 1946/14851, Loss: 0.003144403686746955\n",
      "Batch 1947/14851, Loss: 0.0011833017924800515\n",
      "Batch 1948/14851, Loss: 0.023288704454898834\n",
      "Batch 1949/14851, Loss: 0.03672470524907112\n",
      "Batch 1950/14851, Loss: 0.007875042967498302\n",
      "Batch 1951/14851, Loss: 0.03196336701512337\n",
      "Batch 1952/14851, Loss: 0.003935651388019323\n",
      "Batch 1953/14851, Loss: 0.001284713507629931\n",
      "Batch 1954/14851, Loss: 0.007664088625460863\n",
      "Batch 1955/14851, Loss: 0.006947556976228952\n",
      "Batch 1956/14851, Loss: 0.030248405411839485\n",
      "Batch 1957/14851, Loss: 0.0003470753727015108\n",
      "Batch 1958/14851, Loss: 0.017594750970602036\n",
      "Batch 1959/14851, Loss: 0.00044629329931922257\n",
      "Batch 1960/14851, Loss: 0.00030080025317147374\n",
      "Batch 1961/14851, Loss: 0.0019249431788921356\n",
      "Batch 1962/14851, Loss: 0.0035507450811564922\n",
      "Batch 1963/14851, Loss: 0.004163871053606272\n",
      "Batch 1964/14851, Loss: 0.001811819733120501\n",
      "Batch 1965/14851, Loss: 0.0011233786353841424\n",
      "Batch 1966/14851, Loss: 0.0025815933477133512\n",
      "Batch 1967/14851, Loss: 8.072704076766968e-05\n",
      "Batch 1968/14851, Loss: 0.01025482639670372\n",
      "Batch 1969/14851, Loss: 0.0527140349149704\n",
      "Batch 1970/14851, Loss: 0.015744050964713097\n",
      "Batch 1971/14851, Loss: 0.04179978743195534\n",
      "Batch 1972/14851, Loss: 0.0044406261295080185\n",
      "Batch 1973/14851, Loss: 0.0006266012787818909\n",
      "Batch 1974/14851, Loss: 0.0016864538192749023\n",
      "Batch 1975/14851, Loss: 0.027476703748106956\n",
      "Batch 1976/14851, Loss: 0.10804877430200577\n",
      "Batch 1977/14851, Loss: 0.015503854490816593\n",
      "Batch 1978/14851, Loss: 0.0006850232020951807\n",
      "Batch 1979/14851, Loss: 0.0022089730482548475\n",
      "Batch 1980/14851, Loss: 0.0003906935453414917\n",
      "Batch 1981/14851, Loss: 0.009756148792803288\n",
      "Batch 1982/14851, Loss: 0.00023763005447108299\n",
      "Batch 1983/14851, Loss: 0.0015531989047303796\n",
      "Batch 1984/14851, Loss: 0.00200885022059083\n",
      "Batch 1985/14851, Loss: 0.00013938422489445657\n",
      "Batch 1986/14851, Loss: 0.010897994972765446\n",
      "Batch 1987/14851, Loss: 0.003183860331773758\n",
      "Batch 1988/14851, Loss: 0.0009275263291783631\n",
      "Batch 1989/14851, Loss: 6.171315908432007e-05\n",
      "Batch 1990/14851, Loss: 0.0008010515011847019\n",
      "Batch 1991/14851, Loss: 0.005213817581534386\n",
      "Batch 1992/14851, Loss: 0.006071122363209724\n",
      "Batch 1993/14851, Loss: 0.0006257829372771084\n",
      "Batch 1994/14851, Loss: 0.005627792328596115\n",
      "Batch 1995/14851, Loss: 0.0005270545952953398\n",
      "Batch 1996/14851, Loss: 0.006143574602901936\n",
      "Batch 1997/14851, Loss: 0.003059122711420059\n",
      "Batch 1998/14851, Loss: 0.0009217076003551483\n",
      "Batch 1999/14851, Loss: 0.026438606902956963\n",
      "Batch 2000/14851, Loss: 0.02267702855169773\n",
      "Batch 2001/14851, Loss: 0.001063430099748075\n",
      "Batch 2002/14851, Loss: 0.016647914424538612\n",
      "Batch 2003/14851, Loss: 0.05955140292644501\n",
      "Batch 2004/14851, Loss: 0.0025870155077427626\n",
      "Batch 2005/14851, Loss: 0.007240255828946829\n",
      "Batch 2006/14851, Loss: 0.04163900390267372\n",
      "Batch 2007/14851, Loss: 0.026696443557739258\n",
      "Batch 2008/14851, Loss: 0.006231304258108139\n",
      "Batch 2009/14851, Loss: 0.0002732177672442049\n",
      "Batch 2010/14851, Loss: 0.020343393087387085\n",
      "Batch 2011/14851, Loss: 0.0119544118642807\n",
      "Batch 2012/14851, Loss: 0.0019553385209292173\n",
      "Batch 2013/14851, Loss: 0.00013998647045809776\n",
      "Batch 2014/14851, Loss: 0.0006080332095734775\n",
      "Batch 2015/14851, Loss: 0.03433481231331825\n",
      "Batch 2016/14851, Loss: 0.007162047550082207\n",
      "Batch 2017/14851, Loss: 0.00722831254824996\n",
      "Batch 2018/14851, Loss: 0.0002397472271695733\n",
      "Batch 2019/14851, Loss: 0.01345839910209179\n",
      "Batch 2020/14851, Loss: 0.005829344037920237\n",
      "Batch 2021/14851, Loss: 0.0008653614204376936\n",
      "Batch 2022/14851, Loss: 0.04103781655430794\n",
      "Batch 2023/14851, Loss: 0.005089955870062113\n",
      "Batch 2024/14851, Loss: 0.0003091096878051758\n",
      "Batch 2025/14851, Loss: 0.01307984720915556\n",
      "Batch 2026/14851, Loss: 0.003334309905767441\n",
      "Batch 2027/14851, Loss: 0.0023153387010097504\n",
      "Batch 2028/14851, Loss: 0.0356462299823761\n",
      "Batch 2029/14851, Loss: 0.050082117319107056\n",
      "Batch 2030/14851, Loss: 0.006865399423986673\n",
      "Batch 2031/14851, Loss: 5.21114889124874e-05\n",
      "Batch 2032/14851, Loss: 0.016300061717629433\n",
      "Batch 2033/14851, Loss: 0.008215207606554031\n",
      "Batch 2034/14851, Loss: 0.017699500545859337\n",
      "Batch 2035/14851, Loss: 0.004104373510926962\n",
      "Batch 2036/14851, Loss: 0.01821727491915226\n",
      "Batch 2037/14851, Loss: 0.0062253461219370365\n",
      "Batch 2038/14851, Loss: 0.005949517246335745\n",
      "Batch 2039/14851, Loss: 0.00012798111129086465\n",
      "Batch 2040/14851, Loss: 0.001873798668384552\n",
      "Batch 2041/14851, Loss: 0.009453966282308102\n",
      "Batch 2042/14851, Loss: 0.048823315650224686\n",
      "Batch 2043/14851, Loss: 0.04277416691184044\n",
      "Batch 2044/14851, Loss: 0.0010116224875673652\n",
      "Batch 2045/14851, Loss: 0.00029838457703590393\n",
      "Batch 2046/14851, Loss: 0.0010803602635860443\n",
      "Batch 2047/14851, Loss: 0.029395708814263344\n",
      "Batch 2048/14851, Loss: 0.0009227233822457492\n",
      "Batch 2049/14851, Loss: 0.0002546645700931549\n",
      "Batch 2050/14851, Loss: 0.013062641955912113\n",
      "Batch 2051/14851, Loss: 0.004043914377689362\n",
      "Batch 2052/14851, Loss: 0.00043767582974396646\n",
      "Batch 2053/14851, Loss: 0.03816479071974754\n",
      "Batch 2054/14851, Loss: 0.00013527397823054343\n",
      "Batch 2055/14851, Loss: 0.003333229571580887\n",
      "Batch 2056/14851, Loss: 0.007176851853728294\n",
      "Batch 2057/14851, Loss: 0.0009298995137214661\n",
      "Batch 2058/14851, Loss: 0.017393499612808228\n",
      "Batch 2059/14851, Loss: 0.0012804133584722877\n",
      "Batch 2060/14851, Loss: 3.684436160256155e-05\n",
      "Batch 2061/14851, Loss: 0.004947793669998646\n",
      "Batch 2062/14851, Loss: 0.04903529956936836\n",
      "Batch 2063/14851, Loss: 0.03623831644654274\n",
      "Batch 2064/14851, Loss: 0.001032426138408482\n",
      "Batch 2065/14851, Loss: 0.0007810654933564365\n",
      "Batch 2066/14851, Loss: 0.0007828250527381897\n",
      "Batch 2067/14851, Loss: 0.0014788297703489661\n",
      "Batch 2068/14851, Loss: 0.005925652105361223\n",
      "Batch 2069/14851, Loss: 0.002043357817456126\n",
      "Batch 2070/14851, Loss: 0.01064370945096016\n",
      "Batch 2071/14851, Loss: 0.03105335682630539\n",
      "Batch 2072/14851, Loss: 0.004070756491273642\n",
      "Batch 2073/14851, Loss: 0.009266018867492676\n",
      "Batch 2074/14851, Loss: 0.02285139635205269\n",
      "Batch 2075/14851, Loss: 5.068341124569997e-05\n",
      "Batch 2076/14851, Loss: 0.0010999286314472556\n",
      "Batch 2077/14851, Loss: 0.004755399655550718\n",
      "Batch 2078/14851, Loss: 0.017794230952858925\n",
      "Batch 2079/14851, Loss: 4.0995579183800146e-05\n",
      "Batch 2080/14851, Loss: 0.0034770879428833723\n",
      "Batch 2081/14851, Loss: 0.03391411155462265\n",
      "Batch 2082/14851, Loss: 0.07972349226474762\n",
      "Batch 2083/14851, Loss: 0.03702753782272339\n",
      "Batch 2084/14851, Loss: 0.004428856074810028\n",
      "Batch 2085/14851, Loss: 0.0024973179679363966\n",
      "Batch 2086/14851, Loss: 0.0005634377594105899\n",
      "Batch 2087/14851, Loss: 0.0009764842689037323\n",
      "Batch 2088/14851, Loss: 0.00028307861066423357\n",
      "Batch 2089/14851, Loss: 0.06058960035443306\n",
      "Batch 2090/14851, Loss: 0.0219778660684824\n",
      "Batch 2091/14851, Loss: 0.013008788228034973\n",
      "Batch 2092/14851, Loss: 0.0007567740976810455\n",
      "Batch 2093/14851, Loss: 0.004696700721979141\n",
      "Batch 2094/14851, Loss: 0.0038503084797412157\n",
      "Batch 2095/14851, Loss: 0.003841002704575658\n",
      "Batch 2096/14851, Loss: 0.0398855023086071\n",
      "Batch 2097/14851, Loss: 0.0004243720613885671\n",
      "Batch 2098/14851, Loss: 0.0023240491282194853\n",
      "Batch 2099/14851, Loss: 0.03222477436065674\n",
      "Batch 2100/14851, Loss: 0.0033177766017615795\n",
      "Batch 2101/14851, Loss: 0.01323674712330103\n",
      "Batch 2102/14851, Loss: 0.0030106704216450453\n",
      "Batch 2103/14851, Loss: 0.0005757138133049011\n",
      "Batch 2104/14851, Loss: 0.0009584538638591766\n",
      "Batch 2105/14851, Loss: 0.00022963558149058372\n",
      "Batch 2106/14851, Loss: 0.00548619544133544\n",
      "Batch 2107/14851, Loss: 0.03573131933808327\n",
      "Batch 2108/14851, Loss: 0.0002842085959855467\n",
      "Batch 2109/14851, Loss: 0.002761269686743617\n",
      "Batch 2110/14851, Loss: 0.0007503563538193703\n",
      "Batch 2111/14851, Loss: 0.008649464696645737\n",
      "Batch 2112/14851, Loss: 0.010593296028673649\n",
      "Batch 2113/14851, Loss: 0.010312304832041264\n",
      "Batch 2114/14851, Loss: 0.022599905729293823\n",
      "Batch 2115/14851, Loss: 0.002373249502852559\n",
      "Batch 2116/14851, Loss: 0.03531485050916672\n",
      "Batch 2117/14851, Loss: 0.004014589358121157\n",
      "Batch 2118/14851, Loss: 0.001008782535791397\n",
      "Batch 2119/14851, Loss: 0.0014177573611959815\n",
      "Batch 2120/14851, Loss: 0.021656526252627373\n",
      "Batch 2121/14851, Loss: 0.007382022216916084\n",
      "Batch 2122/14851, Loss: 0.0011945540318265557\n",
      "Batch 2123/14851, Loss: 0.022122923284769058\n",
      "Batch 2124/14851, Loss: 0.0019069385016337037\n",
      "Batch 2125/14851, Loss: 0.016884103417396545\n",
      "Batch 2126/14851, Loss: 0.002802538685500622\n",
      "Batch 2127/14851, Loss: 0.007793443743139505\n",
      "Batch 2128/14851, Loss: 0.01901683211326599\n",
      "Batch 2129/14851, Loss: 0.004054571036249399\n",
      "Batch 2130/14851, Loss: 0.0009718961664475501\n",
      "Batch 2131/14851, Loss: 0.016944902017712593\n",
      "Batch 2132/14851, Loss: 0.00854854192584753\n",
      "Batch 2133/14851, Loss: 0.0033149421215057373\n",
      "Batch 2134/14851, Loss: 0.04040387645363808\n",
      "Batch 2135/14851, Loss: 0.00906845461577177\n",
      "Batch 2136/14851, Loss: 0.0008442815160378814\n",
      "Batch 2137/14851, Loss: 0.0004443017242010683\n",
      "Batch 2138/14851, Loss: 0.0016887100646272302\n",
      "Batch 2139/14851, Loss: 0.018967939540743828\n",
      "Batch 2140/14851, Loss: 0.05865133926272392\n",
      "Batch 2141/14851, Loss: 0.0014708824455738068\n",
      "Batch 2142/14851, Loss: 0.007289492525160313\n",
      "Batch 2143/14851, Loss: 0.008128073997795582\n",
      "Batch 2144/14851, Loss: 0.005516292992979288\n",
      "Batch 2145/14851, Loss: 0.04592609778046608\n",
      "Batch 2146/14851, Loss: 0.00012713298201560974\n",
      "Batch 2147/14851, Loss: 0.018729617819190025\n",
      "Batch 2148/14851, Loss: 0.02415747009217739\n",
      "Batch 2149/14851, Loss: 0.0016830399399623275\n",
      "Batch 2150/14851, Loss: 0.031629111617803574\n",
      "Batch 2151/14851, Loss: 0.026419242843985558\n",
      "Batch 2152/14851, Loss: 0.0014723945641890168\n",
      "Batch 2153/14851, Loss: 0.001456339843571186\n",
      "Batch 2154/14851, Loss: 0.0021320106461644173\n",
      "Batch 2155/14851, Loss: 0.0024958455469459295\n",
      "Batch 2156/14851, Loss: 0.044719211757183075\n",
      "Batch 2157/14851, Loss: 0.008661971427500248\n",
      "Batch 2158/14851, Loss: 0.015312029980123043\n",
      "Batch 2159/14851, Loss: 0.027622662484645844\n",
      "Batch 2160/14851, Loss: 0.005757549777626991\n",
      "Batch 2161/14851, Loss: 0.005085797514766455\n",
      "Batch 2162/14851, Loss: 0.00013803705223836005\n",
      "Batch 2163/14851, Loss: 0.012902741320431232\n",
      "Batch 2164/14851, Loss: 0.0016869803657755256\n",
      "Batch 2165/14851, Loss: 0.02901419624686241\n",
      "Batch 2166/14851, Loss: 0.006695185787975788\n",
      "Batch 2167/14851, Loss: 0.005801066756248474\n",
      "Batch 2168/14851, Loss: 0.019825302064418793\n",
      "Batch 2169/14851, Loss: 0.00027902176952920854\n",
      "Batch 2170/14851, Loss: 0.005316948983818293\n",
      "Batch 2171/14851, Loss: 0.006564411334693432\n",
      "Batch 2172/14851, Loss: 0.00984699372202158\n",
      "Batch 2173/14851, Loss: 0.0034980662167072296\n",
      "Batch 2174/14851, Loss: 9.752561891218647e-05\n",
      "Batch 2175/14851, Loss: 0.0069184876047074795\n",
      "Batch 2176/14851, Loss: 0.0007191064651124179\n",
      "Batch 2177/14851, Loss: 0.0004502236843109131\n",
      "Batch 2178/14851, Loss: 0.0175815150141716\n",
      "Batch 2179/14851, Loss: 0.012988422065973282\n",
      "Batch 2180/14851, Loss: 0.0005850419402122498\n",
      "Batch 2181/14851, Loss: 0.0029348197858780622\n",
      "Batch 2182/14851, Loss: 0.011258341372013092\n",
      "Batch 2183/14851, Loss: 0.0009639131603762507\n",
      "Batch 2184/14851, Loss: 0.00032738843583501875\n",
      "Batch 2185/14851, Loss: 0.03308430314064026\n",
      "Batch 2186/14851, Loss: 0.0021292443852871656\n",
      "Batch 2187/14851, Loss: 0.011531149968504906\n",
      "Batch 2188/14851, Loss: 0.0030326442793011665\n",
      "Batch 2189/14851, Loss: 0.000830666278488934\n",
      "Batch 2190/14851, Loss: 0.0007873463327996433\n",
      "Batch 2191/14851, Loss: 0.043446123600006104\n",
      "Batch 2192/14851, Loss: 0.00018462662410456687\n",
      "Batch 2193/14851, Loss: 0.05639784038066864\n",
      "Batch 2194/14851, Loss: 0.009281044825911522\n",
      "Batch 2195/14851, Loss: 0.00041208244510926306\n",
      "Batch 2196/14851, Loss: 0.0008763447403907776\n",
      "Batch 2197/14851, Loss: 0.0019463859498500824\n",
      "Batch 2198/14851, Loss: 0.0008918233215808868\n",
      "Batch 2199/14851, Loss: 9.834890806814656e-05\n",
      "Batch 2200/14851, Loss: 0.0033528741914778948\n",
      "Batch 2201/14851, Loss: 0.0002163313329219818\n",
      "Batch 2202/14851, Loss: 0.00041323775076307356\n",
      "Batch 2203/14851, Loss: 0.0072184293530881405\n",
      "Batch 2204/14851, Loss: 0.00022581715893466026\n",
      "Batch 2205/14851, Loss: 0.019977638497948647\n",
      "Batch 2206/14851, Loss: 0.006501009222120047\n",
      "Batch 2207/14851, Loss: 0.0022012158297002316\n",
      "Batch 2208/14851, Loss: 0.008139153011143208\n",
      "Batch 2209/14851, Loss: 0.015809547156095505\n",
      "Batch 2210/14851, Loss: 0.003773654578253627\n",
      "Batch 2211/14851, Loss: 0.0008015471394173801\n",
      "Batch 2212/14851, Loss: 0.002496712375432253\n",
      "Batch 2213/14851, Loss: 0.024652883410453796\n",
      "Batch 2214/14851, Loss: 0.001550234854221344\n",
      "Batch 2215/14851, Loss: 0.0057189613580703735\n",
      "Batch 2216/14851, Loss: 0.03256449103355408\n",
      "Batch 2217/14851, Loss: 0.001874620676971972\n",
      "Batch 2218/14851, Loss: 0.001668672077357769\n",
      "Batch 2219/14851, Loss: 0.008238210342824459\n",
      "Batch 2220/14851, Loss: 0.0015761234099045396\n",
      "Batch 2221/14851, Loss: 0.015088227577507496\n",
      "Batch 2222/14851, Loss: 0.0058923447504639626\n",
      "Batch 2223/14851, Loss: 0.008756729774177074\n",
      "Batch 2224/14851, Loss: 0.01750713773071766\n",
      "Batch 2225/14851, Loss: 0.017440926283597946\n",
      "Batch 2226/14851, Loss: 0.05661546066403389\n",
      "Batch 2227/14851, Loss: 0.0014231366803869605\n",
      "Batch 2228/14851, Loss: 0.0003850981593132019\n",
      "Batch 2229/14851, Loss: 0.004977960139513016\n",
      "Batch 2230/14851, Loss: 0.0026416380424052477\n",
      "Batch 2231/14851, Loss: 0.029139908030629158\n",
      "Batch 2232/14851, Loss: 0.0029866951517760754\n",
      "Batch 2233/14851, Loss: 0.002975230338051915\n",
      "Batch 2234/14851, Loss: 0.011949289590120316\n",
      "Batch 2235/14851, Loss: 0.0012417348334565759\n",
      "Batch 2236/14851, Loss: 0.0001444779336452484\n",
      "Batch 2237/14851, Loss: 0.03578940033912659\n",
      "Batch 2238/14851, Loss: 0.007284093182533979\n",
      "Batch 2239/14851, Loss: 0.009517067112028599\n",
      "Batch 2240/14851, Loss: 0.005886147730052471\n",
      "Batch 2241/14851, Loss: 0.0017882324755191803\n",
      "Batch 2242/14851, Loss: 0.019820239394903183\n",
      "Batch 2243/14851, Loss: 0.05107015743851662\n",
      "Batch 2244/14851, Loss: 0.01638837344944477\n",
      "Batch 2245/14851, Loss: 0.0025828813668340445\n",
      "Batch 2246/14851, Loss: 0.0013605484273284674\n",
      "Batch 2247/14851, Loss: 0.004717776086181402\n",
      "Batch 2248/14851, Loss: 0.03656738996505737\n",
      "Batch 2249/14851, Loss: 0.0006232122541405261\n",
      "Batch 2250/14851, Loss: 0.01719840057194233\n",
      "Batch 2251/14851, Loss: 0.03235580027103424\n",
      "Batch 2252/14851, Loss: 0.0011662021279335022\n",
      "Batch 2253/14851, Loss: 0.02320978045463562\n",
      "Batch 2254/14851, Loss: 0.0011046553263440728\n",
      "Batch 2255/14851, Loss: 0.0012687841663137078\n",
      "Batch 2256/14851, Loss: 0.018997186794877052\n",
      "Batch 2257/14851, Loss: 0.0009738345397636294\n",
      "Batch 2258/14851, Loss: 0.025189831852912903\n",
      "Batch 2259/14851, Loss: 0.05875742807984352\n",
      "Batch 2260/14851, Loss: 0.0027316485065966845\n",
      "Batch 2261/14851, Loss: 0.017901072278618813\n",
      "Batch 2262/14851, Loss: 0.0057092332281172276\n",
      "Batch 2263/14851, Loss: 0.007879645563662052\n",
      "Batch 2264/14851, Loss: 9.776447404874489e-05\n",
      "Batch 2265/14851, Loss: 0.0001774144620867446\n",
      "Batch 2266/14851, Loss: 0.011176712810993195\n",
      "Batch 2267/14851, Loss: 0.03060935065150261\n",
      "Batch 2268/14851, Loss: 0.0004258354601915926\n",
      "Batch 2269/14851, Loss: 0.0002810793521348387\n",
      "Batch 2270/14851, Loss: 0.0008921523694880307\n",
      "Batch 2271/14851, Loss: 0.0011977789690718055\n",
      "Batch 2272/14851, Loss: 0.012447909452021122\n",
      "Batch 2273/14851, Loss: 0.000644981861114502\n",
      "Batch 2274/14851, Loss: 0.014512371271848679\n",
      "Batch 2275/14851, Loss: 0.0035505767446011305\n",
      "Batch 2276/14851, Loss: 0.006492518819868565\n",
      "Batch 2277/14851, Loss: 0.0038995363283902407\n",
      "Batch 2278/14851, Loss: 0.01653636433184147\n",
      "Batch 2279/14851, Loss: 0.0005541592836380005\n",
      "Batch 2280/14851, Loss: 0.04012430086731911\n",
      "Batch 2281/14851, Loss: 4.175926369498484e-05\n",
      "Batch 2282/14851, Loss: 0.0008534864755347371\n",
      "Batch 2283/14851, Loss: 0.04174461588263512\n",
      "Batch 2284/14851, Loss: 0.0034253299236297607\n",
      "Batch 2285/14851, Loss: 0.0004126553831156343\n",
      "Batch 2286/14851, Loss: 0.014339146204292774\n",
      "Batch 2287/14851, Loss: 0.03440441936254501\n",
      "Batch 2288/14851, Loss: 0.023469209671020508\n",
      "Batch 2289/14851, Loss: 0.0009337912197224796\n",
      "Batch 2290/14851, Loss: 0.010031845420598984\n",
      "Batch 2291/14851, Loss: 0.0005100878770463169\n",
      "Batch 2292/14851, Loss: 0.012586104683578014\n",
      "Batch 2293/14851, Loss: 0.0025854099076241255\n",
      "Batch 2294/14851, Loss: 0.03158137574791908\n",
      "Batch 2295/14851, Loss: 7.65584409236908e-05\n",
      "Batch 2296/14851, Loss: 7.701789581915364e-05\n",
      "Batch 2297/14851, Loss: 0.0004238275287207216\n",
      "Batch 2298/14851, Loss: 0.0004921357030980289\n",
      "Batch 2299/14851, Loss: 0.0011849403381347656\n",
      "Batch 2300/14851, Loss: 0.013113929890096188\n",
      "Batch 2301/14851, Loss: 0.020471489056944847\n",
      "Batch 2302/14851, Loss: 0.0003642585943453014\n",
      "Batch 2303/14851, Loss: 0.0001596336514921859\n",
      "Batch 2304/14851, Loss: 0.018245922401547432\n",
      "Batch 2305/14851, Loss: 0.001188172260299325\n",
      "Batch 2306/14851, Loss: 0.011064812541007996\n",
      "Batch 2307/14851, Loss: 0.0031584773678332567\n",
      "Batch 2308/14851, Loss: 0.027149973437190056\n",
      "Batch 2309/14851, Loss: 0.0005834329640492797\n",
      "Batch 2310/14851, Loss: 0.062757708132267\n",
      "Batch 2311/14851, Loss: 0.0014965631999075413\n",
      "Batch 2312/14851, Loss: 0.0022118266206234694\n",
      "Batch 2313/14851, Loss: 0.051562465727329254\n",
      "Batch 2314/14851, Loss: 0.02274799346923828\n",
      "Batch 2315/14851, Loss: 0.04838307946920395\n",
      "Batch 2316/14851, Loss: 0.0007468350813724101\n",
      "Batch 2317/14851, Loss: 0.02894357405602932\n",
      "Batch 2318/14851, Loss: 0.00017071515321731567\n",
      "Batch 2319/14851, Loss: 0.00447863107547164\n",
      "Batch 2320/14851, Loss: 0.019002322107553482\n",
      "Batch 2321/14851, Loss: 0.0029611068312078714\n",
      "Batch 2322/14851, Loss: 0.00014877568173687905\n",
      "Batch 2323/14851, Loss: 0.01273178867995739\n",
      "Batch 2324/14851, Loss: 0.012848768383264542\n",
      "Batch 2325/14851, Loss: 0.001274774200282991\n",
      "Batch 2326/14851, Loss: 0.02592073194682598\n",
      "Batch 2327/14851, Loss: 0.019066547974944115\n",
      "Batch 2328/14851, Loss: 0.001825476996600628\n",
      "Batch 2329/14851, Loss: 0.037552572786808014\n",
      "Batch 2330/14851, Loss: 0.002074497053399682\n",
      "Batch 2331/14851, Loss: 0.00011928627645829692\n",
      "Batch 2332/14851, Loss: 0.0029524394776672125\n",
      "Batch 2333/14851, Loss: 0.0011919231619685888\n",
      "Batch 2334/14851, Loss: 0.01756075583398342\n",
      "Batch 2335/14851, Loss: 0.003291156142950058\n",
      "Batch 2336/14851, Loss: 0.013649397529661655\n",
      "Batch 2337/14851, Loss: 0.042554259300231934\n",
      "Batch 2338/14851, Loss: 0.0006955755525268614\n",
      "Batch 2339/14851, Loss: 0.002879549516364932\n",
      "Batch 2340/14851, Loss: 0.00021356741490308195\n",
      "Batch 2341/14851, Loss: 0.0007741649751551449\n",
      "Batch 2342/14851, Loss: 0.0004565544077195227\n",
      "Batch 2343/14851, Loss: 0.0021697592455893755\n",
      "Batch 2344/14851, Loss: 0.0004185195721220225\n",
      "Batch 2345/14851, Loss: 0.0008949550683610141\n",
      "Batch 2346/14851, Loss: 0.01908882148563862\n",
      "Batch 2347/14851, Loss: 0.009756886400282383\n",
      "Batch 2348/14851, Loss: 0.03363167122006416\n",
      "Batch 2349/14851, Loss: 0.00045684477663598955\n",
      "Batch 2350/14851, Loss: 0.025538722053170204\n",
      "Batch 2351/14851, Loss: 0.003546004416421056\n",
      "Batch 2352/14851, Loss: 0.01344155054539442\n",
      "Batch 2353/14851, Loss: 0.03762965649366379\n",
      "Batch 2354/14851, Loss: 0.0005864177946932614\n",
      "Batch 2355/14851, Loss: 0.0014521062839776278\n",
      "Batch 2356/14851, Loss: 0.006557884160429239\n",
      "Batch 2357/14851, Loss: 0.00011413097672630101\n",
      "Batch 2358/14851, Loss: 0.08448430150747299\n",
      "Batch 2359/14851, Loss: 0.003134526312351227\n",
      "Batch 2360/14851, Loss: 0.01767306588590145\n",
      "Batch 2361/14851, Loss: 0.0005396343767642975\n",
      "Batch 2362/14851, Loss: 0.014306657947599888\n",
      "Batch 2363/14851, Loss: 0.007157022599130869\n",
      "Batch 2364/14851, Loss: 0.0036339859943836927\n",
      "Batch 2365/14851, Loss: 0.0006204645032994449\n",
      "Batch 2366/14851, Loss: 0.0014575012028217316\n",
      "Batch 2367/14851, Loss: 0.0002651383401826024\n",
      "Batch 2368/14851, Loss: 0.010717804543673992\n",
      "Batch 2369/14851, Loss: 0.00818998645991087\n",
      "Batch 2370/14851, Loss: 0.0001614093780517578\n",
      "Batch 2371/14851, Loss: 0.0022110778372734785\n",
      "Batch 2372/14851, Loss: 0.013690972700715065\n",
      "Batch 2373/14851, Loss: 0.03117700107395649\n",
      "Batch 2374/14851, Loss: 0.0020208207424730062\n",
      "Batch 2375/14851, Loss: 0.000295247882604599\n",
      "Batch 2376/14851, Loss: 0.0024225611705332994\n",
      "Batch 2377/14851, Loss: 0.0005231139366514981\n",
      "Batch 2378/14851, Loss: 0.0001403142960043624\n",
      "Batch 2379/14851, Loss: 0.06184452772140503\n",
      "Batch 2380/14851, Loss: 0.016287093982100487\n",
      "Batch 2381/14851, Loss: 0.00010342150926589966\n",
      "Batch 2382/14851, Loss: 0.019109496846795082\n",
      "Batch 2383/14851, Loss: 0.009976129978895187\n",
      "Batch 2384/14851, Loss: 0.005070406943559647\n",
      "Batch 2385/14851, Loss: 0.003798810765147209\n",
      "Batch 2386/14851, Loss: 0.0030542986933141947\n",
      "Batch 2387/14851, Loss: 0.0008605222101323307\n",
      "Batch 2388/14851, Loss: 0.0062925186939537525\n",
      "Batch 2389/14851, Loss: 5.069429244031198e-05\n",
      "Batch 2390/14851, Loss: 0.049997806549072266\n",
      "Batch 2391/14851, Loss: 0.0017614172538742423\n",
      "Batch 2392/14851, Loss: 0.0014148304471746087\n",
      "Batch 2393/14851, Loss: 0.013222523033618927\n",
      "Batch 2394/14851, Loss: 0.0605764240026474\n",
      "Batch 2395/14851, Loss: 0.011716282926499844\n",
      "Batch 2396/14851, Loss: 0.0004464462399482727\n",
      "Batch 2397/14851, Loss: 0.0002153019158868119\n",
      "Batch 2398/14851, Loss: 0.00012613956641871482\n",
      "Batch 2399/14851, Loss: 0.00035510622547008097\n",
      "Batch 2400/14851, Loss: 0.002993965055793524\n",
      "Batch 2401/14851, Loss: 0.05062892287969589\n",
      "Batch 2402/14851, Loss: 0.003966661635786295\n",
      "Batch 2403/14851, Loss: 0.0028374074026942253\n",
      "Batch 2404/14851, Loss: 0.0005373621243052185\n",
      "Batch 2405/14851, Loss: 0.013862640596926212\n",
      "Batch 2406/14851, Loss: 0.0024412882048636675\n",
      "Batch 2407/14851, Loss: 0.0024499285500496626\n",
      "Batch 2408/14851, Loss: 0.0004757878778036684\n",
      "Batch 2409/14851, Loss: 0.00025263181305490434\n",
      "Batch 2410/14851, Loss: 0.0009875192772597075\n",
      "Batch 2411/14851, Loss: 0.006839924957603216\n",
      "Batch 2412/14851, Loss: 0.012800239957869053\n",
      "Batch 2413/14851, Loss: 0.00102287158370018\n",
      "Batch 2414/14851, Loss: 0.01243859063833952\n",
      "Batch 2415/14851, Loss: 0.0013105132384225726\n",
      "Batch 2416/14851, Loss: 0.0016443481435999274\n",
      "Batch 2417/14851, Loss: 0.05212024226784706\n",
      "Batch 2418/14851, Loss: 0.006458108779042959\n",
      "Batch 2419/14851, Loss: 0.0005118933622725308\n",
      "Batch 2420/14851, Loss: 0.0009229828719981015\n",
      "Batch 2421/14851, Loss: 0.0029059138614684343\n",
      "Batch 2422/14851, Loss: 0.02217038907110691\n",
      "Batch 2423/14851, Loss: 0.0008633245015516877\n",
      "Batch 2424/14851, Loss: 0.010310572572052479\n",
      "Batch 2425/14851, Loss: 0.0007951371371746063\n",
      "Batch 2426/14851, Loss: 0.0008945305598899722\n",
      "Batch 2427/14851, Loss: 0.007273613475263119\n",
      "Batch 2428/14851, Loss: 0.004706729669123888\n",
      "Batch 2429/14851, Loss: 0.001143291941843927\n",
      "Batch 2430/14851, Loss: 0.006618363317102194\n",
      "Batch 2431/14851, Loss: 0.039714962244033813\n",
      "Skipping batch 2432 due to NaN loss\n",
      "Batch 2433/14851, Loss: nan\n",
      "Batch 2434/14851, Loss: 0.020760223269462585\n",
      "Batch 2435/14851, Loss: 0.00428288197144866\n",
      "Batch 2436/14851, Loss: 9.989986574510112e-05\n",
      "Batch 2437/14851, Loss: 0.0005129587952978909\n",
      "Batch 2438/14851, Loss: 0.0009652015869505703\n",
      "Batch 2439/14851, Loss: 0.006323392037302256\n",
      "Batch 2440/14851, Loss: 0.05522967502474785\n",
      "Batch 2441/14851, Loss: 0.0035232293885201216\n",
      "Batch 2442/14851, Loss: 0.0029623955488204956\n",
      "Batch 2443/14851, Loss: 0.0016798588912934065\n",
      "Batch 2444/14851, Loss: 0.001999619882553816\n",
      "Batch 2445/14851, Loss: 0.00011568392073968425\n",
      "Batch 2446/14851, Loss: 0.00037330761551856995\n",
      "Batch 2447/14851, Loss: 0.031092273071408272\n",
      "Batch 2448/14851, Loss: 0.0010321622248739004\n",
      "Batch 2449/14851, Loss: 0.001120839617215097\n",
      "Batch 2450/14851, Loss: 3.9969880162971094e-05\n",
      "Batch 2451/14851, Loss: 0.019608430564403534\n",
      "Batch 2452/14851, Loss: 0.0009357555536553264\n",
      "Batch 2453/14851, Loss: 0.005552211776375771\n",
      "Batch 2454/14851, Loss: 0.0014933185884729028\n",
      "Batch 2455/14851, Loss: 0.0001664198935031891\n",
      "Batch 2456/14851, Loss: 0.021119264885783195\n",
      "Batch 2457/14851, Loss: 0.007052802946418524\n",
      "Batch 2458/14851, Loss: 0.06183302402496338\n",
      "Batch 2459/14851, Loss: 0.030958449468016624\n",
      "Batch 2460/14851, Loss: 0.009742175228893757\n",
      "Batch 2461/14851, Loss: 0.022735552862286568\n",
      "Batch 2462/14851, Loss: 0.0032648672349750996\n",
      "Batch 2463/14851, Loss: 0.0018660173518583179\n",
      "Batch 2464/14851, Loss: 0.0035250219516456127\n",
      "Batch 2465/14851, Loss: 0.18578040599822998\n",
      "Batch 2466/14851, Loss: 0.0020874959882348776\n",
      "Batch 2467/14851, Loss: 0.05111538991332054\n",
      "Batch 2468/14851, Loss: 0.06600493937730789\n",
      "Batch 2469/14851, Loss: 0.03005392663180828\n",
      "Batch 2470/14851, Loss: 0.006027703173458576\n",
      "Batch 2471/14851, Loss: 0.011823512613773346\n",
      "Batch 2472/14851, Loss: 0.0025864536873996258\n",
      "Batch 2473/14851, Loss: 0.01980213075876236\n",
      "Batch 2474/14851, Loss: 0.03881853446364403\n",
      "Batch 2475/14851, Loss: 0.02192171849310398\n",
      "Batch 2476/14851, Loss: 0.00027809789753519\n",
      "Batch 2477/14851, Loss: 0.0010224487632513046\n",
      "Batch 2478/14851, Loss: 0.0001639177353354171\n",
      "Batch 2479/14851, Loss: 0.0002477628586348146\n",
      "Batch 2480/14851, Loss: 0.00030164545751176775\n",
      "Batch 2481/14851, Loss: 0.008554809726774693\n",
      "Batch 2482/14851, Loss: 0.0004289963108021766\n",
      "Batch 2483/14851, Loss: 0.0631130263209343\n",
      "Batch 2484/14851, Loss: 0.038520343601703644\n",
      "Batch 2485/14851, Loss: 0.000945181876886636\n",
      "Batch 2486/14851, Loss: 0.016515187919139862\n",
      "Batch 2487/14851, Loss: 0.006026024464517832\n",
      "Batch 2488/14851, Loss: 0.009607831947505474\n",
      "Batch 2489/14851, Loss: 0.010337607003748417\n",
      "Batch 2490/14851, Loss: 0.0021859619300812483\n",
      "Batch 2491/14851, Loss: 0.01129386480897665\n",
      "Batch 2492/14851, Loss: 0.005773227661848068\n",
      "Batch 2493/14851, Loss: 0.000181191906449385\n",
      "Batch 2494/14851, Loss: 0.006049406714737415\n",
      "Batch 2495/14851, Loss: 0.006186992395669222\n",
      "Batch 2496/14851, Loss: 0.04043325409293175\n",
      "Batch 2497/14851, Loss: 0.005032659508287907\n",
      "Batch 2498/14851, Loss: 0.0013225637376308441\n",
      "Batch 2499/14851, Loss: 0.02058909460902214\n",
      "Batch 2500/14851, Loss: 0.0003369964542798698\n",
      "Batch 2501/14851, Loss: 0.006187051068991423\n",
      "Batch 2502/14851, Loss: 0.0056966133415699005\n",
      "Batch 2503/14851, Loss: 0.008432785049080849\n",
      "Batch 2504/14851, Loss: 0.0008128595654852688\n",
      "Batch 2505/14851, Loss: 0.0002332801668671891\n",
      "Batch 2506/14851, Loss: 0.0012024355819448829\n",
      "Batch 2507/14851, Loss: 0.007456336170434952\n",
      "Batch 2508/14851, Loss: 0.051096104085445404\n",
      "Batch 2509/14851, Loss: 0.004725499544292688\n",
      "Batch 2510/14851, Loss: 0.0028980288188904524\n",
      "Batch 2511/14851, Loss: 0.006235193926841021\n",
      "Batch 2512/14851, Loss: 0.0013049630215391517\n",
      "Batch 2513/14851, Loss: 0.01860642433166504\n",
      "Batch 2514/14851, Loss: 0.0014702081680297852\n",
      "Batch 2515/14851, Loss: 0.012527682818472385\n",
      "Batch 2516/14851, Loss: 0.0014815852046012878\n",
      "Batch 2517/14851, Loss: 0.005261804908514023\n",
      "Batch 2518/14851, Loss: 0.001390946446917951\n",
      "Batch 2519/14851, Loss: 0.0027849760372191668\n",
      "Batch 2520/14851, Loss: 0.003279411466792226\n",
      "Batch 2521/14851, Loss: 0.01783311367034912\n",
      "Batch 2522/14851, Loss: 0.003841642290353775\n",
      "Batch 2523/14851, Loss: 0.0074827177450060844\n",
      "Batch 2524/14851, Loss: 0.028063591569662094\n",
      "Batch 2525/14851, Loss: 9.612987196305767e-05\n",
      "Batch 2526/14851, Loss: 0.0011775688035413623\n",
      "Batch 2527/14851, Loss: 0.014303225092589855\n",
      "Batch 2528/14851, Loss: 0.007761197630316019\n",
      "Batch 2529/14851, Loss: 9.299939119955525e-05\n",
      "Batch 2530/14851, Loss: 0.02224845252931118\n",
      "Batch 2531/14851, Loss: 0.00028875842690467834\n",
      "Batch 2532/14851, Loss: 0.006889229640364647\n",
      "Batch 2533/14851, Loss: 0.0041940598748624325\n",
      "Batch 2534/14851, Loss: 5.5485714256064966e-05\n",
      "Batch 2535/14851, Loss: 0.01795840449631214\n",
      "Batch 2536/14851, Loss: 0.0008853320614434779\n",
      "Batch 2537/14851, Loss: 0.04616314545273781\n",
      "Batch 2538/14851, Loss: 0.0269437488168478\n",
      "Batch 2539/14851, Loss: 0.0015515520935878158\n",
      "Batch 2540/14851, Loss: 0.017426636070013046\n",
      "Batch 2541/14851, Loss: 0.0004730145155917853\n",
      "Batch 2542/14851, Loss: 0.03035331703722477\n",
      "Batch 2543/14851, Loss: 0.0011658543953672051\n",
      "Batch 2544/14851, Loss: 0.0009305769926868379\n",
      "Batch 2545/14851, Loss: 0.04504905268549919\n",
      "Batch 2546/14851, Loss: 0.0008594568935222924\n",
      "Batch 2547/14851, Loss: 0.0015237977495416999\n",
      "Batch 2548/14851, Loss: 0.0035863546654582024\n",
      "Batch 2549/14851, Loss: 0.0006863648886792362\n",
      "Batch 2550/14851, Loss: 0.0004114075272809714\n",
      "Batch 2551/14851, Loss: 0.006747381296008825\n",
      "Batch 2552/14851, Loss: 0.006676166318356991\n",
      "Batch 2553/14851, Loss: 0.0031276580411940813\n",
      "Batch 2554/14851, Loss: 0.0011309513356536627\n",
      "Batch 2555/14851, Loss: 0.03353789448738098\n",
      "Batch 2556/14851, Loss: 0.003473046235740185\n",
      "Batch 2557/14851, Loss: 0.00033243498182855546\n",
      "Batch 2558/14851, Loss: 0.05710286647081375\n",
      "Batch 2559/14851, Loss: 0.008803094737231731\n",
      "Batch 2560/14851, Loss: 0.04281451553106308\n",
      "Batch 2561/14851, Loss: 0.0006597911124117672\n",
      "Batch 2562/14851, Loss: 0.011843083426356316\n",
      "Batch 2563/14851, Loss: 0.004008942283689976\n",
      "Batch 2564/14851, Loss: 0.002786915982142091\n",
      "Batch 2565/14851, Loss: 0.001471889205276966\n",
      "Batch 2566/14851, Loss: 0.0016432000556960702\n",
      "Batch 2567/14851, Loss: 0.0012787915766239166\n",
      "Batch 2568/14851, Loss: 0.07066148519515991\n",
      "Batch 2569/14851, Loss: 0.0022768869530409575\n",
      "Batch 2570/14851, Loss: 0.0019102026708424091\n",
      "Batch 2571/14851, Loss: 0.001869402825832367\n",
      "Batch 2572/14851, Loss: 0.0030983861070126295\n",
      "Batch 2573/14851, Loss: 0.0014029355952516198\n",
      "Batch 2574/14851, Loss: 0.003165803849697113\n",
      "Batch 2575/14851, Loss: 0.01220976747572422\n",
      "Batch 2576/14851, Loss: 0.0006299891392700374\n",
      "Batch 2577/14851, Loss: 0.0015370019245892763\n",
      "Batch 2578/14851, Loss: 0.003510465146973729\n",
      "Batch 2579/14851, Loss: 0.013534392230212688\n",
      "Batch 2580/14851, Loss: 0.022664692252874374\n",
      "Batch 2581/14851, Loss: 0.0259152352809906\n",
      "Batch 2582/14851, Loss: 0.008898342959582806\n",
      "Batch 2583/14851, Loss: 0.03210864216089249\n",
      "Batch 2584/14851, Loss: 0.06649385392665863\n",
      "Batch 2585/14851, Loss: 0.008710787631571293\n",
      "Batch 2586/14851, Loss: 0.0004384095373097807\n",
      "Batch 2587/14851, Loss: 0.0026238709688186646\n",
      "Batch 2588/14851, Loss: 0.0019266125746071339\n",
      "Batch 2589/14851, Loss: 0.00020715098071377724\n",
      "Batch 2590/14851, Loss: 0.08807004988193512\n",
      "Batch 2591/14851, Loss: 0.006461647804826498\n",
      "Batch 2592/14851, Loss: 0.0003783692081924528\n",
      "Batch 2593/14851, Loss: 0.0018596649169921875\n",
      "Batch 2594/14851, Loss: 0.006126187276095152\n",
      "Batch 2595/14851, Loss: 0.006601465400308371\n",
      "Batch 2596/14851, Loss: 0.00010392814874649048\n",
      "Batch 2597/14851, Loss: 0.012805262580513954\n",
      "Batch 2598/14851, Loss: 0.0010775526752695441\n",
      "Batch 2599/14851, Loss: 0.0035305791534483433\n",
      "Batch 2600/14851, Loss: 0.010993323288857937\n",
      "Batch 2601/14851, Loss: 0.006944071967154741\n",
      "Batch 2602/14851, Loss: 0.0015104326885193586\n",
      "Batch 2603/14851, Loss: 0.000784698873758316\n",
      "Batch 2604/14851, Loss: 0.0005825671250931919\n",
      "Batch 2605/14851, Loss: 0.0007949285209178925\n",
      "Batch 2606/14851, Loss: 0.0003877145645674318\n",
      "Batch 2607/14851, Loss: 0.027336014434695244\n",
      "Batch 2608/14851, Loss: 0.0012305118143558502\n",
      "Batch 2609/14851, Loss: 0.006906336173415184\n",
      "Batch 2610/14851, Loss: 0.0010622849222272635\n",
      "Batch 2611/14851, Loss: 0.01051190122961998\n",
      "Batch 2612/14851, Loss: 0.049249399453401566\n",
      "Batch 2613/14851, Loss: 0.07566749304533005\n",
      "Batch 2614/14851, Loss: 0.004319712519645691\n",
      "Batch 2615/14851, Loss: 0.00980332214385271\n",
      "Batch 2616/14851, Loss: 0.019162289798259735\n",
      "Batch 2617/14851, Loss: 0.00631491607055068\n",
      "Batch 2618/14851, Loss: 0.0019618819933384657\n",
      "Batch 2619/14851, Loss: 0.0071161785162985325\n",
      "Batch 2620/14851, Loss: 0.010138935409486294\n",
      "Batch 2621/14851, Loss: 0.01639273576438427\n",
      "Batch 2622/14851, Loss: 0.008062762208282948\n",
      "Batch 2623/14851, Loss: 0.00243387115187943\n",
      "Batch 2624/14851, Loss: 0.0014804750680923462\n",
      "Batch 2625/14851, Loss: 0.012959124520421028\n",
      "Batch 2626/14851, Loss: 0.008188147097826004\n",
      "Batch 2627/14851, Loss: 0.0015188083052635193\n",
      "Batch 2628/14851, Loss: 0.013236655853688717\n",
      "Batch 2629/14851, Loss: 0.0013864301145076752\n",
      "Batch 2630/14851, Loss: 0.0001633937208680436\n",
      "Batch 2631/14851, Loss: 0.0011823831591755152\n",
      "Batch 2632/14851, Loss: 0.02041560970246792\n",
      "Batch 2633/14851, Loss: 0.0015409415354952216\n",
      "Batch 2634/14851, Loss: 0.0004189958272036165\n",
      "Batch 2635/14851, Loss: 0.002553217113018036\n",
      "Batch 2636/14851, Loss: 0.00419752299785614\n",
      "Batch 2637/14851, Loss: 0.0010581240057945251\n",
      "Batch 2638/14851, Loss: 0.00047120204544626176\n",
      "Batch 2639/14851, Loss: 0.0014929714379832149\n",
      "Batch 2640/14851, Loss: 0.01917278580367565\n",
      "Batch 2641/14851, Loss: 0.008668724447488785\n",
      "Batch 2642/14851, Loss: 0.0016389364609494805\n",
      "Batch 2643/14851, Loss: 0.00056488934205845\n",
      "Batch 2644/14851, Loss: 0.0006177826435305178\n",
      "Batch 2645/14851, Loss: 0.002533989492803812\n",
      "Batch 2646/14851, Loss: 0.002888140734285116\n",
      "Batch 2647/14851, Loss: 0.0004797540605068207\n",
      "Batch 2648/14851, Loss: 0.015538755804300308\n",
      "Batch 2649/14851, Loss: 0.028861405327916145\n",
      "Batch 2650/14851, Loss: 0.0037912516854703426\n",
      "Batch 2651/14851, Loss: 0.017976908013224602\n",
      "Batch 2652/14851, Loss: 0.002651581773534417\n",
      "Batch 2653/14851, Loss: 0.01604657620191574\n",
      "Batch 2654/14851, Loss: 0.031494539231061935\n",
      "Batch 2655/14851, Loss: 0.011068172752857208\n",
      "Batch 2656/14851, Loss: 0.0019067004323005676\n",
      "Batch 2657/14851, Loss: 0.0007319413707591593\n",
      "Batch 2658/14851, Loss: 0.0023235769476741552\n",
      "Batch 2659/14851, Loss: 0.00021737240604124963\n",
      "Batch 2660/14851, Loss: 0.001846572500653565\n",
      "Batch 2661/14851, Loss: 0.013893641531467438\n",
      "Batch 2662/14851, Loss: 0.0011939816176891327\n",
      "Batch 2663/14851, Loss: 0.00011160472786286846\n",
      "Batch 2664/14851, Loss: 0.000977788120508194\n",
      "Batch 2665/14851, Loss: 0.00020299706375226378\n",
      "Batch 2666/14851, Loss: 0.00011949738109251484\n",
      "Batch 2667/14851, Loss: 0.034901801496744156\n",
      "Batch 2668/14851, Loss: 0.000283277768176049\n",
      "Batch 2669/14851, Loss: 0.02018292434513569\n",
      "Batch 2670/14851, Loss: 0.0004174212517682463\n",
      "Batch 2671/14851, Loss: 0.000198797628399916\n",
      "Batch 2672/14851, Loss: 0.000310099363559857\n",
      "Batch 2673/14851, Loss: 0.0046459343284368515\n",
      "Batch 2674/14851, Loss: 0.005558853503316641\n",
      "Batch 2675/14851, Loss: 0.0016277329996228218\n",
      "Batch 2676/14851, Loss: 0.0013687139144167304\n",
      "Batch 2677/14851, Loss: 0.00012136995792388916\n",
      "Batch 2678/14851, Loss: 0.0008958305115811527\n",
      "Batch 2679/14851, Loss: 0.0003308194281999022\n",
      "Batch 2680/14851, Loss: 0.0003942760231439024\n",
      "Batch 2681/14851, Loss: 0.00044485306716524065\n",
      "Batch 2682/14851, Loss: 0.00011993572115898132\n",
      "Batch 2683/14851, Loss: 0.007530474569648504\n",
      "Batch 2684/14851, Loss: 0.00022413581609725952\n",
      "Batch 2685/14851, Loss: 0.004632973577827215\n",
      "Batch 2686/14851, Loss: 0.0008918411913327873\n",
      "Batch 2687/14851, Loss: 0.002856313018128276\n",
      "Batch 2688/14851, Loss: 0.013820177875459194\n",
      "Batch 2689/14851, Loss: 0.0002500675618648529\n",
      "Batch 2690/14851, Loss: 0.0014855278423056006\n",
      "Batch 2691/14851, Loss: 0.025573967024683952\n",
      "Batch 2692/14851, Loss: 0.03497336804866791\n",
      "Batch 2693/14851, Loss: 0.0019676366355270147\n",
      "Batch 2694/14851, Loss: 0.0008554806117899716\n",
      "Batch 2695/14851, Loss: 0.00803647842258215\n",
      "Batch 2696/14851, Loss: 0.030270256102085114\n",
      "Batch 2697/14851, Loss: 0.06901207566261292\n",
      "Batch 2698/14851, Loss: 0.0002869392337743193\n",
      "Batch 2699/14851, Loss: 0.02669953741133213\n",
      "Batch 2700/14851, Loss: 0.02307349629700184\n",
      "Batch 2701/14851, Loss: 3.602231663535349e-05\n",
      "Batch 2702/14851, Loss: 0.0013973055174574256\n",
      "Batch 2703/14851, Loss: 0.03412599116563797\n",
      "Batch 2704/14851, Loss: 0.009027242660522461\n",
      "Batch 2705/14851, Loss: 0.001535537769086659\n",
      "Batch 2706/14851, Loss: 0.021812157705426216\n",
      "Batch 2707/14851, Loss: 0.0013842284679412842\n",
      "Batch 2708/14851, Loss: 0.00038980194949544966\n",
      "Batch 2709/14851, Loss: 0.007117304485291243\n",
      "Batch 2710/14851, Loss: 0.0002948617038782686\n",
      "Batch 2711/14851, Loss: 0.0011887789005413651\n",
      "Batch 2712/14851, Loss: 0.003006623825058341\n",
      "Batch 2713/14851, Loss: 0.0025979578495025635\n",
      "Batch 2714/14851, Loss: 0.007266554981470108\n",
      "Batch 2715/14851, Loss: 0.0037069853860884905\n",
      "Batch 2716/14851, Loss: 0.012457083910703659\n",
      "Batch 2717/14851, Loss: 0.0013641563709825277\n",
      "Batch 2718/14851, Loss: 0.004586823284626007\n",
      "Batch 2719/14851, Loss: 0.037369292229413986\n",
      "Batch 2720/14851, Loss: 0.004710236098617315\n",
      "Batch 2721/14851, Loss: 0.0001048284029820934\n",
      "Batch 2722/14851, Loss: 0.0037366647738963366\n",
      "Batch 2723/14851, Loss: 0.028990885242819786\n",
      "Batch 2724/14851, Loss: 0.0006528806989081204\n",
      "Batch 2725/14851, Loss: 0.0004671337374020368\n",
      "Batch 2726/14851, Loss: 0.0027105920016765594\n",
      "Batch 2727/14851, Loss: 0.007380553986877203\n",
      "Batch 2728/14851, Loss: 0.0017068771412596107\n",
      "Batch 2729/14851, Loss: 0.004925149958580732\n",
      "Batch 2730/14851, Loss: 0.01849743351340294\n",
      "Batch 2731/14851, Loss: 0.0006361218984238803\n",
      "Batch 2732/14851, Loss: 0.0002949895861092955\n",
      "Batch 2733/14851, Loss: 0.0017608007183298469\n",
      "Batch 2734/14851, Loss: 0.00011869271838804707\n",
      "Batch 2735/14851, Loss: 0.028593728318810463\n",
      "Batch 2736/14851, Loss: 0.04948032274842262\n",
      "Batch 2737/14851, Loss: 0.026102758944034576\n",
      "Batch 2738/14851, Loss: 0.001834418624639511\n",
      "Batch 2739/14851, Loss: 0.0009226575493812561\n",
      "Batch 2740/14851, Loss: 0.0009287339053116739\n",
      "Batch 2741/14851, Loss: 0.0034268845338374376\n",
      "Batch 2742/14851, Loss: 0.0014338294276967645\n",
      "Batch 2743/14851, Loss: 0.01816290058195591\n",
      "Batch 2744/14851, Loss: 0.0002585090696811676\n",
      "Batch 2745/14851, Loss: 0.00021157458832021803\n",
      "Batch 2746/14851, Loss: 0.0018703825771808624\n",
      "Batch 2747/14851, Loss: 0.03982966020703316\n",
      "Batch 2748/14851, Loss: 0.0076809050515294075\n",
      "Batch 2749/14851, Loss: 0.004073369782418013\n",
      "Batch 2750/14851, Loss: 0.00027179470635019243\n",
      "Batch 2751/14851, Loss: 0.010418214835226536\n",
      "Batch 2752/14851, Loss: 0.0008816953049972653\n",
      "Batch 2753/14851, Loss: 0.00011726468801498413\n",
      "Batch 2754/14851, Loss: 0.04330044612288475\n",
      "Batch 2755/14851, Loss: 0.0074999975040555\n",
      "Batch 2756/14851, Loss: 0.00019617060024756938\n",
      "Batch 2757/14851, Loss: 0.008528576232492924\n",
      "Batch 2758/14851, Loss: 0.0076642632484436035\n",
      "Batch 2759/14851, Loss: 0.0038541238754987717\n",
      "Batch 2760/14851, Loss: 0.0007305977051146328\n",
      "Batch 2761/14851, Loss: 9.648122068028897e-05\n",
      "Batch 2762/14851, Loss: 0.026282694190740585\n",
      "Batch 2763/14851, Loss: 0.0011637331917881966\n",
      "Batch 2764/14851, Loss: 4.7557055950164795e-05\n",
      "Batch 2765/14851, Loss: 0.0179876871407032\n",
      "Batch 2766/14851, Loss: 0.002940304111689329\n",
      "Batch 2767/14851, Loss: 0.002554837381467223\n",
      "Batch 2768/14851, Loss: 0.0006769659812562168\n",
      "Batch 2769/14851, Loss: 0.001803562045097351\n",
      "Batch 2770/14851, Loss: 0.021548250690102577\n",
      "Batch 2771/14851, Loss: 0.00025326013565063477\n",
      "Batch 2772/14851, Loss: 0.0007879668846726418\n",
      "Batch 2773/14851, Loss: 0.00018019601702690125\n",
      "Batch 2774/14851, Loss: 0.03255854919552803\n",
      "Batch 2775/14851, Loss: 0.021945839747786522\n",
      "Batch 2776/14851, Loss: 0.1369522660970688\n",
      "Batch 2777/14851, Loss: 0.00020845483231823891\n",
      "Batch 2778/14851, Loss: 0.00024714379105716944\n",
      "Batch 2779/14851, Loss: 0.00034182556555606425\n",
      "Batch 2780/14851, Loss: 0.00035300906165502965\n",
      "Batch 2781/14851, Loss: 0.001172402175143361\n",
      "Batch 2782/14851, Loss: 3.517419099807739e-05\n",
      "Batch 2783/14851, Loss: 0.0007314248359762132\n",
      "Batch 2784/14851, Loss: 0.016608471050858498\n",
      "Batch 2785/14851, Loss: 0.016011444851756096\n",
      "Batch 2786/14851, Loss: 0.0008601248264312744\n",
      "Batch 2787/14851, Loss: 0.00014881789684295654\n",
      "Batch 2788/14851, Loss: 0.0017417722847312689\n",
      "Batch 2789/14851, Loss: 0.0001340980379609391\n",
      "Batch 2790/14851, Loss: 0.040368519723415375\n",
      "Batch 2791/14851, Loss: 0.0027068923227488995\n",
      "Batch 2792/14851, Loss: 0.01759379357099533\n",
      "Batch 2793/14851, Loss: 0.021412381902337074\n",
      "Batch 2794/14851, Loss: 0.0010560242226347327\n",
      "Batch 2795/14851, Loss: 0.0007063607918098569\n",
      "Batch 2796/14851, Loss: 0.02385035902261734\n",
      "Batch 2797/14851, Loss: 0.005801872815936804\n",
      "Batch 2798/14851, Loss: 0.00013122342352289706\n",
      "Batch 2799/14851, Loss: 0.003942948766052723\n",
      "Batch 2800/14851, Loss: 0.014534487389028072\n",
      "Batch 2801/14851, Loss: 0.05997929349541664\n",
      "Batch 2802/14851, Loss: 0.0013002479681745172\n",
      "Batch 2803/14851, Loss: 0.0004521148803178221\n",
      "Batch 2804/14851, Loss: 0.004897291772067547\n",
      "Batch 2805/14851, Loss: 0.018032213672995567\n",
      "Batch 2806/14851, Loss: 0.011450580321252346\n",
      "Batch 2807/14851, Loss: 0.001972945174202323\n",
      "Batch 2808/14851, Loss: 0.026674490422010422\n",
      "Batch 2809/14851, Loss: 0.0021584650967270136\n",
      "Batch 2810/14851, Loss: 0.002590431598946452\n",
      "Batch 2811/14851, Loss: 0.002117274096235633\n",
      "Batch 2812/14851, Loss: 0.0019560668151825666\n",
      "Batch 2813/14851, Loss: 0.0006232981686480343\n",
      "Batch 2814/14851, Loss: 0.017703870311379433\n",
      "Batch 2815/14851, Loss: 0.00018358354282099754\n",
      "Batch 2816/14851, Loss: 0.013037172146141529\n",
      "Batch 2817/14851, Loss: 0.0004770896630361676\n",
      "Batch 2818/14851, Loss: 0.05071679502725601\n",
      "Batch 2819/14851, Loss: 0.000459735601907596\n",
      "Batch 2820/14851, Loss: 0.0006382623687386513\n",
      "Batch 2821/14851, Loss: 0.001941479742527008\n",
      "Batch 2822/14851, Loss: 0.00016715626406949013\n",
      "Batch 2823/14851, Loss: 0.010587255470454693\n",
      "Batch 2824/14851, Loss: 0.0007861318881623447\n",
      "Batch 2825/14851, Loss: 0.009230360388755798\n",
      "Batch 2826/14851, Loss: 0.014875717461109161\n",
      "Batch 2827/14851, Loss: 0.006129299756139517\n",
      "Batch 2828/14851, Loss: 0.006905831396579742\n",
      "Batch 2829/14851, Loss: 0.00015650440764147788\n",
      "Batch 2830/14851, Loss: 0.0009609423577785492\n",
      "Batch 2831/14851, Loss: 0.005670023616403341\n",
      "Batch 2832/14851, Loss: 6.256873166421428e-05\n",
      "Batch 2833/14851, Loss: 0.00520344590768218\n",
      "Batch 2834/14851, Loss: 0.0001196091398014687\n",
      "Batch 2835/14851, Loss: 0.001199512043967843\n",
      "Batch 2836/14851, Loss: 0.020161079242825508\n",
      "Batch 2837/14851, Loss: 0.006777593400329351\n",
      "Batch 2838/14851, Loss: 0.0006922483444213867\n",
      "Batch 2839/14851, Loss: 0.08422796428203583\n",
      "Batch 2840/14851, Loss: 0.01596326008439064\n",
      "Batch 2841/14851, Loss: 0.004780517425388098\n",
      "Batch 2842/14851, Loss: 0.008992599323391914\n",
      "Batch 2843/14851, Loss: 0.0005543294246308506\n",
      "Batch 2844/14851, Loss: 0.005896471440792084\n",
      "Batch 2845/14851, Loss: 0.0001819332392187789\n",
      "Batch 2846/14851, Loss: 0.0007504423265345395\n",
      "Batch 2847/14851, Loss: 0.03380828723311424\n",
      "Batch 2848/14851, Loss: 0.0004158902738709003\n",
      "Batch 2849/14851, Loss: 0.00028691565967164934\n",
      "Batch 2850/14851, Loss: 0.007167236413806677\n",
      "Batch 2851/14851, Loss: 0.01207449845969677\n",
      "Batch 2852/14851, Loss: 0.002575565129518509\n",
      "Batch 2853/14851, Loss: 0.002583238063380122\n",
      "Batch 2854/14851, Loss: 0.007098794914782047\n",
      "Batch 2855/14851, Loss: 0.008721742779016495\n",
      "Batch 2856/14851, Loss: 0.0033066757023334503\n",
      "Batch 2857/14851, Loss: 0.016260715201497078\n",
      "Batch 2858/14851, Loss: 0.0024154249113053083\n",
      "Batch 2859/14851, Loss: 0.007980002090334892\n",
      "Batch 2860/14851, Loss: 0.0012130551040172577\n",
      "Batch 2861/14851, Loss: 0.0021169136743992567\n",
      "Batch 2862/14851, Loss: 0.0069663976319134235\n",
      "Batch 2863/14851, Loss: 0.00035587200545705855\n",
      "Batch 2864/14851, Loss: 0.0060453289188444614\n",
      "Batch 2865/14851, Loss: 0.007607369218021631\n",
      "Batch 2866/14851, Loss: 0.00030111768865026534\n",
      "Batch 2867/14851, Loss: 0.0014499165117740631\n",
      "Batch 2868/14851, Loss: 0.001217670738697052\n",
      "Batch 2869/14851, Loss: 0.0019012929406017065\n",
      "Batch 2870/14851, Loss: 0.015371630899608135\n",
      "Batch 2871/14851, Loss: 0.001419849693775177\n",
      "Batch 2872/14851, Loss: 0.000539672386366874\n",
      "Batch 2873/14851, Loss: 0.005871977657079697\n",
      "Batch 2874/14851, Loss: 0.00929234642535448\n",
      "Batch 2875/14851, Loss: 0.01891414076089859\n",
      "Batch 2876/14851, Loss: 0.002469355706125498\n",
      "Batch 2877/14851, Loss: 0.004430953413248062\n",
      "Batch 2878/14851, Loss: 0.0007147950236685574\n",
      "Batch 2879/14851, Loss: 0.007666737772524357\n",
      "Batch 2880/14851, Loss: 0.006869003176689148\n",
      "Batch 2881/14851, Loss: 0.0005634439294226468\n",
      "Batch 2882/14851, Loss: 0.0004063509404659271\n",
      "Batch 2883/14851, Loss: 0.00491187022998929\n",
      "Batch 2884/14851, Loss: 0.05880771204829216\n",
      "Batch 2885/14851, Loss: 0.0012418117839843035\n",
      "Batch 2886/14851, Loss: 0.0018100879387930036\n",
      "Batch 2887/14851, Loss: 0.01523062027990818\n",
      "Batch 2888/14851, Loss: 0.04089896380901337\n",
      "Batch 2889/14851, Loss: 0.0020185091998428106\n",
      "Batch 2890/14851, Loss: 0.00010137632489204407\n",
      "Batch 2891/14851, Loss: 0.002323675900697708\n",
      "Batch 2892/14851, Loss: 0.001005473779514432\n",
      "Batch 2893/14851, Loss: 0.022932352498173714\n",
      "Batch 2894/14851, Loss: 0.0034897399600595236\n",
      "Batch 2895/14851, Loss: 0.015116119757294655\n",
      "Batch 2896/14851, Loss: 0.020382316783070564\n",
      "Batch 2897/14851, Loss: 0.10362718254327774\n",
      "Batch 2898/14851, Loss: 0.008664712309837341\n",
      "Batch 2899/14851, Loss: 0.001691861660219729\n",
      "Batch 2900/14851, Loss: 0.004774324595928192\n",
      "Batch 2901/14851, Loss: 0.0036282262299209833\n",
      "Batch 2902/14851, Loss: 0.004130091983824968\n",
      "Batch 2903/14851, Loss: 0.013429571874439716\n",
      "Batch 2904/14851, Loss: 0.056831538677215576\n",
      "Batch 2905/14851, Loss: 0.016714811325073242\n",
      "Batch 2906/14851, Loss: 0.0058286115527153015\n",
      "Batch 2907/14851, Loss: 0.002288859337568283\n",
      "Batch 2908/14851, Loss: 0.0002677738666534424\n",
      "Batch 2909/14851, Loss: 0.014537430368363857\n",
      "Batch 2910/14851, Loss: 0.030340755358338356\n",
      "Batch 2911/14851, Loss: 0.014377018436789513\n",
      "Batch 2912/14851, Loss: 0.0006640851497650146\n",
      "Batch 2913/14851, Loss: 0.0066557712852954865\n",
      "Batch 2914/14851, Loss: 0.0018384667346253991\n",
      "Batch 2915/14851, Loss: 0.0003136148152407259\n",
      "Batch 2916/14851, Loss: 0.006727555301040411\n",
      "Batch 2917/14851, Loss: 0.0021011782810091972\n",
      "Batch 2918/14851, Loss: 0.001612179446965456\n",
      "Batch 2919/14851, Loss: 0.0010787583887577057\n",
      "Batch 2920/14851, Loss: 0.021688148379325867\n",
      "Batch 2921/14851, Loss: 0.029563089832663536\n",
      "Batch 2922/14851, Loss: 0.0027265208773314953\n",
      "Batch 2923/14851, Loss: 0.0064745633862912655\n",
      "Batch 2924/14851, Loss: 0.0032141990959644318\n",
      "Batch 2925/14851, Loss: 0.05150243267416954\n",
      "Batch 2926/14851, Loss: 0.015294229611754417\n",
      "Batch 2927/14851, Loss: 0.0003198931517545134\n",
      "Batch 2928/14851, Loss: 0.03413533791899681\n",
      "Batch 2929/14851, Loss: 0.0020260296296328306\n",
      "Batch 2930/14851, Loss: 0.0074117728509008884\n",
      "Batch 2931/14851, Loss: 0.0036930229980498552\n",
      "Batch 2932/14851, Loss: 4.4124823034508154e-05\n",
      "Batch 2933/14851, Loss: 0.03053249791264534\n",
      "Batch 2934/14851, Loss: 0.001113642007112503\n",
      "Batch 2935/14851, Loss: 5.634625631500967e-05\n",
      "Batch 2936/14851, Loss: 0.00215308484621346\n",
      "Batch 2937/14851, Loss: 0.043681200593709946\n",
      "Batch 2938/14851, Loss: 0.0003565152583178133\n",
      "Batch 2939/14851, Loss: 0.01520441472530365\n",
      "Batch 2940/14851, Loss: 0.010960496962070465\n",
      "Batch 2941/14851, Loss: 0.007079143077135086\n",
      "Batch 2942/14851, Loss: 0.0014494622591882944\n",
      "Batch 2943/14851, Loss: 0.000383360922569409\n",
      "Batch 2944/14851, Loss: 0.005134230945259333\n",
      "Batch 2945/14851, Loss: 0.0032479960937052965\n",
      "Batch 2946/14851, Loss: 0.0019964103121310472\n",
      "Batch 2947/14851, Loss: 0.07067737728357315\n",
      "Batch 2948/14851, Loss: 0.00021922712039668113\n",
      "Batch 2949/14851, Loss: 0.00016552333545405418\n",
      "Batch 2950/14851, Loss: 0.0001718302519293502\n",
      "Batch 2951/14851, Loss: 0.036439042538404465\n",
      "Batch 2952/14851, Loss: 0.006899632513523102\n",
      "Batch 2953/14851, Loss: 0.00012827136379200965\n",
      "Batch 2954/14851, Loss: 0.007611386477947235\n",
      "Batch 2955/14851, Loss: 0.02652970515191555\n",
      "Batch 2956/14851, Loss: 0.0004231346247252077\n",
      "Batch 2957/14851, Loss: 0.0012700321385636926\n",
      "Batch 2958/14851, Loss: 0.029654936864972115\n",
      "Batch 2959/14851, Loss: 0.003215869190171361\n",
      "Batch 2960/14851, Loss: 0.00323880510404706\n",
      "Batch 2961/14851, Loss: 0.0012404548469930887\n",
      "Batch 2962/14851, Loss: 0.0058750733733177185\n",
      "Batch 2963/14851, Loss: 0.038495611399412155\n",
      "Batch 2964/14851, Loss: 0.0014309337129816413\n",
      "Batch 2965/14851, Loss: 0.0471918024122715\n",
      "Batch 2966/14851, Loss: 0.051176708191633224\n",
      "Batch 2967/14851, Loss: 0.01309072133153677\n",
      "Batch 2968/14851, Loss: 0.005504754837602377\n",
      "Batch 2969/14851, Loss: 0.0007939177448861301\n",
      "Batch 2970/14851, Loss: 0.0009885331382974982\n",
      "Batch 2971/14851, Loss: 0.0004761653544846922\n",
      "Batch 2972/14851, Loss: 0.00036437809467315674\n",
      "Batch 2973/14851, Loss: 0.01941501908004284\n",
      "Batch 2974/14851, Loss: 0.0002892377378884703\n",
      "Batch 2975/14851, Loss: 0.0005116984248161316\n",
      "Batch 2976/14851, Loss: 0.0005930945044383407\n",
      "Batch 2977/14851, Loss: 0.021189572289586067\n",
      "Batch 2978/14851, Loss: 0.0017091588815674186\n",
      "Batch 2979/14851, Loss: 0.0003393693477846682\n",
      "Batch 2980/14851, Loss: 0.002661142498254776\n",
      "Batch 2981/14851, Loss: 0.008906804025173187\n",
      "Batch 2982/14851, Loss: 1.1041760444641113e-05\n",
      "Batch 2983/14851, Loss: 0.09314753115177155\n",
      "Batch 2984/14851, Loss: 0.00020033493638038635\n",
      "Batch 2985/14851, Loss: 0.015395747497677803\n",
      "Batch 2986/14851, Loss: 0.012428452260792255\n",
      "Batch 2987/14851, Loss: 0.048637088388204575\n",
      "Batch 2988/14851, Loss: 0.004627407994121313\n",
      "Batch 2989/14851, Loss: 0.025632567703723907\n",
      "Batch 2990/14851, Loss: 0.005323570221662521\n",
      "Batch 2991/14851, Loss: 0.0064163231290876865\n",
      "Batch 2992/14851, Loss: 0.0018633442232385278\n",
      "Batch 2993/14851, Loss: 0.007688432931900024\n",
      "Batch 2994/14851, Loss: 0.014928595162928104\n",
      "Batch 2995/14851, Loss: 0.002142593264579773\n",
      "Batch 2996/14851, Loss: 0.002749506151303649\n",
      "Batch 2997/14851, Loss: 8.89350994839333e-05\n",
      "Batch 2998/14851, Loss: 0.07697822898626328\n",
      "Batch 2999/14851, Loss: 0.03125061094760895\n",
      "Batch 3000/14851, Loss: 0.0023822623770684004\n",
      "Batch 3001/14851, Loss: 0.0067306747660040855\n",
      "Batch 3002/14851, Loss: 0.00803251750767231\n",
      "Batch 3003/14851, Loss: 0.007977435365319252\n",
      "Batch 3004/14851, Loss: 0.0006710712914355099\n",
      "Batch 3005/14851, Loss: 0.003402245230972767\n",
      "Batch 3006/14851, Loss: 0.0041772788390517235\n",
      "Batch 3007/14851, Loss: 2.5803843527683057e-05\n",
      "Batch 3008/14851, Loss: 0.023099923506379128\n",
      "Batch 3009/14851, Loss: 0.005480028688907623\n",
      "Batch 3010/14851, Loss: 0.02293713577091694\n",
      "Batch 3011/14851, Loss: 0.00022567808628082275\n",
      "Batch 3012/14851, Loss: 0.0037827640771865845\n",
      "Batch 3013/14851, Loss: 0.010494102723896503\n",
      "Batch 3014/14851, Loss: 0.0017230521189048886\n",
      "Batch 3015/14851, Loss: 0.02386680617928505\n",
      "Batch 3016/14851, Loss: 0.011142285540699959\n",
      "Batch 3017/14851, Loss: 0.0005387489800341427\n",
      "Batch 3018/14851, Loss: 5.1292281568748876e-05\n",
      "Batch 3019/14851, Loss: 0.001052743406035006\n",
      "Batch 3020/14851, Loss: 0.00019735748355742544\n",
      "Batch 3021/14851, Loss: 0.006179122254252434\n",
      "Batch 3022/14851, Loss: 0.009063980542123318\n",
      "Batch 3023/14851, Loss: 0.00022413581609725952\n",
      "Batch 3024/14851, Loss: 0.00082496675895527\n",
      "Batch 3025/14851, Loss: 0.06828922778367996\n",
      "Batch 3026/14851, Loss: 0.012769379653036594\n",
      "Batch 3027/14851, Loss: 0.001719077816233039\n",
      "Batch 3028/14851, Loss: 0.016554195433855057\n",
      "Batch 3029/14851, Loss: 0.0009458847343921661\n",
      "Batch 3030/14851, Loss: 0.0005561473662965\n",
      "Batch 3031/14851, Loss: 0.0018798025557771325\n",
      "Batch 3032/14851, Loss: 0.007684225216507912\n",
      "Batch 3033/14851, Loss: 0.046677205711603165\n",
      "Batch 3034/14851, Loss: 0.0052507310174405575\n",
      "Batch 3035/14851, Loss: 0.0006301489775069058\n",
      "Batch 3036/14851, Loss: 0.0005418124492280185\n",
      "Batch 3037/14851, Loss: 3.712872785399668e-05\n",
      "Batch 3038/14851, Loss: 0.00894959643483162\n",
      "Batch 3039/14851, Loss: 0.033032093197107315\n",
      "Batch 3040/14851, Loss: 0.00025866428040899336\n",
      "Batch 3041/14851, Loss: 0.004695835057646036\n",
      "Batch 3042/14851, Loss: 0.05936429277062416\n",
      "Batch 3043/14851, Loss: 0.013514681719243526\n",
      "Batch 3044/14851, Loss: 0.010693123564124107\n",
      "Batch 3045/14851, Loss: 0.007738308981060982\n",
      "Batch 3046/14851, Loss: 0.0005966474418528378\n",
      "Batch 3047/14851, Loss: 0.040801677852869034\n",
      "Batch 3048/14851, Loss: 0.024927575141191483\n",
      "Batch 3049/14851, Loss: 0.0012695988407358527\n",
      "Batch 3050/14851, Loss: 0.032990798354148865\n",
      "Batch 3051/14851, Loss: 0.0011548484908416867\n",
      "Batch 3052/14851, Loss: 0.0030358347576111555\n",
      "Batch 3053/14851, Loss: 0.003239768324419856\n",
      "Batch 3054/14851, Loss: 0.0002404103724984452\n",
      "Batch 3055/14851, Loss: 0.03294007480144501\n",
      "Batch 3056/14851, Loss: 0.0013916587922722101\n",
      "Batch 3057/14851, Loss: 0.001197889680042863\n",
      "Batch 3058/14851, Loss: 0.019406592473387718\n",
      "Batch 3059/14851, Loss: 0.011829749681055546\n",
      "Batch 3060/14851, Loss: 0.00619256729260087\n",
      "Batch 3061/14851, Loss: 0.0010666139423847198\n",
      "Batch 3062/14851, Loss: 0.08839958161115646\n",
      "Batch 3063/14851, Loss: 0.006547512020915747\n",
      "Batch 3064/14851, Loss: 0.02496005780994892\n",
      "Batch 3065/14851, Loss: 0.0019166035344824195\n",
      "Batch 3066/14851, Loss: 0.012693354859948158\n",
      "Batch 3067/14851, Loss: 0.0016665993025526404\n",
      "Batch 3068/14851, Loss: 0.0012092528631910682\n",
      "Batch 3069/14851, Loss: 0.0006942174513824284\n",
      "Batch 3070/14851, Loss: 0.0388336181640625\n",
      "Batch 3071/14851, Loss: 0.0011037469375878572\n",
      "Batch 3072/14851, Loss: 0.0017373119480907917\n",
      "Batch 3073/14851, Loss: 0.0014333639992401004\n",
      "Batch 3074/14851, Loss: 0.01570531539618969\n",
      "Batch 3075/14851, Loss: 0.004340715706348419\n",
      "Batch 3076/14851, Loss: 0.004053659737110138\n",
      "Batch 3077/14851, Loss: 0.010660309344530106\n",
      "Batch 3078/14851, Loss: 0.0008927791495807469\n",
      "Batch 3079/14851, Loss: 0.006327785551548004\n",
      "Batch 3080/14851, Loss: 0.00015522167086601257\n",
      "Batch 3081/14851, Loss: 0.015019298531115055\n",
      "Batch 3082/14851, Loss: 0.002133801579475403\n",
      "Batch 3083/14851, Loss: 0.0023871720768511295\n",
      "Batch 3084/14851, Loss: 0.01076848991215229\n",
      "Batch 3085/14851, Loss: 0.022403696551918983\n",
      "Batch 3086/14851, Loss: 0.005948053207248449\n",
      "Batch 3087/14851, Loss: 7.703196024522185e-05\n",
      "Batch 3088/14851, Loss: 0.0007240238483063877\n",
      "Batch 3089/14851, Loss: 0.05050550773739815\n",
      "Batch 3090/14851, Loss: 0.002833632053807378\n",
      "Batch 3091/14851, Loss: 0.0010607242584228516\n",
      "Batch 3092/14851, Loss: 0.007966471835970879\n",
      "Batch 3093/14851, Loss: 0.011475144885480404\n",
      "Batch 3094/14851, Loss: 0.008473439142107964\n",
      "Batch 3095/14851, Loss: 0.0012305350974202156\n",
      "Batch 3096/14851, Loss: 0.0005545032327063382\n",
      "Batch 3097/14851, Loss: 0.00031801313161849976\n",
      "Batch 3098/14851, Loss: 0.03883056715130806\n",
      "Batch 3099/14851, Loss: 0.003139672102406621\n",
      "Batch 3100/14851, Loss: 0.039186540991067886\n",
      "Batch 3101/14851, Loss: 0.015469090081751347\n",
      "Batch 3102/14851, Loss: 0.011877412907779217\n",
      "Batch 3103/14851, Loss: 0.13131830096244812\n",
      "Batch 3104/14851, Loss: 0.008888547308743\n",
      "Batch 3105/14851, Loss: 0.003048540325835347\n",
      "Batch 3106/14851, Loss: 0.0011511532356962562\n",
      "Batch 3107/14851, Loss: 0.007563494611531496\n",
      "Batch 3108/14851, Loss: 0.0022232788614928722\n",
      "Batch 3109/14851, Loss: 0.007335434667766094\n",
      "Batch 3110/14851, Loss: 0.0013015406439080834\n",
      "Batch 3111/14851, Loss: 0.0022148105781525373\n",
      "Batch 3112/14851, Loss: 0.0003850609064102173\n",
      "Batch 3113/14851, Loss: 0.06942152976989746\n",
      "Batch 3114/14851, Loss: 0.00043540573096834123\n",
      "Batch 3115/14851, Loss: 0.00011573980009416118\n",
      "Batch 3116/14851, Loss: 0.046333517879247665\n",
      "Batch 3117/14851, Loss: 0.015484543517231941\n",
      "Batch 3118/14851, Loss: 0.0039072707295417786\n",
      "Batch 3119/14851, Loss: 0.009343291632831097\n",
      "Batch 3120/14851, Loss: 0.0006033939425833523\n",
      "Batch 3121/14851, Loss: 0.0071794139221310616\n",
      "Batch 3122/14851, Loss: 0.07961200922727585\n",
      "Batch 3123/14851, Loss: 0.0009707940043881536\n",
      "Batch 3124/14851, Loss: 0.005390586797147989\n",
      "Batch 3125/14851, Loss: 0.001146594644524157\n",
      "Batch 3126/14851, Loss: 0.000428209692472592\n",
      "Batch 3127/14851, Loss: 0.040355097502470016\n",
      "Batch 3128/14851, Loss: 0.0003461328742559999\n",
      "Batch 3129/14851, Loss: 0.00010050957644125447\n",
      "Batch 3130/14851, Loss: 0.0013530264841392636\n",
      "Batch 3131/14851, Loss: 0.0005907354061491787\n",
      "Batch 3132/14851, Loss: 0.0033712817821651697\n",
      "Batch 3133/14851, Loss: 0.00035836794995702803\n",
      "Batch 3134/14851, Loss: 0.004151050932705402\n",
      "Batch 3135/14851, Loss: 0.016191182658076286\n",
      "Batch 3136/14851, Loss: 0.004529241006821394\n",
      "Batch 3137/14851, Loss: 0.02607225440442562\n",
      "Batch 3138/14851, Loss: 0.0004265146853867918\n",
      "Batch 3139/14851, Loss: 0.03802236542105675\n",
      "Batch 3140/14851, Loss: 0.00011941732373088598\n",
      "Batch 3141/14851, Loss: 0.00431659072637558\n",
      "Batch 3142/14851, Loss: 0.0204574316740036\n",
      "Batch 3143/14851, Loss: 0.03736346960067749\n",
      "Batch 3144/14851, Loss: 0.0026257161516696215\n",
      "Batch 3145/14851, Loss: 0.0011147571494802833\n",
      "Batch 3146/14851, Loss: 0.03084247186779976\n",
      "Batch 3147/14851, Loss: 0.0017913170158863068\n",
      "Batch 3148/14851, Loss: 0.049283869564533234\n",
      "Batch 3149/14851, Loss: 0.0005721698398701847\n",
      "Batch 3150/14851, Loss: 0.0011755878804251552\n",
      "Batch 3151/14851, Loss: 0.004736794624477625\n",
      "Batch 3152/14851, Loss: 0.017226673662662506\n",
      "Batch 3153/14851, Loss: 0.019429540261626244\n",
      "Batch 3154/14851, Loss: 0.04869168996810913\n",
      "Batch 3155/14851, Loss: 0.010195146314799786\n",
      "Batch 3156/14851, Loss: 0.00032594180083833635\n",
      "Batch 3157/14851, Loss: 0.021090766414999962\n",
      "Batch 3158/14851, Loss: 0.003055319655686617\n",
      "Batch 3159/14851, Loss: 0.018928661942481995\n",
      "Batch 3160/14851, Loss: 0.00699782557785511\n",
      "Batch 3161/14851, Loss: 0.0014328235993161798\n",
      "Batch 3162/14851, Loss: 0.017663879320025444\n",
      "Batch 3163/14851, Loss: 0.0004328054783400148\n",
      "Batch 3164/14851, Loss: 0.01640302874147892\n",
      "Batch 3165/14851, Loss: 0.003124944632872939\n",
      "Batch 3166/14851, Loss: 0.04760110750794411\n",
      "Batch 3167/14851, Loss: 0.0012411579955369234\n",
      "Batch 3168/14851, Loss: 0.001819523167796433\n",
      "Batch 3169/14851, Loss: 0.033078089356422424\n",
      "Batch 3170/14851, Loss: 0.01216118037700653\n",
      "Batch 3171/14851, Loss: 0.0007783050532452762\n",
      "Batch 3172/14851, Loss: 0.010869027115404606\n",
      "Batch 3173/14851, Loss: 0.0005437384243123233\n",
      "Batch 3174/14851, Loss: 0.02944682538509369\n",
      "Batch 3175/14851, Loss: 0.0009267575806006789\n",
      "Batch 3176/14851, Loss: 0.001552686095237732\n",
      "Batch 3177/14851, Loss: 0.0067606461234390736\n",
      "Batch 3178/14851, Loss: 0.026650045067071915\n",
      "Batch 3179/14851, Loss: 0.00780852884054184\n",
      "Batch 3180/14851, Loss: 0.03778354823589325\n",
      "Batch 3181/14851, Loss: 0.0002838149666786194\n",
      "Batch 3182/14851, Loss: 6.274133920669556e-05\n",
      "Batch 3183/14851, Loss: 0.00029796361923217773\n",
      "Batch 3184/14851, Loss: 0.022887539118528366\n",
      "Batch 3185/14851, Loss: 0.0003380191919859499\n",
      "Batch 3186/14851, Loss: 0.01273749116808176\n",
      "Batch 3187/14851, Loss: 0.03235198184847832\n",
      "Batch 3188/14851, Loss: 0.029384499415755272\n",
      "Batch 3189/14851, Loss: 0.001071872771717608\n",
      "Batch 3190/14851, Loss: 0.026578357443213463\n",
      "Batch 3191/14851, Loss: 0.07817696034908295\n",
      "Batch 3192/14851, Loss: 0.0004998035728931427\n",
      "Batch 3193/14851, Loss: 0.002148145344108343\n",
      "Batch 3194/14851, Loss: 0.010512576438486576\n",
      "Batch 3195/14851, Loss: 0.0027974385302513838\n",
      "Batch 3196/14851, Loss: 0.0019505067029967904\n",
      "Batch 3197/14851, Loss: 0.0007043834193609655\n",
      "Batch 3198/14851, Loss: 0.001909736543893814\n",
      "Batch 3199/14851, Loss: 0.01589789055287838\n",
      "Batch 3200/14851, Loss: 0.008351004682481289\n",
      "Batch 3201/14851, Loss: 0.0013102032244205475\n",
      "Batch 3202/14851, Loss: 0.035306453704833984\n",
      "Batch 3203/14851, Loss: 0.0028772465884685516\n",
      "Batch 3204/14851, Loss: 0.0553450770676136\n",
      "Batch 3205/14851, Loss: 0.007989391684532166\n",
      "Batch 3206/14851, Loss: 0.00432781595736742\n",
      "Batch 3207/14851, Loss: 0.0021791260223835707\n",
      "Batch 3208/14851, Loss: 0.029191404581069946\n",
      "Batch 3209/14851, Loss: 0.0005071672494523227\n",
      "Batch 3210/14851, Loss: 0.001550227403640747\n",
      "Batch 3211/14851, Loss: 0.005281072109937668\n",
      "Batch 3212/14851, Loss: 0.0024244599044322968\n",
      "Batch 3213/14851, Loss: 0.0077820271253585815\n",
      "Batch 3214/14851, Loss: 0.0678141638636589\n",
      "Batch 3215/14851, Loss: 0.004814427811652422\n",
      "Batch 3216/14851, Loss: 0.0033978528808802366\n",
      "Batch 3217/14851, Loss: 0.014646156691014767\n",
      "Batch 3218/14851, Loss: 0.043156418949365616\n",
      "Batch 3219/14851, Loss: 0.017439810559153557\n",
      "Batch 3220/14851, Loss: 0.0068145571276545525\n",
      "Batch 3221/14851, Loss: 0.002627722453325987\n",
      "Batch 3222/14851, Loss: 0.012212998233735561\n",
      "Batch 3223/14851, Loss: 0.01120733842253685\n",
      "Batch 3224/14851, Loss: 0.0024462181609123945\n",
      "Batch 3225/14851, Loss: 0.01306318212300539\n",
      "Batch 3226/14851, Loss: 0.0034572677686810493\n",
      "Batch 3227/14851, Loss: 0.0059664249420166016\n",
      "Batch 3228/14851, Loss: 0.003004642901942134\n",
      "Batch 3229/14851, Loss: 0.03767024353146553\n",
      "Batch 3230/14851, Loss: 0.00983040127903223\n",
      "Batch 3231/14851, Loss: 0.004654115531593561\n",
      "Batch 3232/14851, Loss: 0.0543404184281826\n",
      "Batch 3233/14851, Loss: 0.0002826365234795958\n",
      "Batch 3234/14851, Loss: 0.016388781368732452\n",
      "Batch 3235/14851, Loss: 0.004849368240684271\n",
      "Batch 3236/14851, Loss: 0.0015779336681589484\n",
      "Batch 3237/14851, Loss: 0.07496701180934906\n",
      "Batch 3238/14851, Loss: 0.0012006000615656376\n",
      "Batch 3239/14851, Loss: 0.06269531697034836\n",
      "Batch 3240/14851, Loss: 0.0031626832205802202\n",
      "Batch 3241/14851, Loss: 0.01195814460515976\n",
      "Batch 3242/14851, Loss: 0.018633438274264336\n",
      "Batch 3243/14851, Loss: 0.011037473566830158\n",
      "Batch 3244/14851, Loss: 0.005042890552431345\n",
      "Batch 3245/14851, Loss: 0.022662362083792686\n",
      "Batch 3246/14851, Loss: 0.00043972829007543623\n",
      "Batch 3247/14851, Loss: 0.0011442243121564388\n",
      "Batch 3248/14851, Loss: 0.0023169214837253094\n",
      "Batch 3249/14851, Loss: 0.001351927756331861\n",
      "Batch 3250/14851, Loss: 0.0013430898543447256\n",
      "Batch 3251/14851, Loss: 0.0008695001597516239\n",
      "Batch 3252/14851, Loss: 0.01894444040954113\n",
      "Batch 3253/14851, Loss: 0.0065583596006035805\n",
      "Batch 3254/14851, Loss: 0.00032982975244522095\n",
      "Batch 3255/14851, Loss: 0.006037494633346796\n",
      "Batch 3256/14851, Loss: 0.013442513532936573\n",
      "Batch 3257/14851, Loss: 0.016490807756781578\n",
      "Batch 3258/14851, Loss: 0.003062353702262044\n",
      "Batch 3259/14851, Loss: 0.012297926470637321\n",
      "Batch 3260/14851, Loss: 0.0005980990827083588\n",
      "Batch 3261/14851, Loss: 0.002293335972353816\n",
      "Batch 3262/14851, Loss: 0.0031859015580266714\n",
      "Batch 3263/14851, Loss: 0.01811881922185421\n",
      "Batch 3264/14851, Loss: 0.026164283975958824\n",
      "Batch 3265/14851, Loss: 0.008353372104465961\n",
      "Batch 3266/14851, Loss: 0.04937324300408363\n",
      "Batch 3267/14851, Loss: 0.00023204211902339011\n",
      "Batch 3268/14851, Loss: 0.001769576221704483\n",
      "Batch 3269/14851, Loss: 0.0021357100922614336\n",
      "Batch 3270/14851, Loss: 0.06538139283657074\n",
      "Batch 3271/14851, Loss: 0.0014373846352100372\n",
      "Batch 3272/14851, Loss: 0.016381757333874702\n",
      "Batch 3273/14851, Loss: 0.003338448703289032\n",
      "Batch 3274/14851, Loss: 0.01351603027433157\n",
      "Batch 3275/14851, Loss: 0.0029856304172426462\n",
      "Batch 3276/14851, Loss: 0.002655117539688945\n",
      "Batch 3277/14851, Loss: 0.007897363044321537\n",
      "Batch 3278/14851, Loss: 0.04131793975830078\n",
      "Batch 3279/14851, Loss: 0.005767083261162043\n",
      "Batch 3280/14851, Loss: 0.0015543901827186346\n",
      "Batch 3281/14851, Loss: 0.00199146312661469\n",
      "Batch 3282/14851, Loss: 0.0012412758078426123\n",
      "Batch 3283/14851, Loss: 0.00490716053172946\n",
      "Batch 3284/14851, Loss: 0.010392843745648861\n",
      "Batch 3285/14851, Loss: 0.003457770450040698\n",
      "Batch 3286/14851, Loss: 0.01033778116106987\n",
      "Batch 3287/14851, Loss: 0.0013776073465123773\n",
      "Batch 3288/14851, Loss: 0.012509413063526154\n",
      "Batch 3289/14851, Loss: 0.0031966830138117075\n",
      "Batch 3290/14851, Loss: 0.0009055347763933241\n",
      "Batch 3291/14851, Loss: 0.0006993624265305698\n",
      "Batch 3292/14851, Loss: 0.0010641637491062284\n",
      "Batch 3293/14851, Loss: 0.00942718330770731\n",
      "Batch 3294/14851, Loss: 0.016231713816523552\n",
      "Batch 3295/14851, Loss: 0.02890932746231556\n",
      "Batch 3296/14851, Loss: 0.012344184331595898\n",
      "Batch 3297/14851, Loss: 0.0031393656972795725\n",
      "Batch 3298/14851, Loss: 0.01245726179331541\n",
      "Batch 3299/14851, Loss: 0.0028804701287299395\n",
      "Batch 3300/14851, Loss: 0.0619354173541069\n",
      "Batch 3301/14851, Loss: 0.005399889778345823\n",
      "Batch 3302/14851, Loss: 0.009709704667329788\n",
      "Batch 3303/14851, Loss: 0.013460250571370125\n",
      "Batch 3304/14851, Loss: 0.025319375097751617\n",
      "Batch 3305/14851, Loss: 0.02903386391699314\n",
      "Batch 3306/14851, Loss: 0.019965587183833122\n",
      "Batch 3307/14851, Loss: 0.0630439892411232\n",
      "Batch 3308/14851, Loss: 0.005751785822212696\n",
      "Batch 3309/14851, Loss: 0.0051482925191521645\n",
      "Batch 3310/14851, Loss: 0.017960267141461372\n",
      "Batch 3311/14851, Loss: 0.012888100929558277\n",
      "Batch 3312/14851, Loss: 0.025336677208542824\n",
      "Batch 3313/14851, Loss: 0.003489589784294367\n",
      "Batch 3314/14851, Loss: 0.006372450385242701\n",
      "Batch 3315/14851, Loss: 0.0002014872879954055\n",
      "Batch 3316/14851, Loss: 0.005667115096002817\n",
      "Batch 3317/14851, Loss: 0.03620808944106102\n",
      "Batch 3318/14851, Loss: 0.0007077184855006635\n",
      "Batch 3319/14851, Loss: 0.013715853914618492\n",
      "Batch 3320/14851, Loss: 0.00045937547110952437\n",
      "Batch 3321/14851, Loss: 0.009618972428143024\n",
      "Batch 3322/14851, Loss: 0.044203054159879684\n",
      "Batch 3323/14851, Loss: 0.002014794619753957\n",
      "Batch 3324/14851, Loss: 0.0284404419362545\n",
      "Batch 3325/14851, Loss: 0.0014845281839370728\n",
      "Batch 3326/14851, Loss: 0.0005743156070820987\n",
      "Batch 3327/14851, Loss: 0.00019920493650715798\n",
      "Batch 3328/14851, Loss: 0.004137847572565079\n",
      "Batch 3329/14851, Loss: 0.0010521874064579606\n",
      "Batch 3330/14851, Loss: 0.010414388962090015\n",
      "Batch 3331/14851, Loss: 0.010320714674890041\n",
      "Batch 3332/14851, Loss: 0.016997694969177246\n",
      "Batch 3333/14851, Loss: 0.015849323943257332\n",
      "Batch 3334/14851, Loss: 0.0015837347600609064\n",
      "Batch 3335/14851, Loss: 0.027691563591361046\n",
      "Batch 3336/14851, Loss: 0.0052904412150382996\n",
      "Batch 3337/14851, Loss: 0.07313612848520279\n",
      "Batch 3338/14851, Loss: 0.00083099928451702\n",
      "Batch 3339/14851, Loss: 0.00975043885409832\n",
      "Batch 3340/14851, Loss: 0.00020390872668940574\n",
      "Batch 3341/14851, Loss: 0.021488269791007042\n",
      "Batch 3342/14851, Loss: 0.007000589277595282\n",
      "Batch 3343/14851, Loss: 0.0011052327463403344\n",
      "Batch 3344/14851, Loss: 0.0037004698533564806\n",
      "Batch 3345/14851, Loss: 0.0016396766295656562\n",
      "Batch 3346/14851, Loss: 0.0027033500373363495\n",
      "Batch 3347/14851, Loss: 0.07015720009803772\n",
      "Batch 3348/14851, Loss: 0.05269662290811539\n",
      "Batch 3349/14851, Loss: 0.0018745725974440575\n",
      "Batch 3350/14851, Loss: 0.0006993723218329251\n",
      "Batch 3351/14851, Loss: 0.034784600138664246\n",
      "Batch 3352/14851, Loss: 0.0007465093513019383\n",
      "Batch 3353/14851, Loss: 0.021623704582452774\n",
      "Batch 3354/14851, Loss: 0.04610631987452507\n",
      "Batch 3355/14851, Loss: 0.00010222444689134136\n",
      "Batch 3356/14851, Loss: 0.0009228487615473568\n",
      "Batch 3357/14851, Loss: 0.0010373257100582123\n",
      "Batch 3358/14851, Loss: 0.010264388285577297\n",
      "Batch 3359/14851, Loss: 0.03590255603194237\n",
      "Batch 3360/14851, Loss: 0.010761029087007046\n",
      "Batch 3361/14851, Loss: 0.021529922261834145\n",
      "Batch 3362/14851, Loss: 0.0016311457147821784\n",
      "Batch 3363/14851, Loss: 0.0002773118612822145\n",
      "Batch 3364/14851, Loss: 0.0023249287623912096\n",
      "Batch 3365/14851, Loss: 0.0016093602171167731\n",
      "Batch 3366/14851, Loss: 0.01605691760778427\n",
      "Batch 3367/14851, Loss: 0.0011040804674848914\n",
      "Batch 3368/14851, Loss: 0.00340300053358078\n",
      "Batch 3369/14851, Loss: 0.006681421305984259\n",
      "Batch 3370/14851, Loss: 0.011197730898857117\n",
      "Batch 3371/14851, Loss: 0.0022908071987330914\n",
      "Batch 3372/14851, Loss: 0.00031359304557554424\n",
      "Batch 3373/14851, Loss: 0.00300983595661819\n",
      "Batch 3374/14851, Loss: 0.0020001803059130907\n",
      "Batch 3375/14851, Loss: 0.003679706482216716\n",
      "Batch 3376/14851, Loss: 0.00020357593894004822\n",
      "Batch 3377/14851, Loss: 0.07258658856153488\n",
      "Batch 3378/14851, Loss: 0.04772783815860748\n",
      "Batch 3379/14851, Loss: 0.00552181014791131\n",
      "Batch 3380/14851, Loss: 0.003916122484952211\n",
      "Batch 3381/14851, Loss: 0.0002810284495353699\n",
      "Batch 3382/14851, Loss: 0.00014909107994753867\n",
      "Batch 3383/14851, Loss: 0.004170224070549011\n",
      "Batch 3384/14851, Loss: 0.00037510195397771895\n",
      "Batch 3385/14851, Loss: 5.0966937124030665e-05\n",
      "Batch 3386/14851, Loss: 0.001330585335381329\n",
      "Batch 3387/14851, Loss: 0.00018529330554883927\n",
      "Batch 3388/14851, Loss: 0.005772795528173447\n",
      "Batch 3389/14851, Loss: 0.0010902894427999854\n",
      "Batch 3390/14851, Loss: 0.015421362593770027\n",
      "Batch 3391/14851, Loss: 0.005332058761268854\n",
      "Batch 3392/14851, Loss: 0.02467038296163082\n",
      "Batch 3393/14851, Loss: 0.0013501705834642053\n",
      "Batch 3394/14851, Loss: 0.0084085026755929\n",
      "Batch 3395/14851, Loss: 0.00034600868821144104\n",
      "Batch 3396/14851, Loss: 0.00014835472393315285\n",
      "Batch 3397/14851, Loss: 0.002306939335539937\n",
      "Batch 3398/14851, Loss: 0.017648309469223022\n",
      "Batch 3399/14851, Loss: 0.023458484560251236\n",
      "Batch 3400/14851, Loss: 0.026643607765436172\n",
      "Batch 3401/14851, Loss: 0.00024687894620001316\n",
      "Batch 3402/14851, Loss: 0.0019007400842383504\n",
      "Batch 3403/14851, Loss: 0.014125917106866837\n",
      "Batch 3404/14851, Loss: 0.03539503738284111\n",
      "Batch 3405/14851, Loss: 0.0034676690120249987\n",
      "Batch 3406/14851, Loss: 0.005779917351901531\n",
      "Batch 3407/14851, Loss: 0.02735505998134613\n",
      "Batch 3408/14851, Loss: 0.009168865159153938\n",
      "Batch 3409/14851, Loss: 0.0012147650122642517\n",
      "Batch 3410/14851, Loss: 0.009827501140534878\n",
      "Batch 3411/14851, Loss: 0.08068910241127014\n",
      "Batch 3412/14851, Loss: 0.00930777844041586\n",
      "Batch 3413/14851, Loss: 0.004392193630337715\n",
      "Batch 3414/14851, Loss: 0.008143097162246704\n",
      "Batch 3415/14851, Loss: 0.004503200761973858\n",
      "Batch 3416/14851, Loss: 0.003702635644003749\n",
      "Batch 3417/14851, Loss: 0.04985519126057625\n",
      "Batch 3418/14851, Loss: 0.011265982873737812\n",
      "Batch 3419/14851, Loss: 0.003910786006599665\n",
      "Batch 3420/14851, Loss: 0.08340010046958923\n",
      "Batch 3421/14851, Loss: 0.019109923392534256\n",
      "Batch 3422/14851, Loss: 0.005180705338716507\n",
      "Batch 3423/14851, Loss: 0.008877596817910671\n",
      "Batch 3424/14851, Loss: 0.002648352412506938\n",
      "Batch 3425/14851, Loss: 0.0033771346788853407\n",
      "Batch 3426/14851, Loss: 0.0009700276423245668\n",
      "Batch 3427/14851, Loss: 0.001321215182542801\n",
      "Batch 3428/14851, Loss: 0.0069525535218417645\n",
      "Batch 3429/14851, Loss: 0.011702625080943108\n",
      "Batch 3430/14851, Loss: 0.08440200239419937\n",
      "Batch 3431/14851, Loss: 0.02464698813855648\n",
      "Batch 3432/14851, Loss: 0.0170766469091177\n",
      "Batch 3433/14851, Loss: 0.0025742214638739824\n",
      "Batch 3434/14851, Loss: 0.012371022254228592\n",
      "Batch 3435/14851, Loss: 0.03252600133419037\n",
      "Batch 3436/14851, Loss: 0.0004252927901688963\n",
      "Batch 3437/14851, Loss: 0.029202360659837723\n",
      "Batch 3438/14851, Loss: 0.022636787965893745\n",
      "Batch 3439/14851, Loss: 0.006283686496317387\n",
      "Batch 3440/14851, Loss: 0.006460036616772413\n",
      "Batch 3441/14851, Loss: 0.00020902541291434318\n",
      "Batch 3442/14851, Loss: 0.056508325040340424\n",
      "Batch 3443/14851, Loss: 0.001764003187417984\n",
      "Batch 3444/14851, Loss: 0.019736850634217262\n",
      "Batch 3445/14851, Loss: 0.0184287391602993\n",
      "Batch 3446/14851, Loss: 0.005112280137836933\n",
      "Batch 3447/14851, Loss: 0.00014142815780360252\n",
      "Batch 3448/14851, Loss: 0.0017691103275865316\n",
      "Batch 3449/14851, Loss: 0.03862703591585159\n",
      "Batch 3450/14851, Loss: 0.015020642429590225\n",
      "Batch 3451/14851, Loss: 0.0019746022298932076\n",
      "Batch 3452/14851, Loss: 0.030379660427570343\n",
      "Batch 3453/14851, Loss: 0.0008733061258681118\n",
      "Batch 3454/14851, Loss: 0.0012523271143436432\n",
      "Batch 3455/14851, Loss: 0.01594039611518383\n",
      "Batch 3456/14851, Loss: 0.000831324141472578\n",
      "Batch 3457/14851, Loss: 0.006354904267936945\n",
      "Batch 3458/14851, Loss: 0.00015559668827336282\n",
      "Batch 3459/14851, Loss: 0.012622255831956863\n",
      "Batch 3460/14851, Loss: 0.02643299475312233\n",
      "Batch 3461/14851, Loss: 0.000285416841506958\n",
      "Batch 3462/14851, Loss: 0.014838876202702522\n",
      "Batch 3463/14851, Loss: 0.002518224297091365\n",
      "Batch 3464/14851, Loss: 0.012345471419394016\n",
      "Batch 3465/14851, Loss: 0.0010820826282724738\n",
      "Batch 3466/14851, Loss: 0.0034747265744954348\n",
      "Batch 3467/14851, Loss: 0.0071098594926297665\n",
      "Batch 3468/14851, Loss: 0.029696645215153694\n",
      "Batch 3469/14851, Loss: 0.021244099363684654\n",
      "Batch 3470/14851, Loss: 0.0003951874969061464\n",
      "Batch 3471/14851, Loss: 0.007238768972456455\n",
      "Batch 3472/14851, Loss: 0.003234776435419917\n",
      "Batch 3473/14851, Loss: 0.00047155836364254355\n",
      "Batch 3474/14851, Loss: 0.001752503216266632\n",
      "Batch 3475/14851, Loss: 0.009966454468667507\n",
      "Batch 3476/14851, Loss: 0.029650622978806496\n",
      "Batch 3477/14851, Loss: 0.006138899829238653\n",
      "Batch 3478/14851, Loss: 0.0022545941174030304\n",
      "Batch 3479/14851, Loss: 0.007816980592906475\n",
      "Batch 3480/14851, Loss: 0.0006392747163772583\n",
      "Batch 3481/14851, Loss: 0.0043839276768267155\n",
      "Batch 3482/14851, Loss: 0.03761419281363487\n",
      "Batch 3483/14851, Loss: 0.00047647085739299655\n",
      "Batch 3484/14851, Loss: 0.034438081085681915\n",
      "Batch 3485/14851, Loss: 0.0012918139109387994\n",
      "Batch 3486/14851, Loss: 0.045257363468408585\n",
      "Batch 3487/14851, Loss: 0.02320781536400318\n",
      "Batch 3488/14851, Loss: 0.016672786325216293\n",
      "Batch 3489/14851, Loss: 0.011564853601157665\n",
      "Batch 3490/14851, Loss: 0.003136697458103299\n",
      "Batch 3491/14851, Loss: 0.00938328355550766\n",
      "Batch 3492/14851, Loss: 0.0003200508654117584\n",
      "Batch 3493/14851, Loss: 0.005827364046126604\n",
      "Batch 3494/14851, Loss: 0.0008279035682789981\n",
      "Batch 3495/14851, Loss: 0.000224129602429457\n",
      "Batch 3496/14851, Loss: 0.0013854807475581765\n",
      "Batch 3497/14851, Loss: 0.007892259396612644\n",
      "Batch 3498/14851, Loss: 0.003334940643981099\n",
      "Batch 3499/14851, Loss: 0.0004998259246349335\n",
      "Batch 3500/14851, Loss: 0.0019874146673828363\n",
      "Batch 3501/14851, Loss: 0.030473727732896805\n",
      "Batch 3502/14851, Loss: 0.003245115280151367\n",
      "Batch 3503/14851, Loss: 0.004425302147865295\n",
      "Batch 3504/14851, Loss: 0.00983299221843481\n",
      "Batch 3505/14851, Loss: 0.0018560030730441213\n",
      "Batch 3506/14851, Loss: 0.023936226963996887\n",
      "Batch 3507/14851, Loss: 0.007417305838316679\n",
      "Batch 3508/14851, Loss: 0.010846336372196674\n",
      "Batch 3509/14851, Loss: 8.14621671452187e-05\n",
      "Batch 3510/14851, Loss: 0.001762943807989359\n",
      "Batch 3511/14851, Loss: 0.015102836303412914\n",
      "Batch 3512/14851, Loss: 0.016366353258490562\n",
      "Batch 3513/14851, Loss: 0.0015275543555617332\n",
      "Batch 3514/14851, Loss: 0.018103424459695816\n",
      "Batch 3515/14851, Loss: 0.023695887997746468\n",
      "Batch 3516/14851, Loss: 0.03306152671575546\n",
      "Batch 3517/14851, Loss: 0.0004097546043340117\n",
      "Batch 3518/14851, Loss: 0.002530505182221532\n",
      "Batch 3519/14851, Loss: 0.0368315614759922\n",
      "Batch 3520/14851, Loss: 0.00518898805603385\n",
      "Batch 3521/14851, Loss: 0.0009500297601334751\n",
      "Batch 3522/14851, Loss: 0.0022724606096744537\n",
      "Batch 3523/14851, Loss: 0.019815217703580856\n",
      "Batch 3524/14851, Loss: 0.04638494923710823\n",
      "Batch 3525/14851, Loss: 0.015855077654123306\n",
      "Batch 3526/14851, Loss: 0.0020241066813468933\n",
      "Batch 3527/14851, Loss: 0.0005111706559546292\n",
      "Batch 3528/14851, Loss: 0.000944144616369158\n",
      "Batch 3529/14851, Loss: 0.002049732953310013\n",
      "Batch 3530/14851, Loss: 0.00891115888953209\n",
      "Batch 3531/14851, Loss: 0.045938584953546524\n",
      "Batch 3532/14851, Loss: 0.0007748802308924496\n",
      "Batch 3533/14851, Loss: 0.00019262607384007424\n",
      "Batch 3534/14851, Loss: 0.00015272323798853904\n",
      "Batch 3535/14851, Loss: 0.03249053657054901\n",
      "Batch 3536/14851, Loss: 0.005100564565509558\n",
      "Batch 3537/14851, Loss: 0.019416408613324165\n",
      "Batch 3538/14851, Loss: 0.0012393234064802527\n",
      "Batch 3539/14851, Loss: 0.0012115798890590668\n",
      "Batch 3540/14851, Loss: 0.0013691807398572564\n",
      "Batch 3541/14851, Loss: 0.0015448977937921882\n",
      "Batch 3542/14851, Loss: 0.007963350974023342\n",
      "Batch 3543/14851, Loss: 0.002265690127387643\n",
      "Batch 3544/14851, Loss: 0.0026237007696181536\n",
      "Batch 3545/14851, Loss: 0.02107764594256878\n",
      "Batch 3546/14851, Loss: 0.004920605104416609\n",
      "Batch 3547/14851, Loss: 0.0014982635620981455\n",
      "Batch 3548/14851, Loss: 0.0017248322255909443\n",
      "Batch 3549/14851, Loss: 0.015254713594913483\n",
      "Batch 3550/14851, Loss: 0.1083112582564354\n",
      "Batch 3551/14851, Loss: 9.220093488693237e-05\n",
      "Batch 3552/14851, Loss: 0.0029565037693828344\n",
      "Batch 3553/14851, Loss: 0.00628674728795886\n",
      "Batch 3554/14851, Loss: 0.006922401022166014\n",
      "Batch 3555/14851, Loss: 0.009639052674174309\n",
      "Batch 3556/14851, Loss: 0.0055255889892578125\n",
      "Batch 3557/14851, Loss: 0.0001445064990548417\n",
      "Batch 3558/14851, Loss: 0.022020980715751648\n",
      "Batch 3559/14851, Loss: 0.0009047972853295505\n",
      "Batch 3560/14851, Loss: 0.0008279600297100842\n",
      "Batch 3561/14851, Loss: 0.0007122767274267972\n",
      "Batch 3562/14851, Loss: 0.00292591517791152\n",
      "Batch 3563/14851, Loss: 0.011925237253308296\n",
      "Batch 3564/14851, Loss: 0.007894624955952168\n",
      "Batch 3565/14851, Loss: 0.018929537385702133\n",
      "Batch 3566/14851, Loss: 0.009940555319190025\n",
      "Batch 3567/14851, Loss: 0.0018292987952008843\n",
      "Batch 3568/14851, Loss: 0.0035114188212901354\n",
      "Batch 3569/14851, Loss: 0.0029295857530087233\n",
      "Batch 3570/14851, Loss: 0.0004054222663398832\n",
      "Batch 3571/14851, Loss: 0.011037751100957394\n",
      "Batch 3572/14851, Loss: 0.003201328217983246\n",
      "Batch 3573/14851, Loss: 0.000369158893590793\n",
      "Batch 3574/14851, Loss: 0.004986441694200039\n",
      "Batch 3575/14851, Loss: 0.0022743919398635626\n",
      "Batch 3576/14851, Loss: 0.01592901721596718\n",
      "Batch 3577/14851, Loss: 0.013293503783643246\n",
      "Batch 3578/14851, Loss: 0.006138249766081572\n",
      "Batch 3579/14851, Loss: 0.0031251139007508755\n",
      "Batch 3580/14851, Loss: 0.004852496087551117\n",
      "Batch 3581/14851, Loss: 0.002225542441010475\n",
      "Batch 3582/14851, Loss: 0.016934413462877274\n",
      "Batch 3583/14851, Loss: 0.010329550132155418\n",
      "Batch 3584/14851, Loss: 0.00014188885688781738\n",
      "Batch 3585/14851, Loss: 0.002310642274096608\n",
      "Batch 3586/14851, Loss: 0.001936213462613523\n",
      "Batch 3587/14851, Loss: 0.0006230001454241574\n",
      "Batch 3588/14851, Loss: 0.004358759615570307\n",
      "Batch 3589/14851, Loss: 0.002510189078748226\n",
      "Batch 3590/14851, Loss: 0.22442220151424408\n",
      "Batch 3591/14851, Loss: 0.0005929231410846114\n",
      "Batch 3592/14851, Loss: 0.002443713368847966\n",
      "Batch 3593/14851, Loss: 0.010915255174040794\n",
      "Batch 3594/14851, Loss: 0.0022263554856181145\n",
      "Batch 3595/14851, Loss: 0.0049173953011631966\n",
      "Batch 3596/14851, Loss: 0.0029035296756774187\n",
      "Batch 3597/14851, Loss: 0.024899110198020935\n",
      "Batch 3598/14851, Loss: 0.0007586851716041565\n",
      "Batch 3599/14851, Loss: 0.005238614976406097\n",
      "Batch 3600/14851, Loss: 0.03296365961432457\n",
      "Batch 3601/14851, Loss: 0.025831837207078934\n",
      "Batch 3602/14851, Loss: 0.004895043559372425\n",
      "Batch 3603/14851, Loss: 0.00621138047426939\n",
      "Batch 3604/14851, Loss: 0.020073769614100456\n",
      "Batch 3605/14851, Loss: 0.0025927938986569643\n",
      "Batch 3606/14851, Loss: 0.002097461372613907\n",
      "Batch 3607/14851, Loss: 0.01369349006563425\n",
      "Batch 3608/14851, Loss: 0.00514737144112587\n",
      "Batch 3609/14851, Loss: 0.00278158625587821\n",
      "Batch 3610/14851, Loss: 0.029887069016695023\n",
      "Batch 3611/14851, Loss: 0.0025388002395629883\n",
      "Batch 3612/14851, Loss: 0.0032305484637618065\n",
      "Batch 3613/14851, Loss: 0.004752268549054861\n",
      "Batch 3614/14851, Loss: 0.005483580287545919\n",
      "Batch 3615/14851, Loss: 0.0063886381685733795\n",
      "Batch 3616/14851, Loss: 0.0018379712710157037\n",
      "Batch 3617/14851, Loss: 0.003070191713050008\n",
      "Batch 3618/14851, Loss: 0.006877675652503967\n",
      "Batch 3619/14851, Loss: 0.0023064645938575268\n",
      "Batch 3620/14851, Loss: 0.024498330429196358\n",
      "Batch 3621/14851, Loss: 0.023110954090952873\n",
      "Batch 3622/14851, Loss: 0.016476232558488846\n",
      "Batch 3623/14851, Loss: 0.00921632070094347\n",
      "Batch 3624/14851, Loss: 0.04838104918599129\n",
      "Batch 3625/14851, Loss: 0.0011236952850595117\n",
      "Batch 3626/14851, Loss: 0.001940390677191317\n",
      "Batch 3627/14851, Loss: 0.032217953354120255\n",
      "Batch 3628/14851, Loss: 0.001206427812576294\n",
      "Batch 3629/14851, Loss: 0.034003425389528275\n",
      "Batch 3630/14851, Loss: 0.006312176119536161\n",
      "Batch 3631/14851, Loss: 0.020483121275901794\n",
      "Batch 3632/14851, Loss: 0.003703502006828785\n",
      "Batch 3633/14851, Loss: 0.007917850278317928\n",
      "Batch 3634/14851, Loss: 0.0010339083382859826\n",
      "Batch 3635/14851, Loss: 0.0036815234925597906\n",
      "Batch 3636/14851, Loss: 0.014916201122105122\n",
      "Batch 3637/14851, Loss: 0.008606664836406708\n",
      "Batch 3638/14851, Loss: 0.00018827617168426514\n",
      "Batch 3639/14851, Loss: 0.010625917464494705\n",
      "Batch 3640/14851, Loss: 0.008461222983896732\n",
      "Batch 3641/14851, Loss: 0.00037045529461465776\n",
      "Batch 3642/14851, Loss: 0.005673722829669714\n",
      "Batch 3643/14851, Loss: 0.00010996063792845234\n",
      "Batch 3644/14851, Loss: 0.00036862643901258707\n",
      "Batch 3645/14851, Loss: 0.0033008891623467207\n",
      "Batch 3646/14851, Loss: 0.006517679896205664\n",
      "Batch 3647/14851, Loss: 0.0004810430109500885\n",
      "Batch 3648/14851, Loss: 0.0024177853483706713\n",
      "Batch 3649/14851, Loss: 0.008300774730741978\n",
      "Batch 3650/14851, Loss: 0.048144470900297165\n",
      "Batch 3651/14851, Loss: 0.04003108665347099\n",
      "Batch 3652/14851, Loss: 0.0006041266024112701\n",
      "Batch 3653/14851, Loss: 0.015903618186712265\n",
      "Batch 3654/14851, Loss: 0.0002018647937802598\n",
      "Batch 3655/14851, Loss: 0.009398657828569412\n",
      "Batch 3656/14851, Loss: 0.007880191318690777\n",
      "Batch 3657/14851, Loss: 0.003619721857830882\n",
      "Batch 3658/14851, Loss: 0.0009444679017178714\n",
      "Batch 3659/14851, Loss: 0.07207546383142471\n",
      "Batch 3660/14851, Loss: 0.0015367480227723718\n",
      "Batch 3661/14851, Loss: 0.002767391735687852\n",
      "Batch 3662/14851, Loss: 0.006115559954196215\n",
      "Batch 3663/14851, Loss: 0.04796956479549408\n",
      "Batch 3664/14851, Loss: 0.005810187663882971\n",
      "Batch 3665/14851, Loss: 0.013009638525545597\n",
      "Batch 3666/14851, Loss: 0.004416012670844793\n",
      "Batch 3667/14851, Loss: 0.00917547196149826\n",
      "Batch 3668/14851, Loss: 0.0007427796954289079\n",
      "Batch 3669/14851, Loss: 0.015875423327088356\n",
      "Batch 3670/14851, Loss: 0.01812787353992462\n",
      "Batch 3671/14851, Loss: 0.00035746541107073426\n",
      "Batch 3672/14851, Loss: 0.001491812407039106\n",
      "Batch 3673/14851, Loss: 0.0016693932702764869\n",
      "Batch 3674/14851, Loss: 0.0009162474889308214\n",
      "Batch 3675/14851, Loss: 0.0025512087158858776\n",
      "Batch 3676/14851, Loss: 0.0011092131026089191\n",
      "Batch 3677/14851, Loss: 0.0022859349846839905\n",
      "Batch 3678/14851, Loss: 0.002669369336217642\n",
      "Batch 3679/14851, Loss: 0.026982219889760017\n",
      "Batch 3680/14851, Loss: 0.008842105977237225\n",
      "Batch 3681/14851, Loss: 0.002882175613194704\n",
      "Batch 3682/14851, Loss: 0.006689450237900019\n",
      "Batch 3683/14851, Loss: 0.025959070771932602\n",
      "Batch 3684/14851, Loss: 0.007304397877305746\n",
      "Batch 3685/14851, Loss: 0.0020463522523641586\n",
      "Batch 3686/14851, Loss: 0.0026467074640095234\n",
      "Batch 3687/14851, Loss: 0.014161616563796997\n",
      "Batch 3688/14851, Loss: 0.007353552151471376\n",
      "Batch 3689/14851, Loss: 0.0040292199701070786\n",
      "Batch 3690/14851, Loss: 0.0017469549784436822\n",
      "Batch 3691/14851, Loss: 0.0016187732107937336\n",
      "Batch 3692/14851, Loss: 0.003011246444657445\n",
      "Batch 3693/14851, Loss: 0.01712777465581894\n",
      "Batch 3694/14851, Loss: 0.0019075870513916016\n",
      "Batch 3695/14851, Loss: 0.001915695727802813\n",
      "Batch 3696/14851, Loss: 0.016088873147964478\n",
      "Batch 3697/14851, Loss: 0.001211211085319519\n",
      "Batch 3698/14851, Loss: 0.015378490090370178\n",
      "Batch 3699/14851, Loss: 0.001879574148915708\n",
      "Batch 3700/14851, Loss: 0.048671871423721313\n",
      "Batch 3701/14851, Loss: 0.00039654597640037537\n",
      "Batch 3702/14851, Loss: 0.041298650205135345\n",
      "Batch 3703/14851, Loss: 0.0006922688917256892\n",
      "Batch 3704/14851, Loss: 0.01357258204370737\n",
      "Batch 3705/14851, Loss: 0.009499249048531055\n",
      "Batch 3706/14851, Loss: 0.007301612291485071\n",
      "Batch 3707/14851, Loss: 0.011628903448581696\n",
      "Batch 3708/14851, Loss: 0.0071689672768116\n",
      "Batch 3709/14851, Loss: 0.002074269577860832\n",
      "Batch 3710/14851, Loss: 0.06284317374229431\n",
      "Batch 3711/14851, Loss: 0.0068509834818542\n",
      "Batch 3712/14851, Loss: 0.005342017859220505\n",
      "Batch 3713/14851, Loss: 0.0004162987170275301\n",
      "Batch 3714/14851, Loss: 0.002058799145743251\n",
      "Batch 3715/14851, Loss: 0.004893303848803043\n",
      "Batch 3716/14851, Loss: 0.016058754175901413\n",
      "Batch 3717/14851, Loss: 0.0044611780904233456\n",
      "Batch 3718/14851, Loss: 0.03124449960887432\n",
      "Batch 3719/14851, Loss: 0.04302443563938141\n",
      "Batch 3720/14851, Loss: 0.009674439206719398\n",
      "Batch 3721/14851, Loss: 0.0034037106670439243\n",
      "Batch 3722/14851, Loss: 0.012784255668520927\n",
      "Batch 3723/14851, Loss: 0.0016431224066764116\n",
      "Batch 3724/14851, Loss: 0.06444288045167923\n",
      "Batch 3725/14851, Loss: 0.011821459047496319\n",
      "Batch 3726/14851, Loss: 0.007466381881386042\n",
      "Batch 3727/14851, Loss: 0.0007740395958535373\n",
      "Batch 3728/14851, Loss: 0.004140790551900864\n",
      "Batch 3729/14851, Loss: 0.022681038826704025\n",
      "Batch 3730/14851, Loss: 0.049461375921964645\n",
      "Batch 3731/14851, Loss: 0.018351247534155846\n",
      "Batch 3732/14851, Loss: 0.01583702117204666\n",
      "Batch 3733/14851, Loss: 0.04252508282661438\n",
      "Batch 3734/14851, Loss: 0.014857765287160873\n",
      "Batch 3735/14851, Loss: 0.07530341297388077\n",
      "Batch 3736/14851, Loss: 0.002583067864179611\n",
      "Batch 3737/14851, Loss: 0.003246378619223833\n",
      "Batch 3738/14851, Loss: 0.006177784409373999\n",
      "Batch 3739/14851, Loss: 2.823770046234131e-05\n",
      "Batch 3740/14851, Loss: 0.013528299517929554\n",
      "Batch 3741/14851, Loss: 0.009388227015733719\n",
      "Batch 3742/14851, Loss: 0.011522084474563599\n",
      "Batch 3743/14851, Loss: 0.028352240100502968\n",
      "Batch 3744/14851, Loss: 0.000153118118760176\n",
      "Batch 3745/14851, Loss: 0.009317166171967983\n",
      "Batch 3746/14851, Loss: 0.0016803765902295709\n",
      "Batch 3747/14851, Loss: 0.0003192499279975891\n",
      "Batch 3748/14851, Loss: 0.0013424679636955261\n",
      "Batch 3749/14851, Loss: 0.0017143066506832838\n",
      "Batch 3750/14851, Loss: 0.0012530597159639\n",
      "Batch 3751/14851, Loss: 0.01920510083436966\n",
      "Batch 3752/14851, Loss: 0.01369796134531498\n",
      "Batch 3753/14851, Loss: 0.03864169120788574\n",
      "Batch 3754/14851, Loss: 0.0019832737743854523\n",
      "Batch 3755/14851, Loss: 0.02388523332774639\n",
      "Batch 3756/14851, Loss: 0.0042036972008645535\n",
      "Batch 3757/14851, Loss: 0.0034619171638041735\n",
      "Batch 3758/14851, Loss: 0.03323134034872055\n",
      "Batch 3759/14851, Loss: 0.03324754908680916\n",
      "Batch 3760/14851, Loss: 0.002309970324859023\n",
      "Batch 3761/14851, Loss: 0.0008101200801320374\n",
      "Batch 3762/14851, Loss: 0.02071385085582733\n",
      "Batch 3763/14851, Loss: 0.031215276569128036\n",
      "Batch 3764/14851, Loss: 0.007085148245096207\n",
      "Batch 3765/14851, Loss: 8.840734517434612e-05\n",
      "Batch 3766/14851, Loss: 0.012264655902981758\n",
      "Batch 3767/14851, Loss: 0.04621497541666031\n",
      "Batch 3768/14851, Loss: 0.02229580655694008\n",
      "Batch 3769/14851, Loss: 0.03086194023489952\n",
      "Batch 3770/14851, Loss: 0.0008763099904172122\n",
      "Batch 3771/14851, Loss: 0.00027057414990849793\n",
      "Batch 3772/14851, Loss: 0.0005544473533518612\n",
      "Batch 3773/14851, Loss: 0.0006019088323228061\n",
      "Batch 3774/14851, Loss: 0.006489087361842394\n",
      "Batch 3775/14851, Loss: 0.04034191742539406\n",
      "Batch 3776/14851, Loss: 0.0013616731157526374\n",
      "Batch 3777/14851, Loss: 0.07542189955711365\n",
      "Batch 3778/14851, Loss: 0.03406481072306633\n",
      "Batch 3779/14851, Loss: 0.014284229837357998\n",
      "Batch 3780/14851, Loss: 0.004831210244446993\n",
      "Batch 3781/14851, Loss: 0.04573535546660423\n",
      "Batch 3782/14851, Loss: 0.03262669965624809\n",
      "Batch 3783/14851, Loss: 0.005289481952786446\n",
      "Batch 3784/14851, Loss: 0.001348225399851799\n",
      "Batch 3785/14851, Loss: 0.006707379128783941\n",
      "Batch 3786/14851, Loss: 0.0008162065059877932\n",
      "Batch 3787/14851, Loss: 4.9831967771751806e-05\n",
      "Batch 3788/14851, Loss: 0.00034726536250673234\n",
      "Batch 3789/14851, Loss: 0.02679302729666233\n",
      "Batch 3790/14851, Loss: 0.0011640406446531415\n",
      "Batch 3791/14851, Loss: 0.0021439306437969208\n",
      "Batch 3792/14851, Loss: 0.008217896334826946\n",
      "Batch 3793/14851, Loss: 0.028299817815423012\n",
      "Batch 3794/14851, Loss: 0.001896928995847702\n",
      "Batch 3795/14851, Loss: 0.000445450161350891\n",
      "Batch 3796/14851, Loss: 4.86647077195812e-05\n",
      "Batch 3797/14851, Loss: 0.0003798777761403471\n",
      "Batch 3798/14851, Loss: 0.002472133608534932\n",
      "Batch 3799/14851, Loss: 0.035340774804353714\n",
      "Batch 3800/14851, Loss: 0.001675780862569809\n",
      "Batch 3801/14851, Loss: 0.0009731948375701904\n",
      "Batch 3802/14851, Loss: 0.030756685882806778\n",
      "Batch 3803/14851, Loss: 0.001633908599615097\n",
      "Batch 3804/14851, Loss: 0.003785782726481557\n",
      "Batch 3805/14851, Loss: 0.0022067190147936344\n",
      "Batch 3806/14851, Loss: 0.006699498742818832\n",
      "Batch 3807/14851, Loss: 0.02594996802508831\n",
      "Batch 3808/14851, Loss: 0.14082063734531403\n",
      "Batch 3809/14851, Loss: 0.0007331735105253756\n",
      "Batch 3810/14851, Loss: 0.004075799137353897\n",
      "Batch 3811/14851, Loss: 0.0002400130033493042\n",
      "Batch 3812/14851, Loss: 0.050747085362672806\n",
      "Batch 3813/14851, Loss: 0.019309671595692635\n",
      "Batch 3814/14851, Loss: 0.035131923854351044\n",
      "Batch 3815/14851, Loss: 0.043397340923547745\n",
      "Batch 3816/14851, Loss: 0.004972483497112989\n",
      "Batch 3817/14851, Loss: 0.05965542048215866\n",
      "Batch 3818/14851, Loss: 0.029459381476044655\n",
      "Batch 3819/14851, Loss: 0.0022445537615567446\n",
      "Batch 3820/14851, Loss: 0.039234913885593414\n",
      "Batch 3821/14851, Loss: 0.033932097256183624\n",
      "Batch 3822/14851, Loss: 0.0021964646875858307\n",
      "Batch 3823/14851, Loss: 0.02863420359790325\n",
      "Batch 3824/14851, Loss: 0.0038642820436507463\n",
      "Batch 3825/14851, Loss: 0.01243414543569088\n",
      "Batch 3826/14851, Loss: 0.012014067731797695\n",
      "Batch 3827/14851, Loss: 0.0203033946454525\n",
      "Batch 3828/14851, Loss: 0.03214215487241745\n",
      "Batch 3829/14851, Loss: 0.0003121495246887207\n",
      "Batch 3830/14851, Loss: 0.003927027806639671\n",
      "Batch 3831/14851, Loss: 0.05583084002137184\n",
      "Batch 3832/14851, Loss: 0.00042296326137147844\n",
      "Batch 3833/14851, Loss: 0.0025662865955382586\n",
      "Batch 3834/14851, Loss: 0.0019600701052695513\n",
      "Batch 3835/14851, Loss: 0.00039298832416534424\n",
      "Batch 3836/14851, Loss: 0.04179167374968529\n",
      "Batch 3837/14851, Loss: 0.0021279193460941315\n",
      "Batch 3838/14851, Loss: 0.012348633259534836\n",
      "Batch 3839/14851, Loss: 0.018279150128364563\n",
      "Batch 3840/14851, Loss: 0.0009495243430137634\n",
      "Batch 3841/14851, Loss: 0.0011146771721541882\n",
      "Batch 3842/14851, Loss: 0.0904352068901062\n",
      "Batch 3843/14851, Loss: 0.011064096353948116\n",
      "Batch 3844/14851, Loss: 0.006311908829957247\n",
      "Batch 3845/14851, Loss: 0.011286155320703983\n",
      "Batch 3846/14851, Loss: 0.0008333437144756317\n",
      "Batch 3847/14851, Loss: 0.003415619255974889\n",
      "Batch 3848/14851, Loss: 0.0008907604496926069\n",
      "Batch 3849/14851, Loss: 0.010407070629298687\n",
      "Batch 3850/14851, Loss: 0.016169840469956398\n",
      "Batch 3851/14851, Loss: 0.007362609263509512\n",
      "Batch 3852/14851, Loss: 0.0137864351272583\n",
      "Batch 3853/14851, Loss: 0.006466101389378309\n",
      "Batch 3854/14851, Loss: 0.005639591719955206\n",
      "Batch 3855/14851, Loss: 0.00013169273734092712\n",
      "Batch 3856/14851, Loss: 0.005997702479362488\n",
      "Batch 3857/14851, Loss: 0.0006087844376452267\n",
      "Batch 3858/14851, Loss: 0.012627982534468174\n",
      "Batch 3859/14851, Loss: 0.0012985927751287818\n",
      "Batch 3860/14851, Loss: 0.0007505777175538242\n",
      "Batch 3861/14851, Loss: 0.004242606461048126\n",
      "Batch 3862/14851, Loss: 0.052958231419324875\n",
      "Batch 3863/14851, Loss: 0.00022249419998843223\n",
      "Batch 3864/14851, Loss: 0.0035347354132682085\n",
      "Batch 3865/14851, Loss: 0.006389491260051727\n",
      "Batch 3866/14851, Loss: 0.00023747484374325722\n",
      "Batch 3867/14851, Loss: 0.0016572462627664208\n",
      "Batch 3868/14851, Loss: 0.0048304894007742405\n",
      "Batch 3869/14851, Loss: 0.0015870966017246246\n",
      "Batch 3870/14851, Loss: 0.002348801353946328\n",
      "Batch 3871/14851, Loss: 0.04765067994594574\n",
      "Batch 3872/14851, Loss: 0.006685471627861261\n",
      "Batch 3873/14851, Loss: 0.0036006856244057417\n",
      "Batch 3874/14851, Loss: 0.0017554496880620718\n",
      "Batch 3875/14851, Loss: 0.000334536045556888\n",
      "Batch 3876/14851, Loss: 0.0032855570316314697\n",
      "Batch 3877/14851, Loss: 0.0031128940172493458\n",
      "Batch 3878/14851, Loss: 0.0024898461997509003\n",
      "Batch 3879/14851, Loss: 0.001602455973625183\n",
      "Batch 3880/14851, Loss: 0.017390629276633263\n",
      "Batch 3881/14851, Loss: 0.0014245746424421668\n",
      "Batch 3882/14851, Loss: 0.04098321124911308\n",
      "Batch 3883/14851, Loss: 0.0014728332171216607\n",
      "Batch 3884/14851, Loss: 0.005311771761626005\n",
      "Batch 3885/14851, Loss: 0.011032209731638432\n",
      "Batch 3886/14851, Loss: 0.013884286396205425\n",
      "Batch 3887/14851, Loss: 0.002082156715914607\n",
      "Batch 3888/14851, Loss: 0.004236092325299978\n",
      "Batch 3889/14851, Loss: 0.0006003144080750644\n",
      "Batch 3890/14851, Loss: 0.00042993479291908443\n",
      "Batch 3891/14851, Loss: 0.02137504704296589\n",
      "Batch 3892/14851, Loss: 0.01588604971766472\n",
      "Batch 3893/14851, Loss: 0.009549729526042938\n",
      "Batch 3894/14851, Loss: 9.944277553586289e-05\n",
      "Batch 3895/14851, Loss: 0.0002988576889038086\n",
      "Batch 3896/14851, Loss: 0.0001365281641483307\n",
      "Batch 3897/14851, Loss: 0.00708847725763917\n",
      "Batch 3898/14851, Loss: 0.041987646371126175\n",
      "Batch 3899/14851, Loss: 0.025545088574290276\n",
      "Batch 3900/14851, Loss: 0.029346058145165443\n",
      "Batch 3901/14851, Loss: 0.004636695142835379\n",
      "Batch 3902/14851, Loss: 0.020205780863761902\n",
      "Batch 3903/14851, Loss: 0.04160447418689728\n",
      "Batch 3904/14851, Loss: 0.020482448861002922\n",
      "Batch 3905/14851, Loss: 0.0007807438960298896\n",
      "Batch 3906/14851, Loss: 0.01574849523603916\n",
      "Batch 3907/14851, Loss: 0.01880703493952751\n",
      "Batch 3908/14851, Loss: 0.0013165423879399896\n",
      "Batch 3909/14851, Loss: 0.005478337407112122\n",
      "Batch 3910/14851, Loss: 0.0027245308738201857\n",
      "Batch 3911/14851, Loss: 0.0008124837768264115\n",
      "Batch 3912/14851, Loss: 0.00044004121446050704\n",
      "Batch 3913/14851, Loss: 0.011049552820622921\n",
      "Batch 3914/14851, Loss: 0.0274130180478096\n",
      "Batch 3915/14851, Loss: 0.0011053644120693207\n",
      "Batch 3916/14851, Loss: 0.004173739347606897\n",
      "Batch 3917/14851, Loss: 0.0012178359320387244\n",
      "Batch 3918/14851, Loss: 0.001747415284626186\n",
      "Batch 3919/14851, Loss: 0.008618559688329697\n",
      "Batch 3920/14851, Loss: 0.02053101174533367\n",
      "Batch 3921/14851, Loss: 0.013152929954230785\n",
      "Batch 3922/14851, Loss: 0.0020306892693042755\n",
      "Batch 3923/14851, Loss: 0.03548232093453407\n",
      "Batch 3924/14851, Loss: 0.0004925779066979885\n",
      "Batch 3925/14851, Loss: 0.0015580219915136695\n",
      "Batch 3926/14851, Loss: 0.010041669011116028\n",
      "Batch 3927/14851, Loss: 0.020767638459801674\n",
      "Batch 3928/14851, Loss: 0.01628713123500347\n",
      "Batch 3929/14851, Loss: 0.0011021415702998638\n",
      "Batch 3930/14851, Loss: 0.01587243750691414\n",
      "Batch 3931/14851, Loss: 0.014930260367691517\n",
      "Batch 3932/14851, Loss: 0.0005938691319897771\n",
      "Batch 3933/14851, Loss: 0.024539776146411896\n",
      "Batch 3934/14851, Loss: 0.012534044682979584\n",
      "Batch 3935/14851, Loss: 0.00464450241997838\n",
      "Batch 3936/14851, Loss: 0.000726139813195914\n",
      "Batch 3937/14851, Loss: 0.01412023976445198\n",
      "Batch 3938/14851, Loss: 0.004955617245286703\n",
      "Batch 3939/14851, Loss: 0.005360866896808147\n",
      "Batch 3940/14851, Loss: 0.012955322861671448\n",
      "Batch 3941/14851, Loss: 0.0007817174191586673\n",
      "Batch 3942/14851, Loss: 0.00427434965968132\n",
      "Batch 3943/14851, Loss: 0.0002800027432385832\n",
      "Batch 3944/14851, Loss: 0.00040423497557640076\n",
      "Batch 3945/14851, Loss: 0.00031868243240751326\n",
      "Batch 3946/14851, Loss: 0.04296059161424637\n",
      "Batch 3947/14851, Loss: 0.0031674255151301622\n",
      "Batch 3948/14851, Loss: 0.010010020807385445\n",
      "Batch 3949/14851, Loss: 0.004120908677577972\n",
      "Batch 3950/14851, Loss: 0.0007668166072107852\n",
      "Batch 3951/14851, Loss: 0.0005329996347427368\n",
      "Batch 3952/14851, Loss: 0.0007379527087323368\n",
      "Batch 3953/14851, Loss: 0.008321577683091164\n",
      "Batch 3954/14851, Loss: 0.005223233252763748\n",
      "Batch 3955/14851, Loss: 0.04101165756583214\n",
      "Batch 3956/14851, Loss: 0.0063987127505242825\n",
      "Batch 3957/14851, Loss: 0.018512466922402382\n",
      "Batch 3958/14851, Loss: 0.00024508064961992204\n",
      "Batch 3959/14851, Loss: 0.0016167958965525031\n",
      "Batch 3960/14851, Loss: 0.001021110569126904\n",
      "Batch 3961/14851, Loss: 0.054748084396123886\n",
      "Batch 3962/14851, Loss: 0.005149338394403458\n",
      "Batch 3963/14851, Loss: 0.0011582301231101155\n",
      "Batch 3964/14851, Loss: 0.008872725069522858\n",
      "Batch 3965/14851, Loss: 0.018915265798568726\n",
      "Batch 3966/14851, Loss: 0.009577151387929916\n",
      "Batch 3967/14851, Loss: 0.0016632141778245568\n",
      "Batch 3968/14851, Loss: 0.00130183482542634\n",
      "Batch 3969/14851, Loss: 0.010513345710933208\n",
      "Batch 3970/14851, Loss: 0.0035700153093785048\n",
      "Batch 3971/14851, Loss: 0.003572202054783702\n",
      "Batch 3972/14851, Loss: 0.00019998227071482688\n",
      "Batch 3973/14851, Loss: 0.0012572247069329023\n",
      "Batch 3974/14851, Loss: 0.04121185094118118\n",
      "Batch 3975/14851, Loss: 6.911158561706543e-05\n",
      "Batch 3976/14851, Loss: 0.007716115098446608\n",
      "Batch 3977/14851, Loss: 0.0026864612009376287\n",
      "Batch 3978/14851, Loss: 0.00018953668768517673\n",
      "Batch 3979/14851, Loss: 5.319342017173767e-05\n",
      "Batch 3980/14851, Loss: 0.002653607400134206\n",
      "Batch 3981/14851, Loss: 0.0022865068167448044\n",
      "Batch 3982/14851, Loss: 0.02152891270816326\n",
      "Batch 3983/14851, Loss: 0.008600793778896332\n",
      "Batch 3984/14851, Loss: 0.0965760126709938\n",
      "Batch 3985/14851, Loss: 0.009868190623819828\n",
      "Batch 3986/14851, Loss: 0.0038324135821312666\n",
      "Batch 3987/14851, Loss: 0.004213934298604727\n",
      "Batch 3988/14851, Loss: 0.0039381771348416805\n",
      "Batch 3989/14851, Loss: 0.0002853625628631562\n",
      "Batch 3990/14851, Loss: 0.00038982057594694197\n",
      "Batch 3991/14851, Loss: 0.011508392170071602\n",
      "Batch 3992/14851, Loss: 3.312031549285166e-05\n",
      "Batch 3993/14851, Loss: 0.0009210817515850067\n",
      "Batch 3994/14851, Loss: 0.0005198802100494504\n",
      "Batch 3995/14851, Loss: 0.006122229155153036\n",
      "Batch 3996/14851, Loss: 0.030499454587697983\n",
      "Batch 3997/14851, Loss: 0.02484988048672676\n",
      "Batch 3998/14851, Loss: 0.005713915918022394\n",
      "Batch 3999/14851, Loss: 0.0007257067481987178\n",
      "Batch 4000/14851, Loss: 0.0006994244758971035\n",
      "Batch 4001/14851, Loss: 0.004323536064475775\n",
      "Batch 4002/14851, Loss: 0.002986554056406021\n",
      "Batch 4003/14851, Loss: 0.0016188197769224644\n",
      "Batch 4004/14851, Loss: 0.003906516823917627\n",
      "Batch 4005/14851, Loss: 0.0010815212735906243\n",
      "Batch 4006/14851, Loss: 0.00024972730898298323\n",
      "Batch 4007/14851, Loss: 0.026380637660622597\n",
      "Batch 4008/14851, Loss: 0.0008738060714676976\n",
      "Batch 4009/14851, Loss: 0.00013702984142582864\n",
      "Batch 4010/14851, Loss: 0.0027143731713294983\n",
      "Batch 4011/14851, Loss: 0.0006931166280992329\n",
      "Batch 4012/14851, Loss: 0.00010035807645181194\n",
      "Batch 4013/14851, Loss: 0.0015934854745864868\n",
      "Batch 4014/14851, Loss: 0.0006347869639284909\n",
      "Batch 4015/14851, Loss: 0.02922731079161167\n",
      "Batch 4016/14851, Loss: 0.018976634368300438\n",
      "Batch 4017/14851, Loss: 0.0016547214472666383\n",
      "Batch 4018/14851, Loss: 0.0038533059414476156\n",
      "Batch 4019/14851, Loss: 0.030561961233615875\n",
      "Batch 4020/14851, Loss: 7.0395071816165e-05\n",
      "Batch 4021/14851, Loss: 0.01416582241654396\n",
      "Batch 4022/14851, Loss: 0.012906829826533794\n",
      "Batch 4023/14851, Loss: 0.0014733726857230067\n",
      "Batch 4024/14851, Loss: 0.0008037649095058441\n",
      "Batch 4025/14851, Loss: 0.0044450946152210236\n",
      "Batch 4026/14851, Loss: 0.007808968890458345\n",
      "Batch 4027/14851, Loss: 0.013540816493332386\n",
      "Batch 4028/14851, Loss: 0.001297804992645979\n",
      "Batch 4029/14851, Loss: 0.003940749913454056\n",
      "Batch 4030/14851, Loss: 0.013816014863550663\n",
      "Batch 4031/14851, Loss: 0.0036834031343460083\n",
      "Batch 4032/14851, Loss: 0.0036514142993837595\n",
      "Batch 4033/14851, Loss: 0.0046383086591959\n",
      "Batch 4034/14851, Loss: 0.02012179233133793\n",
      "Batch 4035/14851, Loss: 0.0066819582134485245\n",
      "Batch 4036/14851, Loss: 0.004250774625688791\n",
      "Batch 4037/14851, Loss: 0.005721420515328646\n",
      "Batch 4038/14851, Loss: 0.004456870257854462\n",
      "Batch 4039/14851, Loss: 0.017373034730553627\n",
      "Batch 4040/14851, Loss: 0.0015080663142725825\n",
      "Batch 4041/14851, Loss: 3.380825000931509e-05\n",
      "Batch 4042/14851, Loss: 0.005231683608144522\n",
      "Batch 4043/14851, Loss: 0.003963528200984001\n",
      "Batch 4044/14851, Loss: 0.04013434424996376\n",
      "Batch 4045/14851, Loss: 0.01267281174659729\n",
      "Batch 4046/14851, Loss: 0.03823480382561684\n",
      "Batch 4047/14851, Loss: 0.055645812302827835\n",
      "Batch 4048/14851, Loss: 0.010774710215628147\n",
      "Batch 4049/14851, Loss: 0.00129640509840101\n",
      "Batch 4050/14851, Loss: 0.01799173653125763\n",
      "Batch 4051/14851, Loss: 0.015583517961204052\n",
      "Batch 4052/14851, Loss: 0.026920191943645477\n",
      "Batch 4053/14851, Loss: 0.06892584264278412\n",
      "Batch 4054/14851, Loss: 0.008242557756602764\n",
      "Batch 4055/14851, Loss: 0.0011861895909532905\n",
      "Batch 4056/14851, Loss: 0.09416421502828598\n",
      "Batch 4057/14851, Loss: 0.0018438324332237244\n",
      "Batch 4058/14851, Loss: 0.0037807440385222435\n",
      "Batch 4059/14851, Loss: 0.0011916024377569556\n",
      "Batch 4060/14851, Loss: 0.003631518455222249\n",
      "Batch 4061/14851, Loss: 0.0017499178647994995\n",
      "Batch 4062/14851, Loss: 0.0018787897424772382\n",
      "Batch 4063/14851, Loss: 0.0013497695326805115\n",
      "Batch 4064/14851, Loss: 0.03029165416955948\n",
      "Batch 4065/14851, Loss: 0.005229011178016663\n",
      "Batch 4066/14851, Loss: 0.01328427903354168\n",
      "Batch 4067/14851, Loss: 0.0025097867473959923\n",
      "Batch 4068/14851, Loss: 0.0002988340856973082\n",
      "Batch 4069/14851, Loss: 0.0010934124002233148\n",
      "Batch 4070/14851, Loss: 0.024828827008605003\n",
      "Batch 4071/14851, Loss: 0.0012181935599073768\n",
      "Batch 4072/14851, Loss: 0.021904170513153076\n",
      "Batch 4073/14851, Loss: 0.004556783940643072\n",
      "Batch 4074/14851, Loss: 0.0013171146856620908\n",
      "Batch 4075/14851, Loss: 0.05810580030083656\n",
      "Batch 4076/14851, Loss: 0.0026438061613589525\n",
      "Batch 4077/14851, Loss: 0.006256390362977982\n",
      "Batch 4078/14851, Loss: 0.007820392958819866\n",
      "Batch 4079/14851, Loss: 0.01430453546345234\n",
      "Batch 4080/14851, Loss: 0.008726409636437893\n",
      "Batch 4081/14851, Loss: 0.003772973082959652\n",
      "Batch 4082/14851, Loss: 0.0012528611114248633\n",
      "Batch 4083/14851, Loss: 0.0042839450761675835\n",
      "Batch 4084/14851, Loss: 0.002237178385257721\n",
      "Batch 4085/14851, Loss: 0.00200123293325305\n",
      "Batch 4086/14851, Loss: 0.014909583143889904\n",
      "Batch 4087/14851, Loss: 0.007500398904085159\n",
      "Batch 4088/14851, Loss: 0.00035932534956373274\n",
      "Batch 4089/14851, Loss: 0.04299599677324295\n",
      "Batch 4090/14851, Loss: 0.0055526201613247395\n",
      "Batch 4091/14851, Loss: 0.0030448846518993378\n",
      "Batch 4092/14851, Loss: 0.004071278031915426\n",
      "Batch 4093/14851, Loss: 0.011354279704391956\n",
      "Batch 4094/14851, Loss: 0.0008734594448469579\n",
      "Batch 4095/14851, Loss: 0.013888319954276085\n",
      "Batch 4096/14851, Loss: 0.024626486003398895\n",
      "Batch 4097/14851, Loss: 0.003721653250977397\n",
      "Batch 4098/14851, Loss: 0.012331470847129822\n",
      "Batch 4099/14851, Loss: 0.0017362261423841119\n",
      "Batch 4100/14851, Loss: 0.0005770683055743575\n",
      "Batch 4101/14851, Loss: 0.018575264140963554\n",
      "Batch 4102/14851, Loss: 0.010341585613787174\n",
      "Batch 4103/14851, Loss: 0.0031483520288020372\n",
      "Batch 4104/14851, Loss: 0.009748358279466629\n",
      "Batch 4105/14851, Loss: 0.016484417021274567\n",
      "Batch 4106/14851, Loss: 0.043996427208185196\n",
      "Batch 4107/14851, Loss: 0.005392356310039759\n",
      "Batch 4108/14851, Loss: 0.011166976764798164\n",
      "Batch 4109/14851, Loss: 0.02079523541033268\n",
      "Batch 4110/14851, Loss: 0.005337752401828766\n",
      "Batch 4111/14851, Loss: 0.009988016448915005\n",
      "Batch 4112/14851, Loss: 0.0005312711000442505\n",
      "Batch 4113/14851, Loss: 0.0029571899212896824\n",
      "Batch 4114/14851, Loss: 0.013880349695682526\n",
      "Batch 4115/14851, Loss: 0.013951177708804607\n",
      "Batch 4116/14851, Loss: 0.0048108347691595554\n",
      "Batch 4117/14851, Loss: 6.637970363954082e-05\n",
      "Batch 4118/14851, Loss: 0.0022012621629983187\n",
      "Batch 4119/14851, Loss: 0.005340525880455971\n",
      "Batch 4120/14851, Loss: 0.004050776362419128\n",
      "Batch 4121/14851, Loss: 0.00016995023179333657\n",
      "Batch 4122/14851, Loss: 0.0025175015907734632\n",
      "Batch 4123/14851, Loss: 0.03769465163350105\n",
      "Batch 4124/14851, Loss: 0.0001651744096307084\n",
      "Batch 4125/14851, Loss: 0.04199295490980148\n",
      "Batch 4126/14851, Loss: 0.003607944818213582\n",
      "Batch 4127/14851, Loss: 0.007015641313046217\n",
      "Batch 4128/14851, Loss: 8.441383397439495e-05\n",
      "Batch 4129/14851, Loss: 0.001094592153094709\n",
      "Batch 4130/14851, Loss: 0.006049002055078745\n",
      "Batch 4131/14851, Loss: 0.0003345819714013487\n",
      "Batch 4132/14851, Loss: 0.004551384598016739\n",
      "Batch 4133/14851, Loss: 0.00432741641998291\n",
      "Batch 4134/14851, Loss: 0.007771636825054884\n",
      "Batch 4135/14851, Loss: 0.017005449160933495\n",
      "Batch 4136/14851, Loss: 0.00020348529506009072\n",
      "Batch 4137/14851, Loss: 0.04572109505534172\n",
      "Batch 4138/14851, Loss: 0.004190405365079641\n",
      "Batch 4139/14851, Loss: 6.575509905815125e-05\n",
      "Batch 4140/14851, Loss: 0.00013638164091389626\n",
      "Batch 4141/14851, Loss: 0.007473278325051069\n",
      "Batch 4142/14851, Loss: 0.000903937965631485\n",
      "Batch 4143/14851, Loss: 0.016952456906437874\n",
      "Batch 4144/14851, Loss: 0.002134625567123294\n",
      "Batch 4145/14851, Loss: 0.032280031591653824\n",
      "Batch 4146/14851, Loss: 0.08332367241382599\n",
      "Batch 4147/14851, Loss: 0.006040445063263178\n",
      "Batch 4148/14851, Loss: 0.0012879259884357452\n",
      "Batch 4149/14851, Loss: 0.0024240221828222275\n",
      "Batch 4150/14851, Loss: 0.01818644255399704\n",
      "Batch 4151/14851, Loss: 0.00821076799184084\n",
      "Batch 4152/14851, Loss: 0.016445666551589966\n",
      "Batch 4153/14851, Loss: 0.0002132145018549636\n",
      "Batch 4154/14851, Loss: 0.0002257414162158966\n",
      "Batch 4155/14851, Loss: 0.0046025048941373825\n",
      "Batch 4156/14851, Loss: 0.00015469516802113503\n",
      "Batch 4157/14851, Loss: 0.0003333576023578644\n",
      "Batch 4158/14851, Loss: 0.05969353765249252\n",
      "Batch 4159/14851, Loss: 0.0365452840924263\n",
      "Batch 4160/14851, Loss: 0.0049651130102574825\n",
      "Batch 4161/14851, Loss: 0.002008306561037898\n",
      "Batch 4162/14851, Loss: 0.025820868089795113\n",
      "Batch 4163/14851, Loss: 6.111214315751567e-05\n",
      "Batch 4164/14851, Loss: 0.0017223904142156243\n",
      "Batch 4165/14851, Loss: 0.11711547523736954\n",
      "Batch 4166/14851, Loss: 0.0022474005818367004\n",
      "Batch 4167/14851, Loss: 0.0021738000214099884\n",
      "Batch 4168/14851, Loss: 9.704256808618084e-05\n",
      "Batch 4169/14851, Loss: 0.04360883682966232\n",
      "Batch 4170/14851, Loss: 0.02004881016910076\n",
      "Batch 4171/14851, Loss: 0.0006670815055258572\n",
      "Batch 4172/14851, Loss: 0.019391613081097603\n",
      "Batch 4173/14851, Loss: 0.010203096084296703\n",
      "Batch 4174/14851, Loss: 0.0103038614615798\n",
      "Batch 4175/14851, Loss: 0.0004282641166355461\n",
      "Batch 4176/14851, Loss: 0.032992374151945114\n",
      "Batch 4177/14851, Loss: 0.001719321240670979\n",
      "Batch 4178/14851, Loss: 0.001070501864887774\n",
      "Batch 4179/14851, Loss: 0.0003772340714931488\n",
      "Batch 4180/14851, Loss: 0.019413502886891365\n",
      "Batch 4181/14851, Loss: 0.0008311371202580631\n",
      "Batch 4182/14851, Loss: 0.009608195163309574\n",
      "Batch 4183/14851, Loss: 0.02740444988012314\n",
      "Batch 4184/14851, Loss: 4.649410766432993e-05\n",
      "Batch 4185/14851, Loss: 0.00019374738621991128\n",
      "Batch 4186/14851, Loss: 0.0020805662497878075\n",
      "Batch 4187/14851, Loss: 0.0006862013833597302\n",
      "Batch 4188/14851, Loss: 0.055098358541727066\n",
      "Batch 4189/14851, Loss: 0.0031851434614509344\n",
      "Batch 4190/14851, Loss: 0.0001794832496671006\n",
      "Batch 4191/14851, Loss: 0.002168218605220318\n",
      "Batch 4192/14851, Loss: 0.03127220645546913\n",
      "Batch 4193/14851, Loss: 0.0011749874101951718\n",
      "Batch 4194/14851, Loss: 0.0005529125337488949\n",
      "Batch 4195/14851, Loss: 0.005271554924547672\n",
      "Batch 4196/14851, Loss: 0.08820237964391708\n",
      "Batch 4197/14851, Loss: 0.0030357632786035538\n",
      "Batch 4198/14851, Loss: 0.0070343827828764915\n",
      "Batch 4199/14851, Loss: 0.0025431278627365828\n",
      "Batch 4200/14851, Loss: 0.002929914277046919\n",
      "Batch 4201/14851, Loss: 0.0023635055404156446\n",
      "Batch 4202/14851, Loss: 0.004738325718790293\n",
      "Batch 4203/14851, Loss: 0.0013040764024481177\n",
      "Batch 4204/14851, Loss: 0.055917151272296906\n",
      "Batch 4205/14851, Loss: 0.0008658990263938904\n",
      "Batch 4206/14851, Loss: 0.00030985873308964074\n",
      "Batch 4207/14851, Loss: 0.0038119193632155657\n",
      "Batch 4208/14851, Loss: 0.001830858876928687\n",
      "Batch 4209/14851, Loss: 0.0019730564672499895\n",
      "Batch 4210/14851, Loss: 0.024827733635902405\n",
      "Batch 4211/14851, Loss: 0.0037788853514939547\n",
      "Batch 4212/14851, Loss: 0.0010878212051466107\n",
      "Batch 4213/14851, Loss: 0.1032760739326477\n",
      "Batch 4214/14851, Loss: 0.0015344731509685516\n",
      "Batch 4215/14851, Loss: 0.02430715039372444\n",
      "Batch 4216/14851, Loss: 0.0010777339339256287\n",
      "Batch 4217/14851, Loss: 0.0005800041253678501\n",
      "Batch 4218/14851, Loss: 0.011962341144680977\n",
      "Batch 4219/14851, Loss: 0.012012957595288754\n",
      "Batch 4220/14851, Loss: 0.001851550186984241\n",
      "Batch 4221/14851, Loss: 0.00020712241530418396\n",
      "Batch 4222/14851, Loss: 0.015606404282152653\n",
      "Batch 4223/14851, Loss: 0.050551798194646835\n",
      "Batch 4224/14851, Loss: 0.0009305628482252359\n",
      "Batch 4225/14851, Loss: 0.0026616051327437162\n",
      "Batch 4226/14851, Loss: 0.004318671766668558\n",
      "Batch 4227/14851, Loss: 0.0004236685635987669\n",
      "Batch 4228/14851, Loss: 0.001472608302719891\n",
      "Batch 4229/14851, Loss: 0.003541128244251013\n",
      "Batch 4230/14851, Loss: 0.01814592257142067\n",
      "Batch 4231/14851, Loss: 0.02388710156083107\n",
      "Batch 4232/14851, Loss: 0.0010278100380674005\n",
      "Batch 4233/14851, Loss: 0.00010824700439115986\n",
      "Batch 4234/14851, Loss: 0.0014310168335214257\n",
      "Batch 4235/14851, Loss: 0.010297723114490509\n",
      "Batch 4236/14851, Loss: 0.0007261261343955994\n",
      "Batch 4237/14851, Loss: 0.0004842206835746765\n",
      "Batch 4238/14851, Loss: 0.03742528706789017\n",
      "Batch 4239/14851, Loss: 0.0016116761835291982\n",
      "Batch 4240/14851, Loss: 0.0013867244124412537\n",
      "Batch 4241/14851, Loss: 0.015451072715222836\n",
      "Batch 4242/14851, Loss: 0.00016616178618278354\n",
      "Batch 4243/14851, Loss: 0.00025261499104090035\n",
      "Batch 4244/14851, Loss: 0.03006509505212307\n",
      "Batch 4245/14851, Loss: 0.017932813614606857\n",
      "Batch 4246/14851, Loss: 0.0017713370034471154\n",
      "Batch 4247/14851, Loss: 0.04187346622347832\n",
      "Batch 4248/14851, Loss: 0.00032637393451295793\n",
      "Batch 4249/14851, Loss: 0.000880195468198508\n",
      "Batch 4250/14851, Loss: 0.03324573487043381\n",
      "Batch 4251/14851, Loss: 0.0014682242181152105\n",
      "Batch 4252/14851, Loss: 0.058973561972379684\n",
      "Batch 4253/14851, Loss: 0.0032751436810940504\n",
      "Batch 4254/14851, Loss: 0.0021845619194209576\n",
      "Batch 4255/14851, Loss: 0.0007775803678669035\n",
      "Batch 4256/14851, Loss: 0.0005218448932282627\n",
      "Batch 4257/14851, Loss: 0.001261702855117619\n",
      "Batch 4258/14851, Loss: 0.0002128557680407539\n",
      "Batch 4259/14851, Loss: 0.010171093046665192\n",
      "Batch 4260/14851, Loss: 0.00698845973238349\n",
      "Batch 4261/14851, Loss: 0.0038764073979109526\n",
      "Batch 4262/14851, Loss: 0.017778772860765457\n",
      "Batch 4263/14851, Loss: 0.01494213379919529\n",
      "Batch 4264/14851, Loss: 0.0025610264856368303\n",
      "Batch 4265/14851, Loss: 0.0002673653361853212\n",
      "Batch 4266/14851, Loss: 0.0007418914465233684\n",
      "Batch 4267/14851, Loss: 0.004083264619112015\n",
      "Batch 4268/14851, Loss: 0.005974349100142717\n",
      "Batch 4269/14851, Loss: 0.01584845595061779\n",
      "Batch 4270/14851, Loss: 0.004067694768309593\n",
      "Batch 4271/14851, Loss: 0.005460063461214304\n",
      "Batch 4272/14851, Loss: 9.711956226965412e-05\n",
      "Batch 4273/14851, Loss: 0.015987208113074303\n",
      "Batch 4274/14851, Loss: 0.023484908044338226\n",
      "Batch 4275/14851, Loss: 0.03519194945693016\n",
      "Batch 4276/14851, Loss: 0.018924782052636147\n",
      "Batch 4277/14851, Loss: 0.0201257336884737\n",
      "Batch 4278/14851, Loss: 0.023649271577596664\n",
      "Batch 4279/14851, Loss: 9.220142965205014e-05\n",
      "Batch 4280/14851, Loss: 0.0013580300146713853\n",
      "Batch 4281/14851, Loss: 1.774480006133672e-05\n",
      "Batch 4282/14851, Loss: 0.00199961569160223\n",
      "Batch 4283/14851, Loss: 0.0006335191428661346\n",
      "Batch 4284/14851, Loss: 0.019008291885256767\n",
      "Batch 4285/14851, Loss: 0.001167832757346332\n",
      "Batch 4286/14851, Loss: 0.020909935235977173\n",
      "Batch 4287/14851, Loss: 0.00506730517372489\n",
      "Batch 4288/14851, Loss: 0.0010245604207739234\n",
      "Batch 4289/14851, Loss: 0.0051499311812222\n",
      "Batch 4290/14851, Loss: 0.0009655642206780612\n",
      "Batch 4291/14851, Loss: 0.023226944729685783\n",
      "Batch 4292/14851, Loss: 0.013017370365560055\n",
      "Batch 4293/14851, Loss: 0.01622338593006134\n",
      "Batch 4294/14851, Loss: 0.022251948714256287\n",
      "Batch 4295/14851, Loss: 0.0003539783356245607\n",
      "Batch 4296/14851, Loss: 0.008684355765581131\n",
      "Batch 4297/14851, Loss: 0.0003383470175322145\n",
      "Batch 4298/14851, Loss: 0.018804950639605522\n",
      "Batch 4299/14851, Loss: 0.03462596610188484\n",
      "Batch 4300/14851, Loss: 0.0033739469945430756\n",
      "Batch 4301/14851, Loss: 0.00046044340706430376\n",
      "Batch 4302/14851, Loss: 0.05442279577255249\n",
      "Batch 4303/14851, Loss: 0.0333961583673954\n",
      "Batch 4304/14851, Loss: 0.004954016301780939\n",
      "Batch 4305/14851, Loss: 0.03322058171033859\n",
      "Batch 4306/14851, Loss: 0.0035713932011276484\n",
      "Batch 4307/14851, Loss: 0.0004697901604231447\n",
      "Batch 4308/14851, Loss: 0.0003866450279019773\n",
      "Batch 4309/14851, Loss: 0.001522026490420103\n",
      "Batch 4310/14851, Loss: 0.0010625709546729922\n",
      "Batch 4311/14851, Loss: 0.01615278422832489\n",
      "Batch 4312/14851, Loss: 0.013092881068587303\n",
      "Batch 4313/14851, Loss: 0.0011264237109571695\n",
      "Batch 4314/14851, Loss: 0.0010423511266708374\n",
      "Batch 4315/14851, Loss: 0.03754232078790665\n",
      "Batch 4316/14851, Loss: 0.030681882053613663\n",
      "Batch 4317/14851, Loss: 0.002018451690673828\n",
      "Batch 4318/14851, Loss: 0.022887904196977615\n",
      "Batch 4319/14851, Loss: 0.024607283994555473\n",
      "Batch 4320/14851, Loss: 0.0969778373837471\n",
      "Batch 4321/14851, Loss: 0.0033864863216876984\n",
      "Batch 4322/14851, Loss: 0.0004035028687212616\n",
      "Batch 4323/14851, Loss: 0.0029557175002992153\n",
      "Batch 4324/14851, Loss: 0.007896638475358486\n",
      "Batch 4325/14851, Loss: 0.03835524991154671\n",
      "Batch 4326/14851, Loss: 0.0010151577880606055\n",
      "Batch 4327/14851, Loss: 0.0010597655782476068\n",
      "Batch 4328/14851, Loss: 0.0038129992317408323\n",
      "Batch 4329/14851, Loss: 0.0008051395416259766\n",
      "Batch 4330/14851, Loss: 0.006665194872766733\n",
      "Batch 4331/14851, Loss: 0.00109051913022995\n",
      "Batch 4332/14851, Loss: 0.005119801964610815\n",
      "Batch 4333/14851, Loss: 0.005546326283365488\n",
      "Batch 4334/14851, Loss: 0.002684280276298523\n",
      "Batch 4335/14851, Loss: 0.0003466928901616484\n",
      "Batch 4336/14851, Loss: 0.004898213315755129\n",
      "Batch 4337/14851, Loss: 0.032259199768304825\n",
      "Batch 4338/14851, Loss: 0.001292659668251872\n",
      "Batch 4339/14851, Loss: 0.008888188749551773\n",
      "Batch 4340/14851, Loss: 0.005819356068968773\n",
      "Batch 4341/14851, Loss: 0.0003957065346185118\n",
      "Batch 4342/14851, Loss: 0.00875732209533453\n",
      "Batch 4343/14851, Loss: 0.0029548879247158766\n",
      "Batch 4344/14851, Loss: 0.0018712737364694476\n",
      "Batch 4345/14851, Loss: 0.0025374791584908962\n",
      "Batch 4346/14851, Loss: 0.03468601405620575\n",
      "Batch 4347/14851, Loss: 8.364269888261333e-05\n",
      "Batch 4348/14851, Loss: 0.002579630585387349\n",
      "Batch 4349/14851, Loss: 0.0009923229226842523\n",
      "Batch 4350/14851, Loss: 0.005491702351719141\n",
      "Batch 4351/14851, Loss: 0.00031557504553347826\n",
      "Batch 4352/14851, Loss: 0.009123574942350388\n",
      "Batch 4353/14851, Loss: 0.04848496988415718\n",
      "Batch 4354/14851, Loss: 0.002733201952651143\n",
      "Batch 4355/14851, Loss: 0.034963056445121765\n",
      "Batch 4356/14851, Loss: 0.0010451326379552484\n",
      "Batch 4357/14851, Loss: 0.00029261939926072955\n",
      "Batch 4358/14851, Loss: 0.05496174842119217\n",
      "Batch 4359/14851, Loss: 0.0003475832345429808\n",
      "Batch 4360/14851, Loss: 0.044237785041332245\n",
      "Batch 4361/14851, Loss: 0.0008981066639535129\n",
      "Batch 4362/14851, Loss: 0.016422593966126442\n",
      "Batch 4363/14851, Loss: 0.001056127017363906\n",
      "Batch 4364/14851, Loss: 0.009898184798657894\n",
      "Batch 4365/14851, Loss: 0.0016764340689405799\n",
      "Batch 4366/14851, Loss: 0.04355404153466225\n",
      "Batch 4367/14851, Loss: 0.034569066017866135\n",
      "Batch 4368/14851, Loss: 0.00042854496859945357\n",
      "Batch 4369/14851, Loss: 0.003290284425020218\n",
      "Batch 4370/14851, Loss: 0.007920903153717518\n",
      "Batch 4371/14851, Loss: 0.006412118673324585\n",
      "Batch 4372/14851, Loss: 0.0038290468510240316\n",
      "Batch 4373/14851, Loss: 0.0029081571847200394\n",
      "Batch 4374/14851, Loss: 0.0040150596760213375\n",
      "Batch 4375/14851, Loss: 0.0007913298904895782\n",
      "Batch 4376/14851, Loss: 0.05895322188735008\n",
      "Batch 4377/14851, Loss: 0.04487759992480278\n",
      "Batch 4378/14851, Loss: 0.052381888031959534\n",
      "Batch 4379/14851, Loss: 0.02338641695678234\n",
      "Batch 4380/14851, Loss: 0.002915656892582774\n",
      "Batch 4381/14851, Loss: 0.004032853990793228\n",
      "Batch 4382/14851, Loss: 0.0052614822052419186\n",
      "Batch 4383/14851, Loss: 0.0032108116429299116\n",
      "Batch 4384/14851, Loss: 0.00018292665481567383\n",
      "Batch 4385/14851, Loss: 0.002732145367190242\n",
      "Batch 4386/14851, Loss: 0.0018633989384397864\n",
      "Batch 4387/14851, Loss: 0.003338244976475835\n",
      "Batch 4388/14851, Loss: 0.02184887044131756\n",
      "Batch 4389/14851, Loss: 0.002268243581056595\n",
      "Batch 4390/14851, Loss: 0.001013097702525556\n",
      "Batch 4391/14851, Loss: 0.0012071989476680756\n",
      "Batch 4392/14851, Loss: 0.0024815078359097242\n",
      "Batch 4393/14851, Loss: 0.012042826972901821\n",
      "Batch 4394/14851, Loss: 0.00013171385216992348\n",
      "Batch 4395/14851, Loss: 0.0014900254318490624\n",
      "Batch 4396/14851, Loss: 0.003934366162866354\n",
      "Batch 4397/14851, Loss: 0.023153871297836304\n",
      "Batch 4398/14851, Loss: 0.002162860007956624\n",
      "Batch 4399/14851, Loss: 0.000897873193025589\n",
      "Batch 4400/14851, Loss: 0.08826135098934174\n",
      "Batch 4401/14851, Loss: 0.03888285160064697\n",
      "Batch 4402/14851, Loss: 0.0019642082042992115\n",
      "Batch 4403/14851, Loss: 0.00994359701871872\n",
      "Batch 4404/14851, Loss: 0.03920295462012291\n",
      "Batch 4405/14851, Loss: 0.0381833054125309\n",
      "Batch 4406/14851, Loss: 0.001300673931837082\n",
      "Batch 4407/14851, Loss: 0.01774197444319725\n",
      "Batch 4408/14851, Loss: 0.004824946168810129\n",
      "Batch 4409/14851, Loss: 0.009622332639992237\n",
      "Batch 4410/14851, Loss: 0.02334379032254219\n",
      "Batch 4411/14851, Loss: 0.023870883509516716\n",
      "Batch 4412/14851, Loss: 0.04133891314268112\n",
      "Batch 4413/14851, Loss: 0.006515114102512598\n",
      "Batch 4414/14851, Loss: 0.01557060144841671\n",
      "Batch 4415/14851, Loss: 0.010328472591936588\n",
      "Batch 4416/14851, Loss: 0.013166501186788082\n",
      "Batch 4417/14851, Loss: 0.010462768375873566\n",
      "Batch 4418/14851, Loss: 0.009410685859620571\n",
      "Batch 4419/14851, Loss: 0.002601131796836853\n",
      "Batch 4420/14851, Loss: 0.006997711956501007\n",
      "Batch 4421/14851, Loss: 0.03291577473282814\n",
      "Batch 4422/14851, Loss: 0.03269702568650246\n",
      "Batch 4423/14851, Loss: 0.027459481731057167\n",
      "Batch 4424/14851, Loss: 0.006140959449112415\n",
      "Batch 4425/14851, Loss: 0.009139042347669601\n",
      "Batch 4426/14851, Loss: 0.006654416676610708\n",
      "Batch 4427/14851, Loss: 0.004386900458484888\n",
      "Batch 4428/14851, Loss: 0.009546928107738495\n",
      "Batch 4429/14851, Loss: 0.021668992936611176\n",
      "Batch 4430/14851, Loss: 0.01846257783472538\n",
      "Batch 4431/14851, Loss: 0.016780376434326172\n",
      "Batch 4432/14851, Loss: 0.0006899845902808011\n",
      "Batch 4433/14851, Loss: 0.0030584484338760376\n",
      "Batch 4434/14851, Loss: 0.007082400377839804\n",
      "Batch 4435/14851, Loss: 0.006034732796251774\n",
      "Batch 4436/14851, Loss: 0.044414110481739044\n",
      "Batch 4437/14851, Loss: 0.0034098029136657715\n",
      "Batch 4438/14851, Loss: 0.0012509101070463657\n",
      "Batch 4439/14851, Loss: 0.003582302015274763\n",
      "Batch 4440/14851, Loss: 0.014440023340284824\n",
      "Batch 4441/14851, Loss: 0.006827622652053833\n",
      "Batch 4442/14851, Loss: 0.020276660099625587\n",
      "Batch 4443/14851, Loss: 0.003172938944771886\n",
      "Batch 4444/14851, Loss: 0.0012965871719643474\n",
      "Batch 4445/14851, Loss: 0.0004710545181296766\n",
      "Batch 4446/14851, Loss: 0.0032862003426998854\n",
      "Batch 4447/14851, Loss: 0.02605675905942917\n",
      "Batch 4448/14851, Loss: 0.021667687222361565\n",
      "Batch 4449/14851, Loss: 0.009115063585340977\n",
      "Batch 4450/14851, Loss: 0.00036509832716546953\n",
      "Batch 4451/14851, Loss: 0.0028319358825683594\n",
      "Batch 4452/14851, Loss: 0.0007219277322292328\n",
      "Batch 4453/14851, Loss: 0.03710301220417023\n",
      "Batch 4454/14851, Loss: 0.0031051684636622667\n",
      "Batch 4455/14851, Loss: 8.670240640640259e-05\n",
      "Batch 4456/14851, Loss: 0.00017743308853823692\n",
      "Batch 4457/14851, Loss: 0.004518526140600443\n",
      "Batch 4458/14851, Loss: 0.000635166943538934\n",
      "Batch 4459/14851, Loss: 0.0018514968687668443\n",
      "Batch 4460/14851, Loss: 0.000580955296754837\n",
      "Batch 4461/14851, Loss: 0.0019867157097905874\n",
      "Batch 4462/14851, Loss: 0.000722232973203063\n",
      "Batch 4463/14851, Loss: 5.37186861038208e-05\n",
      "Batch 4464/14851, Loss: 0.0014810363063588738\n",
      "Batch 4465/14851, Loss: 0.0014489939203485847\n",
      "Batch 4466/14851, Loss: 0.0025643110275268555\n",
      "Batch 4467/14851, Loss: 0.0017777321627363563\n",
      "Batch 4468/14851, Loss: 0.0003605621459428221\n",
      "Batch 4469/14851, Loss: 0.0043321936391294\n",
      "Batch 4470/14851, Loss: 0.004915581550449133\n",
      "Batch 4471/14851, Loss: 0.00020351384591776878\n",
      "Batch 4472/14851, Loss: 0.06602822244167328\n",
      "Batch 4473/14851, Loss: 0.006295609753578901\n",
      "Batch 4474/14851, Loss: 0.010926904156804085\n",
      "Batch 4475/14851, Loss: 0.0004550119338091463\n",
      "Batch 4476/14851, Loss: 0.0027609120588749647\n",
      "Batch 4477/14851, Loss: 0.054886654019355774\n",
      "Batch 4478/14851, Loss: 0.01775594986975193\n",
      "Batch 4479/14851, Loss: 0.00015941883611958474\n",
      "Batch 4480/14851, Loss: 0.042929984629154205\n",
      "Batch 4481/14851, Loss: 0.004281959962099791\n",
      "Batch 4482/14851, Loss: 0.034750595688819885\n",
      "Batch 4483/14851, Loss: 0.004086170345544815\n",
      "Batch 4484/14851, Loss: 0.0016564818797633052\n",
      "Batch 4485/14851, Loss: 0.002075914526358247\n",
      "Batch 4486/14851, Loss: 0.03149844706058502\n",
      "Batch 4487/14851, Loss: 0.0012387944152578712\n",
      "Batch 4488/14851, Loss: 0.0004968928988091648\n",
      "Batch 4489/14851, Loss: 0.006787382066249847\n",
      "Batch 4490/14851, Loss: 0.0014316762099042535\n",
      "Batch 4491/14851, Loss: 0.003494138829410076\n",
      "Batch 4492/14851, Loss: 0.04588993266224861\n",
      "Batch 4493/14851, Loss: 0.0013611553004011512\n",
      "Batch 4494/14851, Loss: 0.00028735274099744856\n",
      "Batch 4495/14851, Loss: 0.012248788960278034\n",
      "Batch 4496/14851, Loss: 0.009133382700383663\n",
      "Batch 4497/14851, Loss: 0.0014654974220320582\n",
      "Batch 4498/14851, Loss: 0.0011493390193209052\n",
      "Batch 4499/14851, Loss: 0.026967555284500122\n",
      "Batch 4500/14851, Loss: 0.004004395101219416\n",
      "Batch 4501/14851, Loss: 0.008682840503752232\n",
      "Batch 4502/14851, Loss: 0.0024912417866289616\n",
      "Batch 4503/14851, Loss: 0.0009518377482891083\n",
      "Batch 4504/14851, Loss: 0.002047876827418804\n",
      "Batch 4505/14851, Loss: 0.0008508836035616696\n",
      "Batch 4506/14851, Loss: 0.010462264530360699\n",
      "Batch 4507/14851, Loss: 0.006430801469832659\n",
      "Batch 4508/14851, Loss: 0.0007333606481552124\n",
      "Batch 4509/14851, Loss: 0.0033261477947235107\n",
      "Batch 4510/14851, Loss: 8.489439642289653e-05\n",
      "Batch 4511/14851, Loss: 0.0021698959171772003\n",
      "Batch 4512/14851, Loss: 0.005771928932517767\n",
      "Batch 4513/14851, Loss: 0.008851864375174046\n",
      "Batch 4514/14851, Loss: 0.0024256836622953415\n",
      "Batch 4515/14851, Loss: 0.04348403587937355\n",
      "Batch 4516/14851, Loss: 0.0009947664802893996\n",
      "Batch 4517/14851, Loss: 0.008461547084152699\n",
      "Batch 4518/14851, Loss: 0.004425981547683477\n",
      "Batch 4519/14851, Loss: 0.007989088073372841\n",
      "Batch 4520/14851, Loss: 0.0050652846693992615\n",
      "Batch 4521/14851, Loss: 0.07325427979230881\n",
      "Batch 4522/14851, Loss: 0.0016009584069252014\n",
      "Batch 4523/14851, Loss: 7.585684215882793e-05\n",
      "Batch 4524/14851, Loss: 0.06591234356164932\n",
      "Batch 4525/14851, Loss: 0.04189901426434517\n",
      "Batch 4526/14851, Loss: 0.007347612176090479\n",
      "Batch 4527/14851, Loss: 0.041370097547769547\n",
      "Batch 4528/14851, Loss: 0.0013885156949982047\n",
      "Batch 4529/14851, Loss: 0.0029330451507121325\n",
      "Batch 4530/14851, Loss: 0.01222359761595726\n",
      "Batch 4531/14851, Loss: 0.025537701323628426\n",
      "Batch 4532/14851, Loss: 0.03558051213622093\n",
      "Batch 4533/14851, Loss: 0.0010319029679521918\n",
      "Batch 4534/14851, Loss: 0.004255311097949743\n",
      "Batch 4535/14851, Loss: 0.0008508612518198788\n",
      "Batch 4536/14851, Loss: 0.011998183093965054\n",
      "Batch 4537/14851, Loss: 0.003020333359017968\n",
      "Batch 4538/14851, Loss: 0.0003461502492427826\n",
      "Batch 4539/14851, Loss: 0.04556289687752724\n",
      "Batch 4540/14851, Loss: 0.00015990101383067667\n",
      "Batch 4541/14851, Loss: 0.01154901459813118\n",
      "Batch 4542/14851, Loss: 0.0013480521738529205\n",
      "Batch 4543/14851, Loss: 0.0272666122764349\n",
      "Batch 4544/14851, Loss: 0.0015544494381174445\n",
      "Batch 4545/14851, Loss: 0.01628662645816803\n",
      "Batch 4546/14851, Loss: 0.012003120966255665\n",
      "Batch 4547/14851, Loss: 0.0008617937564849854\n",
      "Batch 4548/14851, Loss: 0.014983674511313438\n",
      "Batch 4549/14851, Loss: 0.0009824844310060143\n",
      "Batch 4550/14851, Loss: 0.002943196566775441\n",
      "Batch 4551/14851, Loss: 0.09829486161470413\n",
      "Batch 4552/14851, Loss: 0.00028018554439768195\n",
      "Batch 4553/14851, Loss: 0.02481194958090782\n",
      "Batch 4554/14851, Loss: 0.0017804246162995696\n",
      "Batch 4555/14851, Loss: 0.00020106282318010926\n",
      "Batch 4556/14851, Loss: 0.0005003073019906878\n",
      "Batch 4557/14851, Loss: 0.008157732896506786\n",
      "Batch 4558/14851, Loss: 0.015065901912748814\n",
      "Batch 4559/14851, Loss: 0.02320975624024868\n",
      "Batch 4560/14851, Loss: 0.0030905792955309153\n",
      "Batch 4561/14851, Loss: 0.001435382873751223\n",
      "Batch 4562/14851, Loss: 0.0023436248302459717\n",
      "Batch 4563/14851, Loss: 0.0007718739216215909\n",
      "Batch 4564/14851, Loss: 0.006994729861617088\n",
      "Batch 4565/14851, Loss: 0.0002646898210514337\n",
      "Batch 4566/14851, Loss: 0.001827527186833322\n",
      "Batch 4567/14851, Loss: 0.05242122337222099\n",
      "Batch 4568/14851, Loss: 0.0004642863932531327\n",
      "Batch 4569/14851, Loss: 0.021217523142695427\n",
      "Batch 4570/14851, Loss: 0.0020027372520416975\n",
      "Batch 4571/14851, Loss: 0.002720325021073222\n",
      "Batch 4572/14851, Loss: 0.0008810435538180172\n",
      "Batch 4573/14851, Loss: 0.011486995033919811\n",
      "Batch 4574/14851, Loss: 0.028713000938296318\n",
      "Batch 4575/14851, Loss: 0.0006117696757428348\n",
      "Batch 4576/14851, Loss: 0.03326849639415741\n",
      "Batch 4577/14851, Loss: 0.07743848860263824\n",
      "Batch 4578/14851, Loss: 0.015128769911825657\n",
      "Batch 4579/14851, Loss: 9.792670607566833e-05\n",
      "Batch 4580/14851, Loss: 0.010246563702821732\n",
      "Batch 4581/14851, Loss: 0.00029874363099224865\n",
      "Batch 4582/14851, Loss: 4.125634950469248e-05\n",
      "Batch 4583/14851, Loss: 6.359070539474487e-05\n",
      "Batch 4584/14851, Loss: 0.11106167733669281\n",
      "Batch 4585/14851, Loss: 0.01390062365680933\n",
      "Batch 4586/14851, Loss: 0.027860617265105247\n",
      "Batch 4587/14851, Loss: 0.0015518242726102471\n",
      "Batch 4588/14851, Loss: 0.03580598905682564\n",
      "Batch 4589/14851, Loss: 0.023600885644555092\n",
      "Batch 4590/14851, Loss: 0.045412689447402954\n",
      "Batch 4591/14851, Loss: 0.0006486750207841396\n",
      "Batch 4592/14851, Loss: 0.00899375881999731\n",
      "Batch 4593/14851, Loss: 0.012282225303351879\n",
      "Batch 4594/14851, Loss: 0.003256632946431637\n",
      "Batch 4595/14851, Loss: 0.0013723410665988922\n",
      "Batch 4596/14851, Loss: 0.0005758889019489288\n",
      "Batch 4597/14851, Loss: 0.025890681892633438\n",
      "Batch 4598/14851, Loss: 0.014824611134827137\n",
      "Batch 4599/14851, Loss: 0.02535763755440712\n",
      "Batch 4600/14851, Loss: 0.00834341999143362\n",
      "Batch 4601/14851, Loss: 0.021298127248883247\n",
      "Batch 4602/14851, Loss: 0.00012597937893588096\n",
      "Batch 4603/14851, Loss: 0.0028576648328453302\n",
      "Batch 4604/14851, Loss: 0.0064640166237950325\n",
      "Batch 4605/14851, Loss: 0.0016199499368667603\n",
      "Batch 4606/14851, Loss: 0.00016387924551963806\n",
      "Batch 4607/14851, Loss: 0.0002019580133492127\n",
      "Batch 4608/14851, Loss: 0.02225470542907715\n",
      "Batch 4609/14851, Loss: 0.0588979572057724\n",
      "Batch 4610/14851, Loss: 0.015212535858154297\n",
      "Batch 4611/14851, Loss: 0.011405297555029392\n",
      "Batch 4612/14851, Loss: 0.014773504808545113\n",
      "Batch 4613/14851, Loss: 0.00014174853276927024\n",
      "Batch 4614/14851, Loss: 0.002317279577255249\n",
      "Batch 4615/14851, Loss: 0.0007884154911153018\n",
      "Batch 4616/14851, Loss: 0.00020680204033851624\n",
      "Batch 4617/14851, Loss: 0.00010378410661360249\n",
      "Batch 4618/14851, Loss: 0.014007100835442543\n",
      "Batch 4619/14851, Loss: 0.0460662916302681\n",
      "Batch 4620/14851, Loss: 0.03060857206583023\n",
      "Batch 4621/14851, Loss: 0.0003463116881903261\n",
      "Batch 4622/14851, Loss: 0.002885631052777171\n",
      "Batch 4623/14851, Loss: 0.0005563435843214393\n",
      "Batch 4624/14851, Loss: 0.007902567274868488\n",
      "Batch 4625/14851, Loss: 0.008040742948651314\n",
      "Batch 4626/14851, Loss: 0.032210804522037506\n",
      "Batch 4627/14851, Loss: 0.00641570845618844\n",
      "Batch 4628/14851, Loss: 0.0013802669709548354\n",
      "Batch 4629/14851, Loss: 0.002338198246434331\n",
      "Batch 4630/14851, Loss: 0.010126783512532711\n",
      "Batch 4631/14851, Loss: 0.004348095040768385\n",
      "Batch 4632/14851, Loss: 0.012738863006234169\n",
      "Batch 4633/14851, Loss: 0.010891233570873737\n",
      "Batch 4634/14851, Loss: 0.007295618765056133\n",
      "Batch 4635/14851, Loss: 0.00866101123392582\n",
      "Batch 4636/14851, Loss: 0.005502719432115555\n",
      "Batch 4637/14851, Loss: 0.019570207223296165\n",
      "Batch 4638/14851, Loss: 0.05294807627797127\n",
      "Batch 4639/14851, Loss: 0.004965438041836023\n",
      "Batch 4640/14851, Loss: 0.006906227674335241\n",
      "Batch 4641/14851, Loss: 0.024488771334290504\n",
      "Batch 4642/14851, Loss: 0.04984450340270996\n",
      "Batch 4643/14851, Loss: 0.022245386615395546\n",
      "Batch 4644/14851, Loss: 0.0007292032241821289\n",
      "Batch 4645/14851, Loss: 0.008316975086927414\n",
      "Batch 4646/14851, Loss: 0.013358020223677158\n",
      "Batch 4647/14851, Loss: 0.0007468511466868222\n",
      "Batch 4648/14851, Loss: 0.001461294828914106\n",
      "Batch 4649/14851, Loss: 0.03209320455789566\n",
      "Batch 4650/14851, Loss: 0.0007648865575902164\n",
      "Batch 4651/14851, Loss: 0.015945663675665855\n",
      "Batch 4652/14851, Loss: 0.0003357554378453642\n",
      "Batch 4653/14851, Loss: 0.0004930943250656128\n",
      "Batch 4654/14851, Loss: 0.00016738598060328513\n",
      "Batch 4655/14851, Loss: 0.001003231736831367\n",
      "Batch 4656/14851, Loss: 0.0032375839073210955\n",
      "Batch 4657/14851, Loss: 0.00029780701152049005\n",
      "Batch 4658/14851, Loss: 0.007928244769573212\n",
      "Batch 4659/14851, Loss: 0.007726415526121855\n",
      "Batch 4660/14851, Loss: 0.0001641760318307206\n",
      "Batch 4661/14851, Loss: 0.014130151830613613\n",
      "Batch 4662/14851, Loss: 0.007322011981159449\n",
      "Batch 4663/14851, Loss: 0.0015905828913673759\n",
      "Batch 4664/14851, Loss: 0.0006471264059655368\n",
      "Batch 4665/14851, Loss: 0.00025586909032426775\n",
      "Batch 4666/14851, Loss: 0.000863038410898298\n",
      "Batch 4667/14851, Loss: 0.006153108086436987\n",
      "Batch 4668/14851, Loss: 0.007943169213831425\n",
      "Batch 4669/14851, Loss: 0.0008203734760172665\n",
      "Batch 4670/14851, Loss: 0.004357840865850449\n",
      "Batch 4671/14851, Loss: 0.034815460443496704\n",
      "Batch 4672/14851, Loss: 0.00023477400827687234\n",
      "Batch 4673/14851, Loss: 0.0006026588380336761\n",
      "Batch 4674/14851, Loss: 0.0043061524629592896\n",
      "Batch 4675/14851, Loss: 0.025984013453125954\n",
      "Batch 4676/14851, Loss: 0.0024635661393404007\n",
      "Batch 4677/14851, Loss: 0.079855777323246\n",
      "Batch 4678/14851, Loss: 0.006609709467738867\n",
      "Batch 4679/14851, Loss: 0.0005977340042591095\n",
      "Batch 4680/14851, Loss: 0.03827860578894615\n",
      "Batch 4681/14851, Loss: 0.00042778500937856734\n",
      "Batch 4682/14851, Loss: 0.00034954232978634536\n",
      "Batch 4683/14851, Loss: 7.051974534988403e-06\n",
      "Batch 4684/14851, Loss: 0.0311097614467144\n",
      "Batch 4685/14851, Loss: 0.006161586847156286\n",
      "Batch 4686/14851, Loss: 0.0005102517898194492\n",
      "Batch 4687/14851, Loss: 0.00012366243754513562\n",
      "Batch 4688/14851, Loss: 0.0014399836072698236\n",
      "Batch 4689/14851, Loss: 0.0013197710504755378\n",
      "Batch 4690/14851, Loss: 0.02064620703458786\n",
      "Batch 4691/14851, Loss: 0.031597018241882324\n",
      "Batch 4692/14851, Loss: 7.857257878640667e-05\n",
      "Batch 4693/14851, Loss: 0.0008303510840050876\n",
      "Batch 4694/14851, Loss: 0.030736267566680908\n",
      "Batch 4695/14851, Loss: 0.00035547465085983276\n",
      "Batch 4696/14851, Loss: 0.00013820578169543296\n",
      "Batch 4697/14851, Loss: 0.00018677239131648093\n",
      "Batch 4698/14851, Loss: 0.003783339401707053\n",
      "Batch 4699/14851, Loss: 6.115436553955078e-05\n",
      "Batch 4700/14851, Loss: 0.005548493470996618\n",
      "Batch 4701/14851, Loss: 0.003740024520084262\n",
      "Batch 4702/14851, Loss: 0.006128282286226749\n",
      "Batch 4703/14851, Loss: 0.00035976991057395935\n",
      "Batch 4704/14851, Loss: 0.022897478193044662\n",
      "Batch 4705/14851, Loss: 0.006463428493589163\n",
      "Batch 4706/14851, Loss: 0.0007748181815259159\n",
      "Batch 4707/14851, Loss: 0.012145580723881721\n",
      "Batch 4708/14851, Loss: 0.0055299862287938595\n",
      "Batch 4709/14851, Loss: 0.0002809899451676756\n",
      "Batch 4710/14851, Loss: 0.02432060055434704\n",
      "Batch 4711/14851, Loss: 0.0004196156223770231\n",
      "Batch 4712/14851, Loss: 0.0006614404846914113\n",
      "Batch 4713/14851, Loss: 0.015959927812218666\n",
      "Batch 4714/14851, Loss: 0.00023220633738674223\n",
      "Batch 4715/14851, Loss: 0.005661166738718748\n",
      "Batch 4716/14851, Loss: 0.0004596585931722075\n",
      "Batch 4717/14851, Loss: 0.0009633389418013394\n",
      "Batch 4718/14851, Loss: 0.012350361794233322\n",
      "Batch 4719/14851, Loss: 0.000937604927457869\n",
      "Batch 4720/14851, Loss: 0.008768660016357899\n",
      "Batch 4721/14851, Loss: 0.007749674841761589\n",
      "Batch 4722/14851, Loss: 0.0011224535992369056\n",
      "Batch 4723/14851, Loss: 0.06465049088001251\n",
      "Batch 4724/14851, Loss: 0.00015171740960795432\n",
      "Batch 4725/14851, Loss: 0.0011913776397705078\n",
      "Batch 4726/14851, Loss: 0.0011117715621367097\n",
      "Batch 4727/14851, Loss: 0.07141659408807755\n",
      "Batch 4728/14851, Loss: 0.04184945300221443\n",
      "Batch 4729/14851, Loss: 0.0006697289645671844\n",
      "Batch 4730/14851, Loss: 0.004609557334333658\n",
      "Batch 4731/14851, Loss: 0.0014633387327194214\n",
      "Batch 4732/14851, Loss: 0.001668716431595385\n",
      "Batch 4733/14851, Loss: 0.010869268327951431\n",
      "Batch 4734/14851, Loss: 0.008781771175563335\n",
      "Batch 4735/14851, Loss: 0.008944762870669365\n",
      "Batch 4736/14851, Loss: 0.0021966160275042057\n",
      "Batch 4737/14851, Loss: 0.003077494679018855\n",
      "Batch 4738/14851, Loss: 0.010839375667273998\n",
      "Batch 4739/14851, Loss: 0.0011415047338232398\n",
      "Batch 4740/14851, Loss: 0.006358645856380463\n",
      "Batch 4741/14851, Loss: 0.006741418037563562\n",
      "Batch 4742/14851, Loss: 0.0038957910146564245\n",
      "Batch 4743/14851, Loss: 0.04860306903719902\n",
      "Batch 4744/14851, Loss: 0.0006834911182522774\n",
      "Batch 4745/14851, Loss: 0.0007607403094880283\n",
      "Batch 4746/14851, Loss: 0.0003148565592709929\n",
      "Batch 4747/14851, Loss: 0.00028929818654432893\n",
      "Batch 4748/14851, Loss: 0.0016508715925738215\n",
      "Batch 4749/14851, Loss: 0.0037425633054226637\n",
      "Batch 4750/14851, Loss: 0.004239699337631464\n",
      "Batch 4751/14851, Loss: 8.474290370941162e-05\n",
      "Batch 4752/14851, Loss: 0.01856144703924656\n",
      "Batch 4753/14851, Loss: 0.007948053069412708\n",
      "Batch 4754/14851, Loss: 0.0065871900878846645\n",
      "Batch 4755/14851, Loss: 0.0006157122552394867\n",
      "Batch 4756/14851, Loss: 0.0007580220699310303\n",
      "Batch 4757/14851, Loss: 0.0014964139554649591\n",
      "Batch 4758/14851, Loss: 0.00046803904115222394\n",
      "Batch 4759/14851, Loss: 0.009798559360206127\n",
      "Batch 4760/14851, Loss: 0.005393204744905233\n",
      "Batch 4761/14851, Loss: 0.0006771743064746261\n",
      "Batch 4762/14851, Loss: 0.0005846204585395753\n",
      "Batch 4763/14851, Loss: 0.00394820561632514\n",
      "Batch 4764/14851, Loss: 0.011715195141732693\n",
      "Batch 4765/14851, Loss: 0.007633953355252743\n",
      "Batch 4766/14851, Loss: 0.02513285167515278\n",
      "Batch 4767/14851, Loss: 0.0011282885679975152\n",
      "Batch 4768/14851, Loss: 0.006681802216917276\n",
      "Batch 4769/14851, Loss: 0.0026370601262897253\n",
      "Batch 4770/14851, Loss: 0.002464285818859935\n",
      "Batch 4771/14851, Loss: 0.007234914693981409\n",
      "Batch 4772/14851, Loss: 0.050177812576293945\n",
      "Batch 4773/14851, Loss: 0.0006237785564735532\n",
      "Batch 4774/14851, Loss: 0.00030139958835206926\n",
      "Batch 4775/14851, Loss: 3.432730954955332e-05\n",
      "Batch 4776/14851, Loss: 0.04263875260949135\n",
      "Batch 4777/14851, Loss: 0.008620132692158222\n",
      "Batch 4778/14851, Loss: 0.0018998763989657164\n",
      "Batch 4779/14851, Loss: 0.0018563581397756934\n",
      "Batch 4780/14851, Loss: 0.0006541490438394248\n",
      "Batch 4781/14851, Loss: 0.03517673537135124\n",
      "Batch 4782/14851, Loss: 0.05672115832567215\n",
      "Batch 4783/14851, Loss: 0.00015484665345866233\n",
      "Batch 4784/14851, Loss: 0.00690453639253974\n",
      "Batch 4785/14851, Loss: 0.0064676739275455475\n",
      "Batch 4786/14851, Loss: 0.004378297366201878\n",
      "Batch 4787/14851, Loss: 0.011609652079641819\n",
      "Batch 4788/14851, Loss: 0.00023410965513903648\n",
      "Batch 4789/14851, Loss: 0.01696818880736828\n",
      "Batch 4790/14851, Loss: 0.00011250749230384827\n",
      "Batch 4791/14851, Loss: 0.0011933445930480957\n",
      "Batch 4792/14851, Loss: 0.0004782850155606866\n",
      "Batch 4793/14851, Loss: 0.002047299174591899\n",
      "Batch 4794/14851, Loss: 0.01603112928569317\n",
      "Batch 4795/14851, Loss: 0.0036340628284960985\n",
      "Batch 4796/14851, Loss: 0.0002505853772163391\n",
      "Batch 4797/14851, Loss: 0.026102332398295403\n",
      "Batch 4798/14851, Loss: 0.001194314449094236\n",
      "Batch 4799/14851, Loss: 0.01912439614534378\n",
      "Batch 4800/14851, Loss: 0.0006521842442452908\n",
      "Batch 4801/14851, Loss: 0.04200908541679382\n",
      "Batch 4802/14851, Loss: 0.0005062471027486026\n",
      "Batch 4803/14851, Loss: 0.015982316806912422\n",
      "Batch 4804/14851, Loss: 0.00022425998758990318\n",
      "Batch 4805/14851, Loss: 0.00036766999983228743\n",
      "Batch 4806/14851, Loss: 0.0008910720935091376\n",
      "Batch 4807/14851, Loss: 0.006193262990564108\n",
      "Batch 4808/14851, Loss: 0.0004708133637905121\n",
      "Batch 4809/14851, Loss: 0.001402276218868792\n",
      "Batch 4810/14851, Loss: 7.379800081253052e-05\n",
      "Batch 4811/14851, Loss: 0.013390309177339077\n",
      "Batch 4812/14851, Loss: 0.0011681210016831756\n",
      "Batch 4813/14851, Loss: 0.0009826062014326453\n",
      "Batch 4814/14851, Loss: 0.038988884538412094\n",
      "Batch 4815/14851, Loss: 0.010788635350763798\n",
      "Batch 4816/14851, Loss: 0.005755053833127022\n",
      "Batch 4817/14851, Loss: 0.0006638455088250339\n",
      "Batch 4818/14851, Loss: 0.0019778378773480654\n",
      "Batch 4819/14851, Loss: 0.00010999912774423137\n",
      "Batch 4820/14851, Loss: 0.000827345997095108\n",
      "Batch 4821/14851, Loss: 0.00014528632164001465\n",
      "Batch 4822/14851, Loss: 0.009279049932956696\n",
      "Batch 4823/14851, Loss: 0.0018221345962956548\n",
      "Batch 4824/14851, Loss: 0.006420414429157972\n",
      "Batch 4825/14851, Loss: 0.0008710268884897232\n",
      "Batch 4826/14851, Loss: 0.002003028988838196\n",
      "Batch 4827/14851, Loss: 0.0003811696369666606\n",
      "Batch 4828/14851, Loss: 0.00288230087608099\n",
      "Batch 4829/14851, Loss: 0.019667716696858406\n",
      "Batch 4830/14851, Loss: 0.04642459377646446\n",
      "Batch 4831/14851, Loss: 0.000635395641438663\n",
      "Batch 4832/14851, Loss: 0.0015199979534372687\n",
      "Batch 4833/14851, Loss: 0.0016557443886995316\n",
      "Batch 4834/14851, Loss: 0.03346307948231697\n",
      "Batch 4835/14851, Loss: 0.0020648392383009195\n",
      "Batch 4836/14851, Loss: 0.0019372085807844996\n",
      "Batch 4837/14851, Loss: 0.0024234354496002197\n",
      "Batch 4838/14851, Loss: 0.0036726484540849924\n",
      "Batch 4839/14851, Loss: 0.0013318314449861646\n",
      "Batch 4840/14851, Loss: 0.0009715072810649872\n",
      "Batch 4841/14851, Loss: 0.002561751287430525\n",
      "Batch 4842/14851, Loss: 0.0037967395037412643\n",
      "Batch 4843/14851, Loss: 0.0026067993603646755\n",
      "Batch 4844/14851, Loss: 0.005849847104400396\n",
      "Batch 4845/14851, Loss: 0.03383396565914154\n",
      "Batch 4846/14851, Loss: 0.000726227939594537\n",
      "Batch 4847/14851, Loss: 0.0005800475482828915\n",
      "Batch 4848/14851, Loss: 0.007811289746314287\n",
      "Batch 4849/14851, Loss: 0.0005677628214471042\n",
      "Batch 4850/14851, Loss: 0.02386610396206379\n",
      "Batch 4851/14851, Loss: 0.0023297129664570093\n",
      "Batch 4852/14851, Loss: 0.004743820521980524\n",
      "Batch 4853/14851, Loss: 0.08037737011909485\n",
      "Batch 4854/14851, Loss: 0.030053889378905296\n",
      "Batch 4855/14851, Loss: 0.0005012775654904544\n",
      "Batch 4856/14851, Loss: 0.00672386446967721\n",
      "Batch 4857/14851, Loss: 0.001763053354807198\n",
      "Batch 4858/14851, Loss: 0.005451232194900513\n",
      "Batch 4859/14851, Loss: 0.006788309197872877\n",
      "Batch 4860/14851, Loss: 0.004372524097561836\n",
      "Batch 4861/14851, Loss: 0.10288035869598389\n",
      "Batch 4862/14851, Loss: 0.004555704537779093\n",
      "Batch 4863/14851, Loss: 0.03405691310763359\n",
      "Batch 4864/14851, Loss: 0.00041990354657173157\n",
      "Batch 4865/14851, Loss: 8.917153172660619e-05\n",
      "Batch 4866/14851, Loss: 0.00018936746346298605\n",
      "Batch 4867/14851, Loss: 0.0015273148892447352\n",
      "Batch 4868/14851, Loss: 0.0008651800453662872\n",
      "Batch 4869/14851, Loss: 0.009424463845789433\n",
      "Batch 4870/14851, Loss: 0.010760198347270489\n",
      "Batch 4871/14851, Loss: 0.011013568378984928\n",
      "Batch 4872/14851, Loss: 0.0012090603122487664\n",
      "Batch 4873/14851, Loss: 0.0003238556382711977\n",
      "Batch 4874/14851, Loss: 0.00012792646884918213\n",
      "Batch 4875/14851, Loss: 1.9905613953596912e-05\n",
      "Batch 4876/14851, Loss: 0.0001275402755709365\n",
      "Batch 4877/14851, Loss: 0.0011888941517099738\n",
      "Batch 4878/14851, Loss: 0.018219277262687683\n",
      "Batch 4879/14851, Loss: 0.004043418448418379\n",
      "Batch 4880/14851, Loss: 0.00509806489571929\n",
      "Batch 4881/14851, Loss: 0.0003498272562865168\n",
      "Batch 4882/14851, Loss: 0.002566533861681819\n",
      "Batch 4883/14851, Loss: 0.0120936194434762\n",
      "Batch 4884/14851, Loss: 0.0006909693474881351\n",
      "Batch 4885/14851, Loss: 0.017080960795283318\n",
      "Batch 4886/14851, Loss: 0.0023207818157970905\n",
      "Batch 4887/14851, Loss: 0.0003537336888257414\n",
      "Batch 4888/14851, Loss: 0.027255622670054436\n",
      "Batch 4889/14851, Loss: 0.001588421524502337\n",
      "Batch 4890/14851, Loss: 0.003842649282887578\n",
      "Batch 4891/14851, Loss: 0.0022084873635321856\n",
      "Batch 4892/14851, Loss: 3.233303505112417e-05\n",
      "Batch 4893/14851, Loss: 0.0006030375952832401\n",
      "Batch 4894/14851, Loss: 0.0032259810250252485\n",
      "Batch 4895/14851, Loss: 3.3627537050051615e-05\n",
      "Batch 4896/14851, Loss: 0.003939257003366947\n",
      "Batch 4897/14851, Loss: 0.0036407436709851027\n",
      "Batch 4898/14851, Loss: 0.000563816400244832\n",
      "Batch 4899/14851, Loss: 0.001368896453641355\n",
      "Batch 4900/14851, Loss: 0.011795817874372005\n",
      "Batch 4901/14851, Loss: 9.685630357125774e-05\n",
      "Batch 4902/14851, Loss: 0.012562940828502178\n",
      "Batch 4903/14851, Loss: 0.0005752381985075772\n",
      "Batch 4904/14851, Loss: 0.00030646423692815006\n",
      "Batch 4905/14851, Loss: 0.033037882298231125\n",
      "Batch 4906/14851, Loss: 0.0037171635776758194\n",
      "Batch 4907/14851, Loss: 0.004536056891083717\n",
      "Batch 4908/14851, Loss: 0.0010335022816434503\n",
      "Batch 4909/14851, Loss: 0.00024092446255963296\n",
      "Batch 4910/14851, Loss: 0.005147711839526892\n",
      "Batch 4911/14851, Loss: 0.00157443608622998\n",
      "Batch 4912/14851, Loss: 0.005698988679796457\n",
      "Batch 4913/14851, Loss: 0.01051739975810051\n",
      "Batch 4914/14851, Loss: 0.03182791545987129\n",
      "Batch 4915/14851, Loss: 0.0004550280573312193\n",
      "Batch 4916/14851, Loss: 0.003672264516353607\n",
      "Batch 4917/14851, Loss: 0.017486749216914177\n",
      "Batch 4918/14851, Loss: 0.00013695284724235535\n",
      "Batch 4919/14851, Loss: 0.0012076132697984576\n",
      "Batch 4920/14851, Loss: 0.0006057829596102238\n",
      "Batch 4921/14851, Loss: 0.05082698538899422\n",
      "Batch 4922/14851, Loss: 0.0002046152949333191\n",
      "Batch 4923/14851, Loss: 0.0007674800581298769\n",
      "Batch 4924/14851, Loss: 3.0374774723895825e-05\n",
      "Batch 4925/14851, Loss: 0.006438174284994602\n",
      "Batch 4926/14851, Loss: 0.001109803793951869\n",
      "Batch 4927/14851, Loss: 7.599095260957256e-05\n",
      "Batch 4928/14851, Loss: 0.0005059230024926364\n",
      "Batch 4929/14851, Loss: 0.02832595817744732\n",
      "Batch 4930/14851, Loss: 0.013266940601170063\n",
      "Batch 4931/14851, Loss: 0.00271351239643991\n",
      "Batch 4932/14851, Loss: 0.00230519101023674\n",
      "Batch 4933/14851, Loss: 0.06866694986820221\n",
      "Batch 4934/14851, Loss: 3.9609771192772314e-05\n",
      "Batch 4935/14851, Loss: 0.021054552868008614\n",
      "Batch 4936/14851, Loss: 0.002293578116223216\n",
      "Batch 4937/14851, Loss: 0.00020887330174446106\n",
      "Batch 4938/14851, Loss: 0.0009214613237418234\n",
      "Batch 4939/14851, Loss: 0.047929808497428894\n",
      "Batch 4940/14851, Loss: 0.06233412027359009\n",
      "Batch 4941/14851, Loss: 0.0015444548334926367\n",
      "Batch 4942/14851, Loss: 0.04984201118350029\n",
      "Batch 4943/14851, Loss: 0.002123093931004405\n",
      "Batch 4944/14851, Loss: 0.03180059418082237\n",
      "Batch 4945/14851, Loss: 0.028325581923127174\n",
      "Batch 4946/14851, Loss: 0.0011164930183440447\n",
      "Batch 4947/14851, Loss: 0.03565060719847679\n",
      "Batch 4948/14851, Loss: 0.030577830970287323\n",
      "Batch 4949/14851, Loss: 0.006621004547923803\n",
      "Batch 4950/14851, Loss: 3.94284725189209e-05\n",
      "Batch 4951/14851, Loss: 0.00021518590801861137\n",
      "Batch 4952/14851, Loss: 0.04168020188808441\n",
      "Batch 4953/14851, Loss: 0.04387441277503967\n",
      "Batch 4954/14851, Loss: 0.0008359173662029207\n",
      "Batch 4955/14851, Loss: 0.0005955149536021054\n",
      "Batch 4956/14851, Loss: 0.043549418449401855\n",
      "Batch 4957/14851, Loss: 0.03947558254003525\n",
      "Batch 4958/14851, Loss: 0.010171898640692234\n",
      "Batch 4959/14851, Loss: 0.005034968256950378\n",
      "Batch 4960/14851, Loss: 0.007313283625990152\n",
      "Batch 4961/14851, Loss: 0.00522038945928216\n",
      "Batch 4962/14851, Loss: 0.014732622541487217\n",
      "Batch 4963/14851, Loss: 0.0005194942350499332\n",
      "Batch 4964/14851, Loss: 0.00659386208280921\n",
      "Batch 4965/14851, Loss: 0.0179827231913805\n",
      "Batch 4966/14851, Loss: 0.0009988596430048347\n",
      "Batch 4967/14851, Loss: 0.005679814610630274\n",
      "Batch 4968/14851, Loss: 0.001526022795587778\n",
      "Batch 4969/14851, Loss: 0.012790017761290073\n",
      "Batch 4970/14851, Loss: 0.010838917456567287\n",
      "Batch 4971/14851, Loss: 0.0013314953539520502\n",
      "Batch 4972/14851, Loss: 0.005587986204773188\n",
      "Batch 4973/14851, Loss: 0.0002327824040548876\n",
      "Batch 4974/14851, Loss: 0.008130270056426525\n",
      "Batch 4975/14851, Loss: 0.002231000689789653\n",
      "Batch 4976/14851, Loss: 0.008935146033763885\n",
      "Batch 4977/14851, Loss: 0.0059436471201479435\n",
      "Batch 4978/14851, Loss: 0.0015227434923872352\n",
      "Batch 4979/14851, Loss: 0.00021309901785571128\n",
      "Batch 4980/14851, Loss: 0.0008254784042946994\n",
      "Batch 4981/14851, Loss: 0.017185134813189507\n",
      "Batch 4982/14851, Loss: 0.0001476655452279374\n",
      "Batch 4983/14851, Loss: 0.011851799674332142\n",
      "Batch 4984/14851, Loss: 0.02762572653591633\n",
      "Batch 4985/14851, Loss: 0.03147103637456894\n",
      "Batch 4986/14851, Loss: 0.025496454909443855\n",
      "Batch 4987/14851, Loss: 0.021437600255012512\n",
      "Batch 4988/14851, Loss: 0.0013089841231703758\n",
      "Batch 4989/14851, Loss: 0.024701843038201332\n",
      "Batch 4990/14851, Loss: 0.0005457141087390482\n",
      "Batch 4991/14851, Loss: 0.00034114893060177565\n",
      "Batch 4992/14851, Loss: 0.0006518466980196536\n",
      "Batch 4993/14851, Loss: 0.000989602878689766\n",
      "Batch 4994/14851, Loss: 0.024742454290390015\n",
      "Batch 4995/14851, Loss: 0.002445874037221074\n",
      "Batch 4996/14851, Loss: 0.02452654391527176\n",
      "Batch 4997/14851, Loss: 0.00032819807529449463\n",
      "Batch 4998/14851, Loss: 9.361597039969638e-05\n",
      "Batch 4999/14851, Loss: 0.0250087957829237\n",
      "Batch 5000/14851, Loss: 0.0005449652671813965\n",
      "Batch 5001/14851, Loss: 0.0013481354108080268\n",
      "Batch 5002/14851, Loss: 0.0005237360601313412\n",
      "Batch 5003/14851, Loss: 6.7812702582159545e-06\n",
      "Batch 5004/14851, Loss: 0.00022979702043812722\n",
      "Batch 5005/14851, Loss: 0.001419704407453537\n",
      "Batch 5006/14851, Loss: 0.059897180646657944\n",
      "Batch 5007/14851, Loss: 0.0010235407389700413\n",
      "Batch 5008/14851, Loss: 0.0005739058251492679\n",
      "Batch 5009/14851, Loss: 0.04088857024908066\n",
      "Batch 5010/14851, Loss: 0.02486414834856987\n",
      "Batch 5011/14851, Loss: 0.00014096002269070596\n",
      "Batch 5012/14851, Loss: 0.003323011565953493\n",
      "Batch 5013/14851, Loss: 0.002974649192765355\n",
      "Batch 5014/14851, Loss: 0.0003817690012510866\n",
      "Batch 5015/14851, Loss: 8.21426510810852e-05\n",
      "Batch 5016/14851, Loss: 0.0009159554028883576\n",
      "Batch 5017/14851, Loss: 0.03776707127690315\n",
      "Batch 5018/14851, Loss: 0.0077263228595256805\n",
      "Batch 5019/14851, Loss: 0.0017687733052298427\n",
      "Batch 5020/14851, Loss: 2.3430833607562818e-05\n",
      "Batch 5021/14851, Loss: 0.00015643115330021828\n",
      "Batch 5022/14851, Loss: 0.0016631731996312737\n",
      "Batch 5023/14851, Loss: 0.00014109828043729067\n",
      "Batch 5024/14851, Loss: 0.006106857676059008\n",
      "Batch 5025/14851, Loss: 0.0001245265157194808\n",
      "Batch 5026/14851, Loss: 2.1478781491168775e-05\n",
      "Batch 5027/14851, Loss: 0.0045676338486373425\n",
      "Batch 5028/14851, Loss: 0.0012734048068523407\n",
      "Batch 5029/14851, Loss: 0.00027117007994093\n",
      "Batch 5030/14851, Loss: 0.00011111175263067707\n",
      "Batch 5031/14851, Loss: 0.0004756400012411177\n",
      "Batch 5032/14851, Loss: 0.005072557833045721\n",
      "Batch 5033/14851, Loss: 7.407491648336872e-05\n",
      "Batch 5034/14851, Loss: 0.035823166370391846\n",
      "Batch 5035/14851, Loss: 2.738088369369507e-05\n",
      "Batch 5036/14851, Loss: 2.370774745941162e-05\n",
      "Batch 5037/14851, Loss: 4.263470691512339e-05\n",
      "Batch 5038/14851, Loss: 0.0036962206941097975\n",
      "Batch 5039/14851, Loss: 0.00031272569322027266\n",
      "Batch 5040/14851, Loss: 0.00037464729393832386\n",
      "Batch 5041/14851, Loss: 0.00044712796807289124\n",
      "Batch 5042/14851, Loss: 0.0005328332190401852\n",
      "Batch 5043/14851, Loss: 5.184669134905562e-05\n",
      "Batch 5044/14851, Loss: 0.00010251129424432293\n",
      "Batch 5045/14851, Loss: 0.0003734553756657988\n",
      "Batch 5046/14851, Loss: 0.0022306968457996845\n",
      "Batch 5047/14851, Loss: 0.007804492022842169\n",
      "Batch 5048/14851, Loss: 6.428609140129993e-06\n",
      "Batch 5049/14851, Loss: 0.0004934221506118774\n",
      "Batch 5050/14851, Loss: 0.008833181113004684\n",
      "Batch 5051/14851, Loss: 0.023070627823472023\n",
      "Batch 5052/14851, Loss: 0.0005364790558815002\n",
      "Batch 5053/14851, Loss: 0.001385749434120953\n",
      "Batch 5054/14851, Loss: 0.04550928249955177\n",
      "Batch 5055/14851, Loss: 1.89170241355896e-05\n",
      "Batch 5056/14851, Loss: 0.0003668777644634247\n",
      "Batch 5057/14851, Loss: 0.00013241420674603432\n",
      "Batch 5058/14851, Loss: 0.004936274141073227\n",
      "Batch 5059/14851, Loss: 0.0010772967943921685\n",
      "Batch 5060/14851, Loss: 0.0399261899292469\n",
      "Batch 5061/14851, Loss: 0.03134942054748535\n",
      "Batch 5062/14851, Loss: 0.00016427270020358264\n",
      "Batch 5063/14851, Loss: 0.0006857474218122661\n",
      "Batch 5064/14851, Loss: 0.0019831182435154915\n",
      "Batch 5065/14851, Loss: 0.0006990444962866604\n",
      "Batch 5066/14851, Loss: 8.492668712278828e-05\n",
      "Batch 5067/14851, Loss: 0.04991361126303673\n",
      "Batch 5068/14851, Loss: 2.1064033717266284e-05\n",
      "Batch 5069/14851, Loss: 0.002199227223172784\n",
      "Batch 5070/14851, Loss: 0.0029204809106886387\n",
      "Batch 5071/14851, Loss: 0.061077166348695755\n",
      "Batch 5072/14851, Loss: 0.10423281043767929\n",
      "Batch 5073/14851, Loss: 0.00014085322618484497\n",
      "Batch 5074/14851, Loss: 0.008894368074834347\n",
      "Batch 5075/14851, Loss: 0.029698960483074188\n",
      "Batch 5076/14851, Loss: 0.0016118591884151101\n",
      "Batch 5077/14851, Loss: 0.06429369002580643\n",
      "Batch 5078/14851, Loss: 0.03821419179439545\n",
      "Batch 5079/14851, Loss: 0.0020969051402062178\n",
      "Batch 5080/14851, Loss: 0.0295108575373888\n",
      "Batch 5081/14851, Loss: 0.015237964689731598\n",
      "Batch 5082/14851, Loss: 0.00024040423159021884\n",
      "Batch 5083/14851, Loss: 0.0009868056513369083\n",
      "Batch 5084/14851, Loss: 0.03079701028764248\n",
      "Batch 5085/14851, Loss: 0.015995843335986137\n",
      "Batch 5086/14851, Loss: 0.036231908947229385\n",
      "Batch 5087/14851, Loss: 0.01895478367805481\n",
      "Batch 5088/14851, Loss: 0.036542344838380814\n",
      "Batch 5089/14851, Loss: 0.05318985879421234\n",
      "Batch 5090/14851, Loss: 0.00019924838852602988\n",
      "Batch 5091/14851, Loss: 0.00017289072275161743\n",
      "Batch 5092/14851, Loss: 0.0004951010341756046\n",
      "Batch 5093/14851, Loss: 0.005209630820900202\n",
      "Batch 5094/14851, Loss: 0.002105515915900469\n",
      "Batch 5095/14851, Loss: 0.061866581439971924\n",
      "Batch 5096/14851, Loss: 0.002004612237215042\n",
      "Batch 5097/14851, Loss: 0.0033452275674790144\n",
      "Batch 5098/14851, Loss: 0.0005301671917550266\n",
      "Batch 5099/14851, Loss: 0.022479282692074776\n",
      "Batch 5100/14851, Loss: 0.010958441533148289\n",
      "Batch 5101/14851, Loss: 0.023892318829894066\n",
      "Batch 5102/14851, Loss: 0.05541904643177986\n",
      "Batch 5103/14851, Loss: 0.011764771305024624\n",
      "Batch 5104/14851, Loss: 0.01010962761938572\n",
      "Batch 5105/14851, Loss: 0.006244639866054058\n",
      "Batch 5106/14851, Loss: 0.005658172536641359\n",
      "Batch 5107/14851, Loss: 0.0017207568744197488\n",
      "Batch 5108/14851, Loss: 0.005383988376706839\n",
      "Batch 5109/14851, Loss: 0.03971051797270775\n",
      "Batch 5110/14851, Loss: 0.004457424394786358\n",
      "Batch 5111/14851, Loss: 0.0012283071409910917\n",
      "Batch 5112/14851, Loss: 0.00768033042550087\n",
      "Batch 5113/14851, Loss: 0.011586833745241165\n",
      "Batch 5114/14851, Loss: 0.02928224764764309\n",
      "Batch 5115/14851, Loss: 0.01174534484744072\n",
      "Batch 5116/14851, Loss: 0.013460285030305386\n",
      "Batch 5117/14851, Loss: 0.0018717311322689056\n",
      "Batch 5118/14851, Loss: 0.0052302246913313866\n",
      "Batch 5119/14851, Loss: 0.003288939595222473\n",
      "Batch 5120/14851, Loss: 0.007738940883427858\n",
      "Batch 5121/14851, Loss: 0.0018975710263475776\n",
      "Batch 5122/14851, Loss: 0.003294561756774783\n",
      "Batch 5123/14851, Loss: 0.016318878158926964\n",
      "Batch 5124/14851, Loss: 0.0009145053918473423\n",
      "Batch 5125/14851, Loss: 0.0174766443669796\n",
      "Batch 5126/14851, Loss: 0.0021034677047282457\n",
      "Batch 5127/14851, Loss: 0.024697815999388695\n",
      "Batch 5128/14851, Loss: 0.06841050833463669\n",
      "Batch 5129/14851, Loss: 0.0014382625231519341\n",
      "Batch 5130/14851, Loss: 0.0030338186770677567\n",
      "Batch 5131/14851, Loss: 0.004746889229863882\n",
      "Batch 5132/14851, Loss: 0.00015288715076167136\n",
      "Batch 5133/14851, Loss: 0.0015386877348646522\n",
      "Batch 5134/14851, Loss: 0.0004957119817845523\n",
      "Batch 5135/14851, Loss: 0.0011946732411161065\n",
      "Batch 5136/14851, Loss: 0.00015326761058531702\n",
      "Batch 5137/14851, Loss: 0.004566576797515154\n",
      "Batch 5138/14851, Loss: 0.005261843558400869\n",
      "Batch 5139/14851, Loss: 0.004077850840985775\n",
      "Batch 5140/14851, Loss: 0.00024147250223904848\n",
      "Batch 5141/14851, Loss: 0.009324423968791962\n",
      "Batch 5142/14851, Loss: 0.024869779124855995\n",
      "Batch 5143/14851, Loss: 0.0016180947422981262\n",
      "Batch 5144/14851, Loss: 0.0022968773264437914\n",
      "Batch 5145/14851, Loss: 0.005139094777405262\n",
      "Batch 5146/14851, Loss: 0.018264375627040863\n",
      "Batch 5147/14851, Loss: 0.025034422054886818\n",
      "Batch 5148/14851, Loss: 0.0021911326330155134\n",
      "Batch 5149/14851, Loss: 0.004877981264144182\n",
      "Batch 5150/14851, Loss: 0.001630681217648089\n",
      "Batch 5151/14851, Loss: 0.0012149872491136193\n",
      "Batch 5152/14851, Loss: 0.03401453420519829\n",
      "Batch 5153/14851, Loss: 0.005306403152644634\n",
      "Batch 5154/14851, Loss: 0.0038088734727352858\n",
      "Batch 5155/14851, Loss: 0.032650385051965714\n",
      "Batch 5156/14851, Loss: 0.0011007040739059448\n",
      "Batch 5157/14851, Loss: 0.03265182673931122\n",
      "Batch 5158/14851, Loss: 0.0058996728621423244\n",
      "Batch 5159/14851, Loss: 0.004809818230569363\n",
      "Batch 5160/14851, Loss: 0.000198015317437239\n",
      "Batch 5161/14851, Loss: 0.0011477365624159575\n",
      "Batch 5162/14851, Loss: 0.00040617131162434816\n",
      "Batch 5163/14851, Loss: 0.0002920401457231492\n",
      "Batch 5164/14851, Loss: 0.02281869389116764\n",
      "Batch 5165/14851, Loss: 0.01184058003127575\n",
      "Batch 5166/14851, Loss: 8.502602577209473e-05\n",
      "Batch 5167/14851, Loss: 0.009094839915633202\n",
      "Batch 5168/14851, Loss: 0.0444323793053627\n",
      "Batch 5169/14851, Loss: 0.003182660788297653\n",
      "Batch 5170/14851, Loss: 0.00022226572036743164\n",
      "Batch 5171/14851, Loss: 0.009993789717555046\n",
      "Batch 5172/14851, Loss: 0.0033909890335053205\n",
      "Batch 5173/14851, Loss: 0.002486365381628275\n",
      "Batch 5174/14851, Loss: 0.04876895248889923\n",
      "Batch 5175/14851, Loss: 0.020267609506845474\n",
      "Batch 5176/14851, Loss: 0.02451215870678425\n",
      "Batch 5177/14851, Loss: 0.0011071128537878394\n",
      "Batch 5178/14851, Loss: 0.00501615833491087\n",
      "Batch 5179/14851, Loss: 0.016533508896827698\n",
      "Batch 5180/14851, Loss: 0.0063516972586512566\n",
      "Batch 5181/14851, Loss: 0.028568876907229424\n",
      "Batch 5182/14851, Loss: 0.020744793117046356\n",
      "Batch 5183/14851, Loss: 0.0017332261195406318\n",
      "Batch 5184/14851, Loss: 0.0029052358586341143\n",
      "Batch 5185/14851, Loss: 0.043399445712566376\n",
      "Batch 5186/14851, Loss: 0.0012988517992198467\n",
      "Batch 5187/14851, Loss: 0.0008799110655672848\n",
      "Batch 5188/14851, Loss: 0.00019908323884010315\n",
      "Batch 5189/14851, Loss: 0.0012741857208311558\n",
      "Batch 5190/14851, Loss: 0.02903401479125023\n",
      "Batch 5191/14851, Loss: 0.010034880600869656\n",
      "Batch 5192/14851, Loss: 0.010131911374628544\n",
      "Batch 5193/14851, Loss: 0.00202570972032845\n",
      "Batch 5194/14851, Loss: 0.00466541200876236\n",
      "Batch 5195/14851, Loss: 0.014610410667955875\n",
      "Batch 5196/14851, Loss: 0.011543769389390945\n",
      "Batch 5197/14851, Loss: 0.003060717135667801\n",
      "Batch 5198/14851, Loss: 0.041531503200531006\n",
      "Batch 5199/14851, Loss: 0.0009088839287869632\n",
      "Batch 5200/14851, Loss: 0.00025616586208343506\n",
      "Batch 5201/14851, Loss: 5.318845433066599e-05\n",
      "Batch 5202/14851, Loss: 0.005985397379845381\n",
      "Batch 5203/14851, Loss: 0.005348898470401764\n",
      "Batch 5204/14851, Loss: 0.001285380101762712\n",
      "Batch 5205/14851, Loss: 0.02418307401239872\n",
      "Batch 5206/14851, Loss: 0.0013239370891824365\n",
      "Batch 5207/14851, Loss: 0.03976533189415932\n",
      "Batch 5208/14851, Loss: 0.04577304422855377\n",
      "Batch 5209/14851, Loss: 0.005935877561569214\n",
      "Batch 5210/14851, Loss: 0.0011167111806571484\n",
      "Batch 5211/14851, Loss: 0.00027420869446359575\n",
      "Batch 5212/14851, Loss: 0.00048031038022600114\n",
      "Batch 5213/14851, Loss: 0.014874144457280636\n",
      "Batch 5214/14851, Loss: 0.00023670305381529033\n",
      "Batch 5215/14851, Loss: 0.01933770626783371\n",
      "Batch 5216/14851, Loss: 0.03497033938765526\n",
      "Batch 5217/14851, Loss: 0.000667918473482132\n",
      "Batch 5218/14851, Loss: 0.001537937787361443\n",
      "Batch 5219/14851, Loss: 0.024047384038567543\n",
      "Batch 5220/14851, Loss: 0.006245905067771673\n",
      "Batch 5221/14851, Loss: 0.027140766382217407\n",
      "Batch 5222/14851, Loss: 6.389369809767231e-05\n",
      "Batch 5223/14851, Loss: 0.012388737872242928\n",
      "Batch 5224/14851, Loss: 0.004021561238914728\n",
      "Batch 5225/14851, Loss: 0.019750092178583145\n",
      "Batch 5226/14851, Loss: 0.03339910879731178\n",
      "Batch 5227/14851, Loss: 0.02432507649064064\n",
      "Batch 5228/14851, Loss: 0.02594541572034359\n",
      "Batch 5229/14851, Loss: 0.00038354223943315446\n",
      "Batch 5230/14851, Loss: 0.0030332866590470076\n",
      "Batch 5231/14851, Loss: 0.012622367590665817\n",
      "Batch 5232/14851, Loss: 0.019682468846440315\n",
      "Batch 5233/14851, Loss: 0.026487071067094803\n",
      "Batch 5234/14851, Loss: 0.004885715898126364\n",
      "Batch 5235/14851, Loss: 0.0013956254115328193\n",
      "Batch 5236/14851, Loss: 0.0001245376915903762\n",
      "Batch 5237/14851, Loss: 0.004067903850227594\n",
      "Batch 5238/14851, Loss: 0.0018940294394269586\n",
      "Batch 5239/14851, Loss: 0.0036264907103031874\n",
      "Batch 5240/14851, Loss: 0.0023221808951348066\n",
      "Batch 5241/14851, Loss: 0.013718683272600174\n",
      "Batch 5242/14851, Loss: 0.00024831295013427734\n",
      "Batch 5243/14851, Loss: 0.0006199641502462327\n",
      "Batch 5244/14851, Loss: 0.0007331892848014832\n",
      "Batch 5245/14851, Loss: 0.040928710252046585\n",
      "Batch 5246/14851, Loss: 0.003332480788230896\n",
      "Batch 5247/14851, Loss: 0.06388959288597107\n",
      "Batch 5248/14851, Loss: 0.01744226925075054\n",
      "Batch 5249/14851, Loss: 6.034970283508301e-05\n",
      "Batch 5250/14851, Loss: 0.004598017316311598\n",
      "Batch 5251/14851, Loss: 0.05098801851272583\n",
      "Batch 5252/14851, Loss: 0.0004721023142337799\n",
      "Batch 5253/14851, Loss: 0.06153608486056328\n",
      "Batch 5254/14851, Loss: 0.0003801522252615541\n",
      "Batch 5255/14851, Loss: 0.00015788525342941284\n",
      "Batch 5256/14851, Loss: 0.005603273864835501\n",
      "Batch 5257/14851, Loss: 0.00359243038110435\n",
      "Batch 5258/14851, Loss: 0.03421829640865326\n",
      "Batch 5259/14851, Loss: 0.016244065016508102\n",
      "Batch 5260/14851, Loss: 0.0006219955976121128\n",
      "Batch 5261/14851, Loss: 0.0009097394649870694\n",
      "Batch 5262/14851, Loss: 0.00016290943312924355\n",
      "Batch 5263/14851, Loss: 0.000273258046945557\n",
      "Batch 5264/14851, Loss: 0.0001475811004638672\n",
      "Batch 5265/14851, Loss: 0.0023156951647251844\n",
      "Batch 5266/14851, Loss: 0.008613400161266327\n",
      "Batch 5267/14851, Loss: 0.0004325012268964201\n",
      "Batch 5268/14851, Loss: 0.002660491969436407\n",
      "Batch 5269/14851, Loss: 0.00421887869015336\n",
      "Batch 5270/14851, Loss: 0.049615878611803055\n",
      "Batch 5271/14851, Loss: 0.00028967857360839844\n",
      "Batch 5272/14851, Loss: 0.0035703815519809723\n",
      "Batch 5273/14851, Loss: 0.02187129482626915\n",
      "Batch 5274/14851, Loss: 0.009721246547996998\n",
      "Batch 5275/14851, Loss: 0.05138203501701355\n",
      "Batch 5276/14851, Loss: 0.00017105664301197976\n",
      "Batch 5277/14851, Loss: 0.0005690802936442196\n",
      "Batch 5278/14851, Loss: 0.014570663683116436\n",
      "Batch 5279/14851, Loss: 0.0011980707058683038\n",
      "Batch 5280/14851, Loss: 0.04838539659976959\n",
      "Batch 5281/14851, Loss: 0.020377352833747864\n",
      "Batch 5282/14851, Loss: 0.027793284505605698\n",
      "Batch 5283/14851, Loss: 0.044388141483068466\n",
      "Batch 5284/14851, Loss: 0.00026616951799951494\n",
      "Batch 5285/14851, Loss: 0.016299571841955185\n",
      "Batch 5286/14851, Loss: 0.025124069303274155\n",
      "Batch 5287/14851, Loss: 0.021388428285717964\n",
      "Batch 5288/14851, Loss: 0.006557960994541645\n",
      "Batch 5289/14851, Loss: 0.0012463880702853203\n",
      "Batch 5290/14851, Loss: 0.0020208607893437147\n",
      "Batch 5291/14851, Loss: 0.01004397589713335\n",
      "Batch 5292/14851, Loss: 0.0063070631586015224\n",
      "Batch 5293/14851, Loss: 0.00422689039260149\n",
      "Batch 5294/14851, Loss: 0.0175767932087183\n",
      "Batch 5295/14851, Loss: 0.00017250701785087585\n",
      "Batch 5296/14851, Loss: 0.01014802511781454\n",
      "Batch 5297/14851, Loss: 0.0003983067872468382\n",
      "Batch 5298/14851, Loss: 0.01249231118708849\n",
      "Batch 5299/14851, Loss: 0.03447972238063812\n",
      "Batch 5300/14851, Loss: 0.013192657381296158\n",
      "Batch 5301/14851, Loss: 0.003904547542333603\n",
      "Batch 5302/14851, Loss: 0.027594393119215965\n",
      "Batch 5303/14851, Loss: 0.0002768052218016237\n",
      "Batch 5304/14851, Loss: 0.0017266074428334832\n",
      "Batch 5305/14851, Loss: 0.03401201218366623\n",
      "Batch 5306/14851, Loss: 0.024659648537635803\n",
      "Batch 5307/14851, Loss: 0.05034530907869339\n",
      "Batch 5308/14851, Loss: 0.005599332507699728\n",
      "Batch 5309/14851, Loss: 0.000234742954489775\n",
      "Batch 5310/14851, Loss: 0.002499755471944809\n",
      "Batch 5311/14851, Loss: 0.02151588909327984\n",
      "Batch 5312/14851, Loss: 0.000884010165464133\n",
      "Batch 5313/14851, Loss: 0.005716691724956036\n",
      "Batch 5314/14851, Loss: 0.004847502335906029\n",
      "Batch 5315/14851, Loss: 0.004269024822860956\n",
      "Batch 5316/14851, Loss: 0.03928818553686142\n",
      "Batch 5317/14851, Loss: 0.012821772135794163\n",
      "Batch 5318/14851, Loss: 0.023883039131760597\n",
      "Batch 5319/14851, Loss: 0.0004099234938621521\n",
      "Batch 5320/14851, Loss: 0.00012600670743267983\n",
      "Batch 5321/14851, Loss: 0.004615085665136576\n",
      "Batch 5322/14851, Loss: 0.015991196036338806\n",
      "Batch 5323/14851, Loss: 0.068635493516922\n",
      "Batch 5324/14851, Loss: 0.0030382100958377123\n",
      "Batch 5325/14851, Loss: 0.020342787727713585\n",
      "Batch 5326/14851, Loss: 0.0007141816313378513\n",
      "Batch 5327/14851, Loss: 0.0049724034033715725\n",
      "Batch 5328/14851, Loss: 0.0007139879162423313\n",
      "Batch 5329/14851, Loss: 0.0003213249146938324\n",
      "Batch 5330/14851, Loss: 0.0017073703929781914\n",
      "Batch 5331/14851, Loss: 0.015684859827160835\n",
      "Batch 5332/14851, Loss: 0.0014963059220463037\n",
      "Batch 5333/14851, Loss: 0.012693450786173344\n",
      "Batch 5334/14851, Loss: 0.0010128592839464545\n",
      "Batch 5335/14851, Loss: 0.007010539527982473\n",
      "Batch 5336/14851, Loss: 0.00404469296336174\n",
      "Batch 5337/14851, Loss: 0.0033919811248779297\n",
      "Batch 5338/14851, Loss: 0.0005106094176881015\n",
      "Batch 5339/14851, Loss: 0.027851440012454987\n",
      "Batch 5340/14851, Loss: 0.006042102817445993\n",
      "Batch 5341/14851, Loss: 0.0008468851447105408\n",
      "Batch 5342/14851, Loss: 0.01838800311088562\n",
      "Batch 5343/14851, Loss: 0.00024956464767456055\n",
      "Batch 5344/14851, Loss: 9.535625576972961e-05\n",
      "Batch 5345/14851, Loss: 0.0014647655189037323\n",
      "Batch 5346/14851, Loss: 0.001919632195495069\n",
      "Batch 5347/14851, Loss: 0.09226392209529877\n",
      "Batch 5348/14851, Loss: 0.006664269138127565\n",
      "Batch 5349/14851, Loss: 0.02081565372645855\n",
      "Batch 5350/14851, Loss: 0.002505601616576314\n",
      "Batch 5351/14851, Loss: 0.0005287838284857571\n",
      "Batch 5352/14851, Loss: 0.0009463205933570862\n",
      "Batch 5353/14851, Loss: 0.00032154223299585283\n",
      "Batch 5354/14851, Loss: 0.026831267401576042\n",
      "Batch 5355/14851, Loss: 0.038280781358480453\n",
      "Batch 5356/14851, Loss: 0.00257931393571198\n",
      "Batch 5357/14851, Loss: 0.0013290606439113617\n",
      "Batch 5358/14851, Loss: 0.0050011915154755116\n",
      "Batch 5359/14851, Loss: 0.00016042590141296387\n",
      "Batch 5360/14851, Loss: 0.03127395361661911\n",
      "Batch 5361/14851, Loss: 0.0009355917572975159\n",
      "Batch 5362/14851, Loss: 0.0002651239337865263\n",
      "Batch 5363/14851, Loss: 0.0016729211201891303\n",
      "Batch 5364/14851, Loss: 0.026578404009342194\n",
      "Batch 5365/14851, Loss: 0.016857357695698738\n",
      "Batch 5366/14851, Loss: 0.06890527158975601\n",
      "Batch 5367/14851, Loss: 0.03169311583042145\n",
      "Batch 5368/14851, Loss: 0.0008104120497591794\n",
      "Batch 5369/14851, Loss: 0.0013928820844739676\n",
      "Batch 5370/14851, Loss: 0.03157703951001167\n",
      "Batch 5371/14851, Loss: 0.001462422078475356\n",
      "Batch 5372/14851, Loss: 0.009397484362125397\n",
      "Batch 5373/14851, Loss: 0.0034779380075633526\n",
      "Batch 5374/14851, Loss: 0.0005836226046085358\n",
      "Batch 5375/14851, Loss: 0.03355836868286133\n",
      "Batch 5376/14851, Loss: 0.0014530891785398126\n",
      "Batch 5377/14851, Loss: 0.01126892864704132\n",
      "Batch 5378/14851, Loss: 0.022107521072030067\n",
      "Batch 5379/14851, Loss: 0.015064695850014687\n",
      "Batch 5380/14851, Loss: 0.003763437271118164\n",
      "Batch 5381/14851, Loss: 0.0023070673923939466\n",
      "Batch 5382/14851, Loss: 0.03176456317305565\n",
      "Batch 5383/14851, Loss: 0.0016324531752616167\n",
      "Batch 5384/14851, Loss: 0.02674148976802826\n",
      "Batch 5385/14851, Loss: 0.00048768272972665727\n",
      "Batch 5386/14851, Loss: 0.02552848681807518\n",
      "Batch 5387/14851, Loss: 0.0025912225246429443\n",
      "Batch 5388/14851, Loss: 0.005696152802556753\n",
      "Batch 5389/14851, Loss: 0.0017617695266380906\n",
      "Batch 5390/14851, Loss: 0.012188778258860111\n",
      "Batch 5391/14851, Loss: 0.007074352353811264\n",
      "Batch 5392/14851, Loss: 0.0025715914089232683\n",
      "Batch 5393/14851, Loss: 0.042178940027952194\n",
      "Batch 5394/14851, Loss: 0.023535965010523796\n",
      "Batch 5395/14851, Loss: 0.02757188118994236\n",
      "Batch 5396/14851, Loss: 0.028797989711165428\n",
      "Batch 5397/14851, Loss: 0.00043856663978658617\n",
      "Batch 5398/14851, Loss: 0.0017958196112886071\n",
      "Batch 5399/14851, Loss: 0.02006317675113678\n",
      "Batch 5400/14851, Loss: 0.001475742319598794\n",
      "Batch 5401/14851, Loss: 0.0027949772775173187\n",
      "Batch 5402/14851, Loss: 0.02053883671760559\n",
      "Batch 5403/14851, Loss: 0.0011441136011853814\n",
      "Batch 5404/14851, Loss: 0.03245668113231659\n",
      "Batch 5405/14851, Loss: 0.0003088235971517861\n",
      "Batch 5406/14851, Loss: 0.004787036217749119\n",
      "Batch 5407/14851, Loss: 0.00664938660338521\n",
      "Batch 5408/14851, Loss: 0.0005892813205718994\n",
      "Batch 5409/14851, Loss: 0.01764879748225212\n",
      "Batch 5410/14851, Loss: 3.918011861969717e-05\n",
      "Batch 5411/14851, Loss: 0.0027620058972388506\n",
      "Batch 5412/14851, Loss: 0.00020086020231246948\n",
      "Batch 5413/14851, Loss: 0.0005555344396270812\n",
      "Batch 5414/14851, Loss: 0.0014329241821542382\n",
      "Batch 5415/14851, Loss: 0.021581120789051056\n",
      "Batch 5416/14851, Loss: 0.006472197826951742\n",
      "Batch 5417/14851, Loss: 0.008630579337477684\n",
      "Batch 5418/14851, Loss: 0.00016701221466064453\n",
      "Batch 5419/14851, Loss: 0.018993863835930824\n",
      "Batch 5420/14851, Loss: 0.019511107355356216\n",
      "Batch 5421/14851, Loss: 0.02240048535168171\n",
      "Batch 5422/14851, Loss: 0.014168106950819492\n",
      "Batch 5423/14851, Loss: 0.0005973749211989343\n",
      "Batch 5424/14851, Loss: 0.009261407889425755\n",
      "Batch 5425/14851, Loss: 0.001528628170490265\n",
      "Batch 5426/14851, Loss: 0.04895610362291336\n",
      "Batch 5427/14851, Loss: 0.00845395028591156\n",
      "Batch 5428/14851, Loss: 0.01006843987852335\n",
      "Batch 5429/14851, Loss: 0.009180858731269836\n",
      "Batch 5430/14851, Loss: 0.005892094690352678\n",
      "Batch 5431/14851, Loss: 0.0024990004021674395\n",
      "Batch 5432/14851, Loss: 0.004611258860677481\n",
      "Batch 5433/14851, Loss: 0.01409767847508192\n",
      "Batch 5434/14851, Loss: 0.00186983123421669\n",
      "Batch 5435/14851, Loss: 0.008615811355412006\n",
      "Batch 5436/14851, Loss: 0.022750230506062508\n",
      "Batch 5437/14851, Loss: 7.707750046392903e-05\n",
      "Batch 5438/14851, Loss: 0.00732986256480217\n",
      "Batch 5439/14851, Loss: 0.002998895011842251\n",
      "Batch 5440/14851, Loss: 0.0016427673399448395\n",
      "Batch 5441/14851, Loss: 0.0015420516720041633\n",
      "Batch 5442/14851, Loss: 0.0007250470225699246\n",
      "Batch 5443/14851, Loss: 0.03133976086974144\n",
      "Batch 5444/14851, Loss: 0.015076679177582264\n",
      "Batch 5445/14851, Loss: 0.001312845735810697\n",
      "Batch 5446/14851, Loss: 0.008749433793127537\n",
      "Batch 5447/14851, Loss: 0.0071283080615103245\n",
      "Batch 5448/14851, Loss: 0.005721121560782194\n",
      "Batch 5449/14851, Loss: 0.028743410483002663\n",
      "Batch 5450/14851, Loss: 0.0005279971519485116\n",
      "Batch 5451/14851, Loss: 0.03355736657977104\n",
      "Batch 5452/14851, Loss: 0.016681551933288574\n",
      "Batch 5453/14851, Loss: 0.0043869102373719215\n",
      "Batch 5454/14851, Loss: 0.0016218044329434633\n",
      "Batch 5455/14851, Loss: 0.007750505115836859\n",
      "Batch 5456/14851, Loss: 0.00017882634710986167\n",
      "Batch 5457/14851, Loss: 0.025553710758686066\n",
      "Batch 5458/14851, Loss: 0.0009074897971004248\n",
      "Batch 5459/14851, Loss: 0.04245327040553093\n",
      "Batch 5460/14851, Loss: 0.013293605297803879\n",
      "Batch 5461/14851, Loss: 0.00284363585524261\n",
      "Batch 5462/14851, Loss: 0.0011291304836049676\n",
      "Batch 5463/14851, Loss: 0.0014402233064174652\n",
      "Batch 5464/14851, Loss: 0.03481684997677803\n",
      "Batch 5465/14851, Loss: 0.023938260972499847\n",
      "Batch 5466/14851, Loss: 0.0002654939889907837\n",
      "Batch 5467/14851, Loss: 0.0008742001373320818\n",
      "Batch 5468/14851, Loss: 0.0881674662232399\n",
      "Batch 5469/14851, Loss: 0.001844619051553309\n",
      "Batch 5470/14851, Loss: 0.02125026285648346\n",
      "Batch 5471/14851, Loss: 0.011428830213844776\n",
      "Batch 5472/14851, Loss: 0.00163361057639122\n",
      "Batch 5473/14851, Loss: 0.00033331525628454983\n",
      "Batch 5474/14851, Loss: 0.0290676262229681\n",
      "Batch 5475/14851, Loss: 0.02779269963502884\n",
      "Batch 5476/14851, Loss: 0.004491783678531647\n",
      "Batch 5477/14851, Loss: 0.005069176200777292\n",
      "Batch 5478/14851, Loss: 0.019695663824677467\n",
      "Batch 5479/14851, Loss: 0.00018083553004544228\n",
      "Batch 5480/14851, Loss: 0.07441005855798721\n",
      "Batch 5481/14851, Loss: 0.008443127386271954\n",
      "Batch 5482/14851, Loss: 0.030914951115846634\n",
      "Batch 5483/14851, Loss: 0.00025255605578422546\n",
      "Batch 5484/14851, Loss: 0.010939939878880978\n",
      "Batch 5485/14851, Loss: 0.0030309807043522596\n",
      "Batch 5486/14851, Loss: 0.026689771562814713\n",
      "Batch 5487/14851, Loss: 0.01050008088350296\n",
      "Batch 5488/14851, Loss: 0.0008938436512835324\n",
      "Batch 5489/14851, Loss: 0.0018482148880138993\n",
      "Batch 5490/14851, Loss: 0.0018297433853149414\n",
      "Batch 5491/14851, Loss: 0.024143027141690254\n",
      "Batch 5492/14851, Loss: 0.034156884998083115\n",
      "Batch 5493/14851, Loss: 0.0007593248737975955\n",
      "Batch 5494/14851, Loss: 0.002611402887851\n",
      "Batch 5495/14851, Loss: 0.021765243262052536\n",
      "Batch 5496/14851, Loss: 0.0331672802567482\n",
      "Batch 5497/14851, Loss: 0.0005595025722868741\n",
      "Batch 5498/14851, Loss: 0.008914915844798088\n",
      "Batch 5499/14851, Loss: 0.022996041923761368\n",
      "Batch 5500/14851, Loss: 0.005902981851249933\n",
      "Batch 5501/14851, Loss: 0.009392037987709045\n",
      "Batch 5502/14851, Loss: 0.02071847952902317\n",
      "Batch 5503/14851, Loss: 0.0004726015031337738\n",
      "Batch 5504/14851, Loss: 0.0013108812272548676\n",
      "Batch 5505/14851, Loss: 0.004197589587420225\n",
      "Batch 5506/14851, Loss: 0.0006573076243512332\n",
      "Batch 5507/14851, Loss: 0.002525089541450143\n",
      "Batch 5508/14851, Loss: 0.009723955765366554\n",
      "Batch 5509/14851, Loss: 0.0006706180865876377\n",
      "Batch 5510/14851, Loss: 0.01305308286100626\n",
      "Batch 5511/14851, Loss: 0.007521539926528931\n",
      "Batch 5512/14851, Loss: 0.02084198407828808\n",
      "Batch 5513/14851, Loss: 0.002355296164751053\n",
      "Batch 5514/14851, Loss: 0.011695570312440395\n",
      "Batch 5515/14851, Loss: 0.029999813064932823\n",
      "Batch 5516/14851, Loss: 0.01711530052125454\n",
      "Batch 5517/14851, Loss: 0.002009571762755513\n",
      "Batch 5518/14851, Loss: 0.006680579390376806\n",
      "Batch 5519/14851, Loss: 0.004092734307050705\n",
      "Batch 5520/14851, Loss: 0.01236035767942667\n",
      "Batch 5521/14851, Loss: 0.006898144725710154\n",
      "Batch 5522/14851, Loss: 0.004098424222320318\n",
      "Batch 5523/14851, Loss: 0.00485450541600585\n",
      "Batch 5524/14851, Loss: 0.011231858283281326\n",
      "Batch 5525/14851, Loss: 0.0024383217096328735\n",
      "Batch 5526/14851, Loss: 0.031037474051117897\n",
      "Batch 5527/14851, Loss: 0.0010067189577966928\n",
      "Batch 5528/14851, Loss: 0.00029254829860292375\n",
      "Batch 5529/14851, Loss: 0.0165912676602602\n",
      "Batch 5530/14851, Loss: 0.0001413486897945404\n",
      "Batch 5531/14851, Loss: 0.00011984001321252435\n",
      "Batch 5532/14851, Loss: 0.04351568967103958\n",
      "Batch 5533/14851, Loss: 0.003941148519515991\n",
      "Batch 5534/14851, Loss: 0.00023097793746273965\n",
      "Batch 5535/14851, Loss: 0.037091463804244995\n",
      "Batch 5536/14851, Loss: 0.004399007651954889\n",
      "Batch 5537/14851, Loss: 0.0002918181417044252\n",
      "Batch 5538/14851, Loss: 0.013542652130126953\n",
      "Batch 5539/14851, Loss: 0.0009863778250291944\n",
      "Batch 5540/14851, Loss: 0.0012430321658030152\n",
      "Batch 5541/14851, Loss: 0.013553455471992493\n",
      "Batch 5542/14851, Loss: 0.0014403018867596984\n",
      "Batch 5543/14851, Loss: 0.00027521574520505965\n",
      "Batch 5544/14851, Loss: 0.0028667417354881763\n",
      "Batch 5545/14851, Loss: 0.022609662264585495\n",
      "Batch 5546/14851, Loss: 0.003077937988564372\n",
      "Batch 5547/14851, Loss: 0.00015108038496691734\n",
      "Batch 5548/14851, Loss: 0.00011278688907623291\n",
      "Batch 5549/14851, Loss: 0.0034687973093241453\n",
      "Batch 5550/14851, Loss: 0.037483442574739456\n",
      "Batch 5551/14851, Loss: 0.0013063997030258179\n",
      "Batch 5552/14851, Loss: 0.015005393885076046\n",
      "Batch 5553/14851, Loss: 0.017961475998163223\n",
      "Batch 5554/14851, Loss: 0.011556689627468586\n",
      "Batch 5555/14851, Loss: 0.012995286844670773\n",
      "Batch 5556/14851, Loss: 0.029918406158685684\n",
      "Batch 5557/14851, Loss: 0.0434110201895237\n",
      "Batch 5558/14851, Loss: 0.0010620678076520562\n",
      "Batch 5559/14851, Loss: 0.00033369040465913713\n",
      "Batch 5560/14851, Loss: 0.04867411404848099\n",
      "Batch 5561/14851, Loss: 0.0014276632573455572\n",
      "Batch 5562/14851, Loss: 0.00010018298780778423\n",
      "Batch 5563/14851, Loss: 0.0033817340154200792\n",
      "Batch 5564/14851, Loss: 0.0016860502073541284\n",
      "Batch 5565/14851, Loss: 0.0010044500231742859\n",
      "Batch 5566/14851, Loss: 0.0006167516112327576\n",
      "Batch 5567/14851, Loss: 0.0009377387468703091\n",
      "Batch 5568/14851, Loss: 0.0014691526303067803\n",
      "Batch 5569/14851, Loss: 0.0021916497498750687\n",
      "Batch 5570/14851, Loss: 0.00029217576957307756\n",
      "Batch 5571/14851, Loss: 0.07818599045276642\n",
      "Batch 5572/14851, Loss: 0.0008619030122645199\n",
      "Batch 5573/14851, Loss: 0.0007378818700090051\n",
      "Batch 5574/14851, Loss: 0.009588764980435371\n",
      "Batch 5575/14851, Loss: 0.006387279834598303\n",
      "Batch 5576/14851, Loss: 0.042111046612262726\n",
      "Batch 5577/14851, Loss: 0.00012558822345454246\n",
      "Batch 5578/14851, Loss: 0.0014620199799537659\n",
      "Batch 5579/14851, Loss: 0.0028707587625831366\n",
      "Batch 5580/14851, Loss: 0.0074882907792925835\n",
      "Batch 5581/14851, Loss: 0.010433889925479889\n",
      "Batch 5582/14851, Loss: 0.003177602542564273\n",
      "Batch 5583/14851, Loss: 0.001511836308054626\n",
      "Batch 5584/14851, Loss: 0.0021618541795760393\n",
      "Batch 5585/14851, Loss: 0.0690581351518631\n",
      "Batch 5586/14851, Loss: 0.009141161106526852\n",
      "Batch 5587/14851, Loss: 0.00038223466253839433\n",
      "Batch 5588/14851, Loss: 0.007735785562545061\n",
      "Batch 5589/14851, Loss: 0.00045606991625390947\n",
      "Batch 5590/14851, Loss: 0.014736625365912914\n",
      "Batch 5591/14851, Loss: 0.0005742671783082187\n",
      "Batch 5592/14851, Loss: 0.0053079924546182156\n",
      "Batch 5593/14851, Loss: 0.0012573860585689545\n",
      "Batch 5594/14851, Loss: 0.00043602785444818437\n",
      "Batch 5595/14851, Loss: 0.002004781737923622\n",
      "Batch 5596/14851, Loss: 0.002727436600252986\n",
      "Batch 5597/14851, Loss: 0.0029540008399635553\n",
      "Batch 5598/14851, Loss: 0.011330928653478622\n",
      "Batch 5599/14851, Loss: 0.0004770441446453333\n",
      "Batch 5600/14851, Loss: 0.0009944529738277197\n",
      "Batch 5601/14851, Loss: 0.005844705738127232\n",
      "Batch 5602/14851, Loss: 3.586212915251963e-05\n",
      "Batch 5603/14851, Loss: 0.00012526909995358437\n",
      "Batch 5604/14851, Loss: 0.000550945580471307\n",
      "Batch 5605/14851, Loss: 0.04129401594400406\n",
      "Batch 5606/14851, Loss: 0.00011435896158218384\n",
      "Batch 5607/14851, Loss: 0.035687047988176346\n",
      "Batch 5608/14851, Loss: 0.00709453085437417\n",
      "Batch 5609/14851, Loss: 0.002408993663266301\n",
      "Batch 5610/14851, Loss: 0.006536914501339197\n",
      "Batch 5611/14851, Loss: 0.0007576753268949687\n",
      "Batch 5612/14851, Loss: 3.505994754959829e-05\n",
      "Batch 5613/14851, Loss: 0.0014547320315614343\n",
      "Batch 5614/14851, Loss: 0.00029537081718444824\n",
      "Batch 5615/14851, Loss: 0.004236205480992794\n",
      "Batch 5616/14851, Loss: 0.0009655723697505891\n",
      "Batch 5617/14851, Loss: 0.0018222630023956299\n",
      "Batch 5618/14851, Loss: 0.0006890930235385895\n",
      "Batch 5619/14851, Loss: 0.003619545605033636\n",
      "Batch 5620/14851, Loss: 0.00458495831117034\n",
      "Batch 5621/14851, Loss: 3.108133751084097e-05\n",
      "Batch 5622/14851, Loss: 0.009118511341512203\n",
      "Batch 5623/14851, Loss: 0.030249815434217453\n",
      "Batch 5624/14851, Loss: 0.011311578564345837\n",
      "Batch 5625/14851, Loss: 0.00010085850954055786\n",
      "Batch 5626/14851, Loss: 0.0407545305788517\n",
      "Batch 5627/14851, Loss: 0.0027053942903876305\n",
      "Batch 5628/14851, Loss: 6.631016731262207e-05\n",
      "Batch 5629/14851, Loss: 0.061985597014427185\n",
      "Batch 5630/14851, Loss: 0.009699895046651363\n",
      "Batch 5631/14851, Loss: 0.02919883280992508\n",
      "Batch 5632/14851, Loss: 0.0008595969993621111\n",
      "Batch 5633/14851, Loss: 0.00042280182242393494\n",
      "Batch 5634/14851, Loss: 0.0007862250204198062\n",
      "Batch 5635/14851, Loss: 0.002743258373811841\n",
      "Batch 5636/14851, Loss: 0.09958989173173904\n",
      "Batch 5637/14851, Loss: 0.0036467111203819513\n",
      "Batch 5638/14851, Loss: 0.00921484362334013\n",
      "Batch 5639/14851, Loss: 0.00013992935419082642\n",
      "Batch 5640/14851, Loss: 0.08009737730026245\n",
      "Batch 5641/14851, Loss: 0.000412143359426409\n",
      "Batch 5642/14851, Loss: 0.00030011311173439026\n",
      "Batch 5643/14851, Loss: 8.85141416802071e-05\n",
      "Batch 5644/14851, Loss: 0.0005828033899888396\n",
      "Batch 5645/14851, Loss: 7.750093936920166e-05\n",
      "Batch 5646/14851, Loss: 0.0020962601993232965\n",
      "Batch 5647/14851, Loss: 0.0017236495623365045\n",
      "Batch 5648/14851, Loss: 0.0013586817076429725\n",
      "Batch 5649/14851, Loss: 0.004327511880546808\n",
      "Batch 5650/14851, Loss: 0.0006096524302847683\n",
      "Batch 5651/14851, Loss: 0.0010932199656963348\n",
      "Batch 5652/14851, Loss: 0.041450534015893936\n",
      "Batch 5653/14851, Loss: 0.018362784758210182\n",
      "Batch 5654/14851, Loss: 0.0004019177576992661\n",
      "Batch 5655/14851, Loss: 0.011902816593647003\n",
      "Batch 5656/14851, Loss: 5.88931143283844e-05\n",
      "Batch 5657/14851, Loss: 0.0014317611930891871\n",
      "Batch 5658/14851, Loss: 0.0013856043806299567\n",
      "Batch 5659/14851, Loss: 0.003869830397889018\n",
      "Batch 5660/14851, Loss: 0.019144698977470398\n",
      "Batch 5661/14851, Loss: 0.008702142164111137\n",
      "Batch 5662/14851, Loss: 0.0026018659118562937\n",
      "Batch 5663/14851, Loss: 0.05566344037652016\n",
      "Batch 5664/14851, Loss: 0.00013025477528572083\n",
      "Batch 5665/14851, Loss: 0.0003731437027454376\n",
      "Batch 5666/14851, Loss: 0.0037212285678833723\n",
      "Batch 5667/14851, Loss: 0.001019451767206192\n",
      "Batch 5668/14851, Loss: 0.0002151516528101638\n",
      "Batch 5669/14851, Loss: 0.0018007891485467553\n",
      "Batch 5670/14851, Loss: 0.0015324527630582452\n",
      "Batch 5671/14851, Loss: 0.021802116185426712\n",
      "Batch 5672/14851, Loss: 0.0033103537280112505\n",
      "Batch 5673/14851, Loss: 0.0026196290273219347\n",
      "Batch 5674/14851, Loss: 0.03076494298875332\n",
      "Batch 5675/14851, Loss: 0.03343635052442551\n",
      "Batch 5676/14851, Loss: 0.0023408078122884035\n",
      "Batch 5677/14851, Loss: 0.002675950527191162\n",
      "Batch 5678/14851, Loss: 0.000481421739095822\n",
      "Batch 5679/14851, Loss: 0.0004246719181537628\n",
      "Batch 5680/14851, Loss: 0.002334637101739645\n",
      "Batch 5681/14851, Loss: 0.0012479565339162946\n",
      "Batch 5682/14851, Loss: 0.0012216580798849463\n",
      "Batch 5683/14851, Loss: 0.003603696823120117\n",
      "Batch 5684/14851, Loss: 0.08732249587774277\n",
      "Batch 5685/14851, Loss: 0.0003213969466742128\n",
      "Batch 5686/14851, Loss: 0.0010281516006216407\n",
      "Batch 5687/14851, Loss: 0.021598616614937782\n",
      "Batch 5688/14851, Loss: 0.002158955903723836\n",
      "Batch 5689/14851, Loss: 0.00019938620971515775\n",
      "Batch 5690/14851, Loss: 0.024849265813827515\n",
      "Batch 5691/14851, Loss: 0.004879756364971399\n",
      "Batch 5692/14851, Loss: 0.006218078080564737\n",
      "Batch 5693/14851, Loss: 0.05754154548048973\n",
      "Batch 5694/14851, Loss: 0.04716958850622177\n",
      "Batch 5695/14851, Loss: 0.008563868701457977\n",
      "Batch 5696/14851, Loss: 0.015729276463389397\n",
      "Batch 5697/14851, Loss: 0.000569559633731842\n",
      "Batch 5698/14851, Loss: 0.019626611843705177\n",
      "Batch 5699/14851, Loss: 0.02797536551952362\n",
      "Batch 5700/14851, Loss: 0.0002510999911464751\n",
      "Batch 5701/14851, Loss: 0.029466282576322556\n",
      "Batch 5702/14851, Loss: 0.04467828571796417\n",
      "Batch 5703/14851, Loss: 0.021383939310908318\n",
      "Batch 5704/14851, Loss: 0.0009200560743920505\n",
      "Batch 5705/14851, Loss: 0.0005442177061922848\n",
      "Batch 5706/14851, Loss: 0.016069311648607254\n",
      "Batch 5707/14851, Loss: 0.018009768798947334\n",
      "Batch 5708/14851, Loss: 0.00020170088100712746\n",
      "Batch 5709/14851, Loss: 0.002776114968582988\n",
      "Batch 5710/14851, Loss: 0.005233509931713343\n",
      "Batch 5711/14851, Loss: 0.0003812772629316896\n",
      "Batch 5712/14851, Loss: 0.00027629980468191206\n",
      "Batch 5713/14851, Loss: 0.04538096860051155\n",
      "Batch 5714/14851, Loss: 0.02536882273852825\n",
      "Batch 5715/14851, Loss: 5.1854800403816625e-05\n",
      "Batch 5716/14851, Loss: 0.0032939293887466192\n",
      "Batch 5717/14851, Loss: 0.006265793461352587\n",
      "Batch 5718/14851, Loss: 4.213303327560425e-05\n",
      "Batch 5719/14851, Loss: 0.03760677948594093\n",
      "Batch 5720/14851, Loss: 0.037977542728185654\n",
      "Batch 5721/14851, Loss: 0.00023315277940127999\n",
      "Batch 5722/14851, Loss: 0.02471507526934147\n",
      "Batch 5723/14851, Loss: 0.0010309331119060516\n",
      "Batch 5724/14851, Loss: 0.012490733526647091\n",
      "Batch 5725/14851, Loss: 0.020903797820210457\n",
      "Batch 5726/14851, Loss: 0.00010747462511062622\n",
      "Batch 5727/14851, Loss: 0.014712519943714142\n",
      "Batch 5728/14851, Loss: 0.0034735414665192366\n",
      "Batch 5729/14851, Loss: 0.0007933254237286747\n",
      "Batch 5730/14851, Loss: 0.003274122951552272\n",
      "Batch 5731/14851, Loss: 0.012199170887470245\n",
      "Batch 5732/14851, Loss: 0.004010939039289951\n",
      "Batch 5733/14851, Loss: 0.02536613866686821\n",
      "Batch 5734/14851, Loss: 0.0049883113242685795\n",
      "Batch 5735/14851, Loss: 0.0015228266129270196\n",
      "Batch 5736/14851, Loss: 0.004563414491713047\n",
      "Batch 5737/14851, Loss: 0.002857979154214263\n",
      "Batch 5738/14851, Loss: 0.0025097529869526625\n",
      "Batch 5739/14851, Loss: 0.06930963695049286\n",
      "Batch 5740/14851, Loss: 0.03303506225347519\n",
      "Batch 5741/14851, Loss: 6.68354332447052e-05\n",
      "Batch 5742/14851, Loss: 0.005703834351152182\n",
      "Batch 5743/14851, Loss: 0.0234922394156456\n",
      "Batch 5744/14851, Loss: 0.03972227871417999\n",
      "Batch 5745/14851, Loss: 0.027441807091236115\n",
      "Batch 5746/14851, Loss: 0.021569417789578438\n",
      "Batch 5747/14851, Loss: 0.02974284626543522\n",
      "Batch 5748/14851, Loss: 0.026358947157859802\n",
      "Batch 5749/14851, Loss: 0.017571326345205307\n",
      "Batch 5750/14851, Loss: 0.026030488312244415\n",
      "Batch 5751/14851, Loss: 0.0014154439559206367\n",
      "Batch 5752/14851, Loss: 0.0002115108072757721\n",
      "Batch 5753/14851, Loss: 0.0010776022681966424\n",
      "Batch 5754/14851, Loss: 0.00892624445259571\n",
      "Batch 5755/14851, Loss: 0.009321382269263268\n",
      "Batch 5756/14851, Loss: 0.004871045239269733\n",
      "Batch 5757/14851, Loss: 0.0077964067459106445\n",
      "Batch 5758/14851, Loss: 0.02528846263885498\n",
      "Batch 5759/14851, Loss: 0.016070382669568062\n",
      "Batch 5760/14851, Loss: 0.00038503322866745293\n",
      "Batch 5761/14851, Loss: 0.00012669463467318565\n",
      "Batch 5762/14851, Loss: 0.006032997276633978\n",
      "Batch 5763/14851, Loss: 0.03166614845395088\n",
      "Batch 5764/14851, Loss: 0.0009385856683366001\n",
      "Batch 5765/14851, Loss: 0.0002493696811143309\n",
      "Batch 5766/14851, Loss: 0.008517405949532986\n",
      "Batch 5767/14851, Loss: 0.000679814547766\n",
      "Batch 5768/14851, Loss: 0.006005194503813982\n",
      "Batch 5769/14851, Loss: 0.0013258695835247636\n",
      "Batch 5770/14851, Loss: 0.00968379806727171\n",
      "Batch 5771/14851, Loss: 0.001065643155016005\n",
      "Batch 5772/14851, Loss: 0.00251557189039886\n",
      "Batch 5773/14851, Loss: 0.0005099847912788391\n",
      "Batch 5774/14851, Loss: 0.014081394299864769\n",
      "Batch 5775/14851, Loss: 0.00030069673084653914\n",
      "Batch 5776/14851, Loss: 0.008337135426700115\n",
      "Batch 5777/14851, Loss: 0.016677167266607285\n",
      "Batch 5778/14851, Loss: 0.0026988654863089323\n",
      "Batch 5779/14851, Loss: 0.00015952810645103455\n",
      "Batch 5780/14851, Loss: 0.018756447359919548\n",
      "Batch 5781/14851, Loss: 0.011355848051607609\n",
      "Batch 5782/14851, Loss: 0.013211927376687527\n",
      "Batch 5783/14851, Loss: 0.038725487887859344\n",
      "Batch 5784/14851, Loss: 0.008820672519505024\n",
      "Batch 5785/14851, Loss: 0.002543544163927436\n",
      "Batch 5786/14851, Loss: 0.0011620894074440002\n",
      "Batch 5787/14851, Loss: 4.753221946884878e-05\n",
      "Batch 5788/14851, Loss: 0.008652396500110626\n",
      "Batch 5789/14851, Loss: 0.015628021210432053\n",
      "Batch 5790/14851, Loss: 0.013026401400566101\n",
      "Batch 5791/14851, Loss: 7.869675755500793e-05\n",
      "Batch 5792/14851, Loss: 0.0006520768511109054\n",
      "Batch 5793/14851, Loss: 0.05410156399011612\n",
      "Batch 5794/14851, Loss: 0.01690055988729\n",
      "Batch 5795/14851, Loss: 0.0029485670384019613\n",
      "Batch 5796/14851, Loss: 0.000286198133835569\n",
      "Batch 5797/14851, Loss: 0.00010082001244882122\n",
      "Batch 5798/14851, Loss: 0.0043968032114207745\n",
      "Batch 5799/14851, Loss: 0.032022155821323395\n",
      "Batch 5800/14851, Loss: 0.008350851945579052\n",
      "Batch 5801/14851, Loss: 0.018649281933903694\n",
      "Batch 5802/14851, Loss: 0.0011846640845760703\n",
      "Batch 5803/14851, Loss: 2.664948442543391e-05\n",
      "Batch 5804/14851, Loss: 0.04267050325870514\n",
      "Batch 5805/14851, Loss: 0.0006275490159168839\n",
      "Batch 5806/14851, Loss: 0.00965792965143919\n",
      "Batch 5807/14851, Loss: 0.01891782507300377\n",
      "Batch 5808/14851, Loss: 0.00016888354730326682\n",
      "Batch 5809/14851, Loss: 0.0005608361680060625\n",
      "Batch 5810/14851, Loss: 0.010191956534981728\n",
      "Batch 5811/14851, Loss: 0.0010837713489308953\n",
      "Batch 5812/14851, Loss: 0.0034726765006780624\n",
      "Batch 5813/14851, Loss: 0.023973429575562477\n",
      "Batch 5814/14851, Loss: 0.0010283166775479913\n",
      "Batch 5815/14851, Loss: 0.002103174338117242\n",
      "Batch 5816/14851, Loss: 0.006252036429941654\n",
      "Batch 5817/14851, Loss: 0.01290848571807146\n",
      "Batch 5818/14851, Loss: 0.0003057826543226838\n",
      "Batch 5819/14851, Loss: 0.02507633902132511\n",
      "Batch 5820/14851, Loss: 0.0004992485046386719\n",
      "Batch 5821/14851, Loss: 0.00010509540879866108\n",
      "Batch 5822/14851, Loss: 0.0016611894825473428\n",
      "Batch 5823/14851, Loss: 0.03624168410897255\n",
      "Batch 5824/14851, Loss: 0.007697141729295254\n",
      "Batch 5825/14851, Loss: 0.033002108335494995\n",
      "Batch 5826/14851, Loss: 0.03245749697089195\n",
      "Batch 5827/14851, Loss: 0.0009685668046586215\n",
      "Batch 5828/14851, Loss: 0.03387822210788727\n",
      "Batch 5829/14851, Loss: 0.0006396782700903714\n",
      "Batch 5830/14851, Loss: 0.00034136822796426713\n",
      "Batch 5831/14851, Loss: 0.023289576172828674\n",
      "Batch 5832/14851, Loss: 0.004141378682106733\n",
      "Batch 5833/14851, Loss: 0.0005851549212820828\n",
      "Batch 5834/14851, Loss: 0.0069094086065888405\n",
      "Batch 5835/14851, Loss: 0.0006809619371779263\n",
      "Batch 5836/14851, Loss: 0.012461104430258274\n",
      "Batch 5837/14851, Loss: 0.009537678211927414\n",
      "Batch 5838/14851, Loss: 0.009401742368936539\n",
      "Batch 5839/14851, Loss: 0.047527655959129333\n",
      "Batch 5840/14851, Loss: 0.00736594432964921\n",
      "Batch 5841/14851, Loss: 0.011602812446653843\n",
      "Batch 5842/14851, Loss: 0.0011538515100255609\n",
      "Batch 5843/14851, Loss: 0.0008271721308119595\n",
      "Batch 5844/14851, Loss: 0.0005945253069512546\n",
      "Batch 5845/14851, Loss: 0.0018345594871789217\n",
      "Batch 5846/14851, Loss: 0.01347582507878542\n",
      "Batch 5847/14851, Loss: 0.009447924792766571\n",
      "Batch 5848/14851, Loss: 0.00314987450838089\n",
      "Batch 5849/14851, Loss: 0.000462590396637097\n",
      "Batch 5850/14851, Loss: 0.008111709728837013\n",
      "Batch 5851/14851, Loss: 0.0009484104230068624\n",
      "Batch 5852/14851, Loss: 0.015608640387654305\n",
      "Batch 5853/14851, Loss: 0.02433040551841259\n",
      "Batch 5854/14851, Loss: 0.005293252412229776\n",
      "Batch 5855/14851, Loss: 0.002658529905602336\n",
      "Batch 5856/14851, Loss: 0.0015956138959154487\n",
      "Batch 5857/14851, Loss: 0.029498040676116943\n",
      "Batch 5858/14851, Loss: 0.0037096620071679354\n",
      "Batch 5859/14851, Loss: 0.000476691871881485\n",
      "Batch 5860/14851, Loss: 0.002885999623686075\n",
      "Batch 5861/14851, Loss: 0.04564034566283226\n",
      "Batch 5862/14851, Loss: 0.03810975328087807\n",
      "Batch 5863/14851, Loss: 0.020493898540735245\n",
      "Batch 5864/14851, Loss: 0.03495720773935318\n",
      "Batch 5865/14851, Loss: 0.0006146207451820374\n",
      "Batch 5866/14851, Loss: 0.005021336954087019\n",
      "Batch 5867/14851, Loss: 0.000301409512758255\n",
      "Batch 5868/14851, Loss: 0.007964438758790493\n",
      "Batch 5869/14851, Loss: 0.0027008492033928633\n",
      "Batch 5870/14851, Loss: 4.973759132553823e-05\n",
      "Batch 5871/14851, Loss: 0.009039782918989658\n",
      "Batch 5872/14851, Loss: 0.0027103025931864977\n",
      "Batch 5873/14851, Loss: 0.0041141994297504425\n",
      "Batch 5874/14851, Loss: 4.135320705245249e-05\n",
      "Batch 5875/14851, Loss: 0.0015187588287517428\n",
      "Batch 5876/14851, Loss: 0.019115572795271873\n",
      "Batch 5877/14851, Loss: 0.017363881692290306\n",
      "Batch 5878/14851, Loss: 0.04402682185173035\n",
      "Batch 5879/14851, Loss: 0.0061791869811713696\n",
      "Batch 5880/14851, Loss: 0.03658594936132431\n",
      "Batch 5881/14851, Loss: 0.021463746204972267\n",
      "Batch 5882/14851, Loss: 0.01730596274137497\n",
      "Batch 5883/14851, Loss: 0.004594884347170591\n",
      "Batch 5884/14851, Loss: 0.0051715439185500145\n",
      "Batch 5885/14851, Loss: 0.05324742570519447\n",
      "Batch 5886/14851, Loss: 0.0002983721497002989\n",
      "Batch 5887/14851, Loss: 0.0012786127626895905\n",
      "Batch 5888/14851, Loss: 0.005049488041549921\n",
      "Batch 5889/14851, Loss: 4.3585896492004395e-06\n",
      "Batch 5890/14851, Loss: 0.017586423084139824\n",
      "Batch 5891/14851, Loss: 0.00029123201966285706\n",
      "Batch 5892/14851, Loss: 0.00164775678422302\n",
      "Batch 5893/14851, Loss: 0.000754447013605386\n",
      "Batch 5894/14851, Loss: 7.67571254982613e-05\n",
      "Batch 5895/14851, Loss: 0.0014611209044232965\n",
      "Batch 5896/14851, Loss: 0.019725272431969643\n",
      "Batch 5897/14851, Loss: 0.0030384014826267958\n",
      "Batch 5898/14851, Loss: 0.011134451255202293\n",
      "Batch 5899/14851, Loss: 0.004502215888351202\n",
      "Batch 5900/14851, Loss: 0.007319292984902859\n",
      "Batch 5901/14851, Loss: 0.05117747560143471\n",
      "Batch 5902/14851, Loss: 0.0004958324134349823\n",
      "Batch 5903/14851, Loss: 0.002567742485553026\n",
      "Batch 5904/14851, Loss: 4.9432117521064356e-05\n",
      "Batch 5905/14851, Loss: 7.881969213485718e-05\n",
      "Batch 5906/14851, Loss: 0.022231893613934517\n",
      "Batch 5907/14851, Loss: 0.0025032800622284412\n",
      "Batch 5908/14851, Loss: 0.02242758870124817\n",
      "Batch 5909/14851, Loss: 0.025579452514648438\n",
      "Batch 5910/14851, Loss: 0.01171930506825447\n",
      "Batch 5911/14851, Loss: 0.013699653558433056\n",
      "Batch 5912/14851, Loss: 0.009448197670280933\n",
      "Batch 5913/14851, Loss: 0.02963211201131344\n",
      "Batch 5914/14851, Loss: 8.739282930037007e-05\n",
      "Batch 5915/14851, Loss: 0.01296190544962883\n",
      "Batch 5916/14851, Loss: 0.0004598411323968321\n",
      "Batch 5917/14851, Loss: 0.04300297051668167\n",
      "Batch 5918/14851, Loss: 0.04769544675946236\n",
      "Batch 5919/14851, Loss: 0.00020233790564816445\n",
      "Batch 5920/14851, Loss: 0.0004763007164001465\n",
      "Batch 5921/14851, Loss: 0.006019263062626123\n",
      "Batch 5922/14851, Loss: 0.014660323970019817\n",
      "Batch 5923/14851, Loss: 0.008345105685293674\n",
      "Batch 5924/14851, Loss: 0.0002861618995666504\n",
      "Batch 5925/14851, Loss: 0.00020290414977353066\n",
      "Batch 5926/14851, Loss: 0.005599099677056074\n",
      "Batch 5927/14851, Loss: 0.0033674018923193216\n",
      "Batch 5928/14851, Loss: 0.004509886261075735\n",
      "Batch 5929/14851, Loss: 0.0033807556610554457\n",
      "Batch 5930/14851, Loss: 0.05920024588704109\n",
      "Batch 5931/14851, Loss: 9.304657578468323e-05\n",
      "Batch 5932/14851, Loss: 0.021141158416867256\n",
      "Batch 5933/14851, Loss: 0.0059747882187366486\n",
      "Batch 5934/14851, Loss: 0.02869439870119095\n",
      "Batch 5935/14851, Loss: 0.04649901017546654\n",
      "Batch 5936/14851, Loss: 0.0359790064394474\n",
      "Batch 5937/14851, Loss: 0.0028453341219574213\n",
      "Batch 5938/14851, Loss: 0.01748691126704216\n",
      "Batch 5939/14851, Loss: 3.3685315429465845e-05\n",
      "Batch 5940/14851, Loss: 0.023419005796313286\n",
      "Batch 5941/14851, Loss: 0.034023478627204895\n",
      "Batch 5942/14851, Loss: 0.0027029477059841156\n",
      "Batch 5943/14851, Loss: 0.0012962828623130918\n",
      "Batch 5944/14851, Loss: 0.04729783162474632\n",
      "Batch 5945/14851, Loss: 3.325939178466797e-05\n",
      "Batch 5946/14851, Loss: 0.007383896969258785\n",
      "Batch 5947/14851, Loss: 0.0007747486233711243\n",
      "Batch 5948/14851, Loss: 0.007290920242667198\n",
      "Batch 5949/14851, Loss: 0.030646253377199173\n",
      "Batch 5950/14851, Loss: 0.00527321919798851\n",
      "Batch 5951/14851, Loss: 0.00015826895833015442\n",
      "Batch 5952/14851, Loss: 0.005402971059083939\n",
      "Batch 5953/14851, Loss: 0.12954597175121307\n",
      "Batch 5954/14851, Loss: 0.0033150091767311096\n",
      "Batch 5955/14851, Loss: 0.0003423417510930449\n",
      "Batch 5956/14851, Loss: 0.004607588052749634\n",
      "Batch 5957/14851, Loss: 0.002427916508167982\n",
      "Batch 5958/14851, Loss: 0.008725166320800781\n",
      "Batch 5959/14851, Loss: 0.013560234569013119\n",
      "Batch 5960/14851, Loss: 0.03241743519902229\n",
      "Batch 5961/14851, Loss: 0.0072685168124735355\n",
      "Batch 5962/14851, Loss: 0.001443811459466815\n",
      "Batch 5963/14851, Loss: 0.00019074603915214539\n",
      "Batch 5964/14851, Loss: 0.02070949785411358\n",
      "Batch 5965/14851, Loss: 0.0015830689808353782\n",
      "Batch 5966/14851, Loss: 0.0005320602795109153\n",
      "Batch 5967/14851, Loss: 0.006892218720167875\n",
      "Batch 5968/14851, Loss: 7.40674659027718e-05\n",
      "Batch 5969/14851, Loss: 0.03585982695221901\n",
      "Batch 5970/14851, Loss: 1.7017126083374023e-05\n",
      "Batch 5971/14851, Loss: 0.0006048344075679779\n",
      "Batch 5972/14851, Loss: 0.0019279380794614553\n",
      "Batch 5973/14851, Loss: 0.010825742967426777\n",
      "Batch 5974/14851, Loss: 0.0008366890251636505\n",
      "Batch 5975/14851, Loss: 0.005033253226429224\n",
      "Batch 5976/14851, Loss: 0.0008949090843088925\n",
      "Batch 5977/14851, Loss: 0.019976655021309853\n",
      "Batch 5978/14851, Loss: 0.0034457247238606215\n",
      "Batch 5979/14851, Loss: 0.03677775338292122\n",
      "Batch 5980/14851, Loss: 0.002299634739756584\n",
      "Batch 5981/14851, Loss: 0.0014876002678647637\n",
      "Batch 5982/14851, Loss: 0.00047250837087631226\n",
      "Batch 5983/14851, Loss: 0.005427530501037836\n",
      "Batch 5984/14851, Loss: 0.0053652385249733925\n",
      "Batch 5985/14851, Loss: 0.0631028264760971\n",
      "Batch 5986/14851, Loss: 0.0017506182193756104\n",
      "Batch 5987/14851, Loss: 0.03476915508508682\n",
      "Batch 5988/14851, Loss: 0.009087523445487022\n",
      "Batch 5989/14851, Loss: 0.0077490173280239105\n",
      "Batch 5990/14851, Loss: 0.03641294315457344\n",
      "Batch 5991/14851, Loss: 0.012910961173474789\n",
      "Batch 5992/14851, Loss: 0.004335702396929264\n",
      "Batch 5993/14851, Loss: 0.0034362077713012695\n",
      "Batch 5994/14851, Loss: 0.008490285836160183\n",
      "Batch 5995/14851, Loss: 0.005937703885138035\n",
      "Batch 5996/14851, Loss: 0.022950652986764908\n",
      "Batch 5997/14851, Loss: 0.01813425123691559\n",
      "Batch 5998/14851, Loss: 0.003226306289434433\n",
      "Batch 5999/14851, Loss: 0.0017085945000872016\n",
      "Batch 6000/14851, Loss: 0.010589283891022205\n",
      "Batch 6001/14851, Loss: 0.005126906093209982\n",
      "Batch 6002/14851, Loss: 0.00014122575521469116\n",
      "Batch 6003/14851, Loss: 0.007471685763448477\n",
      "Batch 6004/14851, Loss: 0.07676014304161072\n",
      "Batch 6005/14851, Loss: 0.003916039131581783\n",
      "Batch 6006/14851, Loss: 0.0008839641814120114\n",
      "Batch 6007/14851, Loss: 0.024664292111992836\n",
      "Batch 6008/14851, Loss: 0.0058674439787864685\n",
      "Batch 6009/14851, Loss: 0.00048629692173562944\n",
      "Batch 6010/14851, Loss: 0.027190178632736206\n",
      "Batch 6011/14851, Loss: 0.039092034101486206\n",
      "Batch 6012/14851, Loss: 0.012642588466405869\n",
      "Batch 6013/14851, Loss: 0.009097707457840443\n",
      "Batch 6014/14851, Loss: 0.00038028735434636474\n",
      "Batch 6015/14851, Loss: 0.031101999804377556\n",
      "Batch 6016/14851, Loss: 0.003250859444960952\n",
      "Batch 6017/14851, Loss: 0.035312965512275696\n",
      "Batch 6018/14851, Loss: 0.024374933913350105\n",
      "Batch 6019/14851, Loss: 0.0022359180729836226\n",
      "Batch 6020/14851, Loss: 0.006992664188146591\n",
      "Batch 6021/14851, Loss: 0.0016750629292801023\n",
      "Batch 6022/14851, Loss: 0.004910808056592941\n",
      "Batch 6023/14851, Loss: 0.03889075294137001\n",
      "Batch 6024/14851, Loss: 0.0071155838668346405\n",
      "Batch 6025/14851, Loss: 0.006381777580827475\n",
      "Batch 6026/14851, Loss: 0.005036613438278437\n",
      "Batch 6027/14851, Loss: 0.006455627270042896\n",
      "Batch 6028/14851, Loss: 0.0032707329373806715\n",
      "Batch 6029/14851, Loss: 0.007406937424093485\n",
      "Batch 6030/14851, Loss: 0.020704301074147224\n",
      "Batch 6031/14851, Loss: 0.0937982052564621\n",
      "Batch 6032/14851, Loss: 0.0004334623517934233\n",
      "Batch 6033/14851, Loss: 0.06134360656142235\n",
      "Batch 6034/14851, Loss: 0.02870393544435501\n",
      "Batch 6035/14851, Loss: 0.0045611923560500145\n",
      "Batch 6036/14851, Loss: 0.02228238806128502\n",
      "Batch 6037/14851, Loss: 0.0031946105882525444\n",
      "Batch 6038/14851, Loss: 0.0013045415980741382\n",
      "Batch 6039/14851, Loss: 0.003238540142774582\n",
      "Batch 6040/14851, Loss: 0.006588246673345566\n",
      "Batch 6041/14851, Loss: 0.015194621868431568\n",
      "Batch 6042/14851, Loss: 0.00483490526676178\n",
      "Batch 6043/14851, Loss: 0.03125297278165817\n",
      "Batch 6044/14851, Loss: 0.00022619466471951455\n",
      "Batch 6045/14851, Loss: 0.008386288769543171\n",
      "Batch 6046/14851, Loss: 0.0032475111074745655\n",
      "Batch 6047/14851, Loss: 0.006372422911226749\n",
      "Batch 6048/14851, Loss: 0.002557027619332075\n",
      "Batch 6049/14851, Loss: 0.0009333689813502133\n",
      "Batch 6050/14851, Loss: 0.0014175636461004615\n",
      "Batch 6051/14851, Loss: 7.640694821020588e-05\n",
      "Batch 6052/14851, Loss: 0.004988620989024639\n",
      "Batch 6053/14851, Loss: 0.004112412687391043\n",
      "Batch 6054/14851, Loss: 0.002661011414602399\n",
      "Batch 6055/14851, Loss: 0.0010242993012070656\n",
      "Batch 6056/14851, Loss: 0.012878289446234703\n",
      "Batch 6057/14851, Loss: 0.003402062924578786\n",
      "Batch 6058/14851, Loss: 0.005434324499219656\n",
      "Batch 6059/14851, Loss: 0.0030559401493519545\n",
      "Batch 6060/14851, Loss: 0.0003692507743835449\n",
      "Batch 6061/14851, Loss: 0.02680177427828312\n",
      "Batch 6062/14851, Loss: 0.025239406153559685\n",
      "Batch 6063/14851, Loss: 0.029894564300775528\n",
      "Batch 6064/14851, Loss: 0.0006084417109377682\n",
      "Batch 6065/14851, Loss: 0.0048149763606488705\n",
      "Batch 6066/14851, Loss: 0.03943046182394028\n",
      "Batch 6067/14851, Loss: 0.0009793887147679925\n",
      "Batch 6068/14851, Loss: 0.0008754879236221313\n",
      "Batch 6069/14851, Loss: 0.00406847195699811\n",
      "Batch 6070/14851, Loss: 0.00014129777264315635\n",
      "Batch 6071/14851, Loss: 0.00017290766118094325\n",
      "Batch 6072/14851, Loss: 0.00010291983926435933\n",
      "Batch 6073/14851, Loss: 0.0010781934252008796\n",
      "Batch 6074/14851, Loss: 0.0014746139058843255\n",
      "Batch 6075/14851, Loss: 0.010223179124295712\n",
      "Batch 6076/14851, Loss: 0.010648500174283981\n",
      "Batch 6077/14851, Loss: 0.0019046379020437598\n",
      "Batch 6078/14851, Loss: 0.03606046736240387\n",
      "Batch 6079/14851, Loss: 0.02284855581820011\n",
      "Batch 6080/14851, Loss: 0.027976851910352707\n",
      "Batch 6081/14851, Loss: 0.035627804696559906\n",
      "Batch 6082/14851, Loss: 6.2212347984313965e-06\n",
      "Batch 6083/14851, Loss: 0.000999895273707807\n",
      "Batch 6084/14851, Loss: 0.01727914810180664\n",
      "Batch 6085/14851, Loss: 0.00959575455635786\n",
      "Batch 6086/14851, Loss: 0.0016874531283974648\n",
      "Batch 6087/14851, Loss: 0.007739707361906767\n",
      "Batch 6088/14851, Loss: 0.0007635615766048431\n",
      "Batch 6089/14851, Loss: 0.0011970378691330552\n",
      "Batch 6090/14851, Loss: 0.006647821981459856\n",
      "Batch 6091/14851, Loss: 0.0010339532746002078\n",
      "Batch 6092/14851, Loss: 0.005036538932472467\n",
      "Batch 6093/14851, Loss: 0.007787469774484634\n",
      "Batch 6094/14851, Loss: 0.024049930274486542\n",
      "Batch 6095/14851, Loss: 0.004130907356739044\n",
      "Batch 6096/14851, Loss: 0.004042822867631912\n",
      "Batch 6097/14851, Loss: 0.046832963824272156\n",
      "Batch 6098/14851, Loss: 4.980216544936411e-05\n",
      "Batch 6099/14851, Loss: 0.01760660670697689\n",
      "Batch 6100/14851, Loss: 0.06153876706957817\n",
      "Batch 6101/14851, Loss: 0.00269641843624413\n",
      "Batch 6102/14851, Loss: 0.02553611621260643\n",
      "Batch 6103/14851, Loss: 0.0027471939101815224\n",
      "Batch 6104/14851, Loss: 0.009194690734148026\n",
      "Batch 6105/14851, Loss: 0.004086409229785204\n",
      "Batch 6106/14851, Loss: 0.05778641998767853\n",
      "Batch 6107/14851, Loss: 0.013530462980270386\n",
      "Batch 6108/14851, Loss: 0.02023092657327652\n",
      "Batch 6109/14851, Loss: 0.0365336611866951\n",
      "Batch 6110/14851, Loss: 0.0007127225399017334\n",
      "Batch 6111/14851, Loss: 0.0010934716556221247\n",
      "Batch 6112/14851, Loss: 7.969265425344929e-05\n",
      "Batch 6113/14851, Loss: 0.0011230272939428687\n",
      "Batch 6114/14851, Loss: 0.0018813697388395667\n",
      "Batch 6115/14851, Loss: 0.01523536629974842\n",
      "Batch 6116/14851, Loss: 0.0032007533591240644\n",
      "Batch 6117/14851, Loss: 0.007592915557324886\n",
      "Batch 6118/14851, Loss: 0.00010167434811592102\n",
      "Batch 6119/14851, Loss: 0.0027802959084510803\n",
      "Batch 6120/14851, Loss: 0.0018537044525146484\n",
      "Batch 6121/14851, Loss: 8.454421913484111e-05\n",
      "Batch 6122/14851, Loss: 0.04164943844079971\n",
      "Batch 6123/14851, Loss: 0.008008459582924843\n",
      "Batch 6124/14851, Loss: 0.025831134989857674\n",
      "Batch 6125/14851, Loss: 0.0050842976197600365\n",
      "Batch 6126/14851, Loss: 0.006487180944532156\n",
      "Batch 6127/14851, Loss: 0.007339049130678177\n",
      "Batch 6128/14851, Loss: 0.02600056678056717\n",
      "Batch 6129/14851, Loss: 0.0003868991625495255\n",
      "Batch 6130/14851, Loss: 0.0018541737226769328\n",
      "Batch 6131/14851, Loss: 0.018104951828718185\n",
      "Batch 6132/14851, Loss: 0.004106488544493914\n",
      "Batch 6133/14851, Loss: 0.013380705378949642\n",
      "Batch 6134/14851, Loss: 0.0001466311514377594\n",
      "Batch 6135/14851, Loss: 0.005789761897176504\n",
      "Batch 6136/14851, Loss: 0.0003315421345178038\n",
      "Batch 6137/14851, Loss: 0.01666492596268654\n",
      "Batch 6138/14851, Loss: 0.0018811710178852081\n",
      "Batch 6139/14851, Loss: 0.0014742413768544793\n",
      "Batch 6140/14851, Loss: 0.017121244221925735\n",
      "Batch 6141/14851, Loss: 0.0029625953175127506\n",
      "Batch 6142/14851, Loss: 0.0014740166952833533\n",
      "Batch 6143/14851, Loss: 0.002524371026083827\n",
      "Batch 6144/14851, Loss: 0.0009237080812454224\n",
      "Batch 6145/14851, Loss: 0.0005429436569102108\n",
      "Batch 6146/14851, Loss: 0.004567374940961599\n",
      "Batch 6147/14851, Loss: 0.008684239350259304\n",
      "Batch 6148/14851, Loss: 0.0030969639774411917\n",
      "Batch 6149/14851, Loss: 0.01569521240890026\n",
      "Batch 6150/14851, Loss: 0.004858541302382946\n",
      "Batch 6151/14851, Loss: 0.015251521952450275\n",
      "Batch 6152/14851, Loss: 0.0008968785405158997\n",
      "Batch 6153/14851, Loss: 0.003358313348144293\n",
      "Batch 6154/14851, Loss: 0.037185754626989365\n",
      "Batch 6155/14851, Loss: 0.0021333820186555386\n",
      "Batch 6156/14851, Loss: 0.09847142547369003\n",
      "Batch 6157/14851, Loss: 0.011282580904662609\n",
      "Batch 6158/14851, Loss: 0.004842618014663458\n",
      "Batch 6159/14851, Loss: 0.008247864432632923\n",
      "Batch 6160/14851, Loss: 0.027741139754652977\n",
      "Batch 6161/14851, Loss: 0.0011951402993872762\n",
      "Batch 6162/14851, Loss: 0.012217811308801174\n",
      "Batch 6163/14851, Loss: 0.00362372281961143\n",
      "Batch 6164/14851, Loss: 0.041532646864652634\n",
      "Batch 6165/14851, Loss: 0.02104519121348858\n",
      "Batch 6166/14851, Loss: 0.07300461828708649\n",
      "Batch 6167/14851, Loss: 0.03203844651579857\n",
      "Batch 6168/14851, Loss: 0.0037603508681058884\n",
      "Batch 6169/14851, Loss: 0.0007773935794830322\n",
      "Batch 6170/14851, Loss: 0.002660546451807022\n",
      "Batch 6171/14851, Loss: 0.0014500817051157355\n",
      "Batch 6172/14851, Loss: 0.0017552176723256707\n",
      "Batch 6173/14851, Loss: 0.021162403747439384\n",
      "Batch 6174/14851, Loss: 0.003954483661800623\n",
      "Batch 6175/14851, Loss: 0.00217778910882771\n",
      "Batch 6176/14851, Loss: 0.008997227996587753\n",
      "Batch 6177/14851, Loss: 0.021560166031122208\n",
      "Batch 6178/14851, Loss: 0.004375302232801914\n",
      "Batch 6179/14851, Loss: 0.005844303406774998\n",
      "Batch 6180/14851, Loss: 0.0053472500294446945\n",
      "Batch 6181/14851, Loss: 0.0019032604759559035\n",
      "Batch 6182/14851, Loss: 0.03993139788508415\n",
      "Batch 6183/14851, Loss: 0.0018058477435261011\n",
      "Batch 6184/14851, Loss: 0.011893533170223236\n",
      "Batch 6185/14851, Loss: 0.0024743329267948866\n",
      "Batch 6186/14851, Loss: 0.012864490039646626\n",
      "Batch 6187/14851, Loss: 0.010368768125772476\n",
      "Batch 6188/14851, Loss: 0.005028676241636276\n",
      "Batch 6189/14851, Loss: 0.005579764023423195\n",
      "Batch 6190/14851, Loss: 0.014902997761964798\n",
      "Batch 6191/14851, Loss: 0.07696101814508438\n",
      "Batch 6192/14851, Loss: 0.010771975852549076\n",
      "Batch 6193/14851, Loss: 0.015623102895915508\n",
      "Batch 6194/14851, Loss: 0.003256154479458928\n",
      "Batch 6195/14851, Loss: 0.004060342907905579\n",
      "Batch 6196/14851, Loss: 0.007157556712627411\n",
      "Batch 6197/14851, Loss: 0.0015110907843336463\n",
      "Batch 6198/14851, Loss: 0.0026820823550224304\n",
      "Batch 6199/14851, Loss: 0.003595055313780904\n",
      "Batch 6200/14851, Loss: 0.0011641308665275574\n",
      "Batch 6201/14851, Loss: 0.002192395506426692\n",
      "Batch 6202/14851, Loss: 0.02069668658077717\n",
      "Batch 6203/14851, Loss: 0.0017611657967790961\n",
      "Batch 6204/14851, Loss: 0.029990220442414284\n",
      "Batch 6205/14851, Loss: 0.0004516939225140959\n",
      "Batch 6206/14851, Loss: 0.0016802474856376648\n",
      "Batch 6207/14851, Loss: 0.12108165770769119\n",
      "Batch 6208/14851, Loss: 0.006136165466159582\n",
      "Batch 6209/14851, Loss: 0.0006224860553629696\n",
      "Batch 6210/14851, Loss: 0.014805481769144535\n",
      "Batch 6211/14851, Loss: 0.039291270077228546\n",
      "Batch 6212/14851, Loss: 0.0003667349519673735\n",
      "Batch 6213/14851, Loss: 3.590683263610117e-05\n",
      "Batch 6214/14851, Loss: 0.0006237886846065521\n",
      "Batch 6215/14851, Loss: 0.0027719782665371895\n",
      "Batch 6216/14851, Loss: 0.0011432009050622582\n",
      "Batch 6217/14851, Loss: 0.000474816799396649\n",
      "Batch 6218/14851, Loss: 0.017479930073022842\n",
      "Batch 6219/14851, Loss: 0.004791409242898226\n",
      "Batch 6220/14851, Loss: 0.0004172287881374359\n",
      "Batch 6221/14851, Loss: 0.0004278719425201416\n",
      "Batch 6222/14851, Loss: 0.00030519068241119385\n",
      "Batch 6223/14851, Loss: 0.06091015413403511\n",
      "Batch 6224/14851, Loss: 0.00034446766949258745\n",
      "Batch 6225/14851, Loss: 0.016047338023781776\n",
      "Batch 6226/14851, Loss: 0.03723043203353882\n",
      "Batch 6227/14851, Loss: 0.024468544870615005\n",
      "Batch 6228/14851, Loss: 0.0007020508055575192\n",
      "Batch 6229/14851, Loss: 0.006223130039870739\n",
      "Batch 6230/14851, Loss: 0.034835755825042725\n",
      "Batch 6231/14851, Loss: 0.02657758817076683\n",
      "Batch 6232/14851, Loss: 0.0014302829513326287\n",
      "Batch 6233/14851, Loss: 0.04723026975989342\n",
      "Batch 6234/14851, Loss: 0.019140228629112244\n",
      "Batch 6235/14851, Loss: 0.010890413075685501\n",
      "Batch 6236/14851, Loss: 0.006320774555206299\n",
      "Batch 6237/14851, Loss: 0.009248320944607258\n",
      "Batch 6238/14851, Loss: 0.0024594913702458143\n",
      "Batch 6239/14851, Loss: 0.03248016908764839\n",
      "Batch 6240/14851, Loss: 0.007063725031912327\n",
      "Batch 6241/14851, Loss: 0.018045898526906967\n",
      "Batch 6242/14851, Loss: 0.05327219516038895\n",
      "Batch 6243/14851, Loss: 0.015114941634237766\n",
      "Batch 6244/14851, Loss: 0.0026123858988285065\n",
      "Batch 6245/14851, Loss: 0.05617983266711235\n",
      "Batch 6246/14851, Loss: 0.023197824135422707\n",
      "Batch 6247/14851, Loss: 0.056358348578214645\n",
      "Batch 6248/14851, Loss: 0.016575491055846214\n",
      "Batch 6249/14851, Loss: 0.013004157692193985\n",
      "Batch 6250/14851, Loss: 0.012867012061178684\n",
      "Batch 6251/14851, Loss: 0.049115754663944244\n",
      "Batch 6252/14851, Loss: 0.0005139186978340149\n",
      "Batch 6253/14851, Loss: 0.012201583944261074\n",
      "Batch 6254/14851, Loss: 0.0317033976316452\n",
      "Batch 6255/14851, Loss: 0.03746578097343445\n",
      "Batch 6256/14851, Loss: 0.05441378802061081\n",
      "Batch 6257/14851, Loss: 0.0005337893962860107\n",
      "Batch 6258/14851, Loss: 0.028320660814642906\n",
      "Batch 6259/14851, Loss: 0.013201209716498852\n",
      "Batch 6260/14851, Loss: 0.0077013373374938965\n",
      "Batch 6261/14851, Loss: 0.019666709005832672\n",
      "Batch 6262/14851, Loss: 0.020804457366466522\n",
      "Batch 6263/14851, Loss: 0.0008157069678418338\n",
      "Batch 6264/14851, Loss: 0.0030554435215890408\n",
      "Batch 6265/14851, Loss: 0.017047561705112457\n",
      "Batch 6266/14851, Loss: 0.04893533140420914\n",
      "Batch 6267/14851, Loss: 0.017220258712768555\n",
      "Batch 6268/14851, Loss: 0.03547128662467003\n",
      "Batch 6269/14851, Loss: 0.04615664482116699\n",
      "Batch 6270/14851, Loss: 3.002708217536565e-05\n",
      "Batch 6271/14851, Loss: 0.002378549426794052\n",
      "Batch 6272/14851, Loss: 0.07322513312101364\n",
      "Batch 6273/14851, Loss: 0.01812952198088169\n",
      "Batch 6274/14851, Loss: 0.004821398761123419\n",
      "Batch 6275/14851, Loss: 0.006055209320038557\n",
      "Batch 6276/14851, Loss: 0.015657557174563408\n",
      "Batch 6277/14851, Loss: 0.05723018944263458\n",
      "Batch 6278/14851, Loss: 0.02919946052134037\n",
      "Batch 6279/14851, Loss: 0.011333172209560871\n",
      "Batch 6280/14851, Loss: 0.0017749617109075189\n",
      "Batch 6281/14851, Loss: 0.03617321699857712\n",
      "Batch 6282/14851, Loss: 0.09550663828849792\n",
      "Batch 6283/14851, Loss: 0.047700464725494385\n",
      "Batch 6284/14851, Loss: 0.04235866665840149\n",
      "Batch 6285/14851, Loss: 0.006941011641174555\n",
      "Batch 6286/14851, Loss: 0.02988641895353794\n",
      "Batch 6287/14851, Loss: 0.07655666768550873\n",
      "Batch 6288/14851, Loss: 0.05073685944080353\n",
      "Batch 6289/14851, Loss: 0.03342542424798012\n",
      "Batch 6290/14851, Loss: 0.004530551377683878\n",
      "Batch 6291/14851, Loss: 0.006228043232113123\n",
      "Batch 6292/14851, Loss: 0.05003495141863823\n",
      "Batch 6293/14851, Loss: 0.053268492221832275\n",
      "Batch 6294/14851, Loss: 0.00856963824480772\n",
      "Batch 6295/14851, Loss: 0.05139968916773796\n",
      "Batch 6296/14851, Loss: 0.05608593672513962\n",
      "Batch 6297/14851, Loss: 0.043246835470199585\n",
      "Batch 6298/14851, Loss: 0.012438350357115269\n",
      "Batch 6299/14851, Loss: 0.015267201699316502\n",
      "Batch 6300/14851, Loss: 0.07947071641683578\n",
      "Batch 6301/14851, Loss: 0.01813100464642048\n",
      "Batch 6302/14851, Loss: 0.02130044251680374\n",
      "Batch 6303/14851, Loss: 0.10678469389677048\n",
      "Batch 6304/14851, Loss: 0.007449571043252945\n",
      "Batch 6305/14851, Loss: 0.058706190437078476\n",
      "Batch 6306/14851, Loss: 0.09151995182037354\n",
      "Batch 6307/14851, Loss: 0.011302248574793339\n",
      "Batch 6308/14851, Loss: 0.005807650741189718\n",
      "Batch 6309/14851, Loss: 0.00809656921774149\n",
      "Batch 6310/14851, Loss: 0.029913464561104774\n",
      "Batch 6311/14851, Loss: 0.020245788618922234\n",
      "Batch 6312/14851, Loss: 0.011237637139856815\n",
      "Batch 6313/14851, Loss: 0.015856271609663963\n",
      "Batch 6314/14851, Loss: 0.012794016860425472\n",
      "Batch 6315/14851, Loss: 0.004766989499330521\n",
      "Batch 6316/14851, Loss: 0.02864156849682331\n",
      "Batch 6317/14851, Loss: 0.020398704335093498\n",
      "Batch 6318/14851, Loss: 0.0027540239971131086\n",
      "Batch 6319/14851, Loss: 0.007183366920799017\n",
      "Batch 6320/14851, Loss: 0.03569399565458298\n",
      "Batch 6321/14851, Loss: 0.012129011563956738\n",
      "Batch 6322/14851, Loss: 0.004284057300537825\n",
      "Batch 6323/14851, Loss: 0.09058915823698044\n",
      "Batch 6324/14851, Loss: 0.01542243268340826\n",
      "Batch 6325/14851, Loss: 0.02479822002351284\n",
      "Batch 6326/14851, Loss: 0.046246033161878586\n",
      "Batch 6327/14851, Loss: 0.005540804471820593\n",
      "Batch 6328/14851, Loss: 0.1619928479194641\n",
      "Batch 6329/14851, Loss: 0.004388732369989157\n",
      "Batch 6330/14851, Loss: 0.017534099519252777\n",
      "Batch 6331/14851, Loss: 0.0930413156747818\n",
      "Batch 6332/14851, Loss: 0.005098587833344936\n",
      "Batch 6333/14851, Loss: 0.027409125119447708\n",
      "Batch 6334/14851, Loss: 0.027915464714169502\n",
      "Batch 6335/14851, Loss: 0.00346992164850235\n",
      "Batch 6336/14851, Loss: 0.024713795632123947\n",
      "Batch 6337/14851, Loss: 0.061558425426483154\n",
      "Batch 6338/14851, Loss: 0.033122945576906204\n",
      "Batch 6339/14851, Loss: 0.003714208723977208\n",
      "Batch 6340/14851, Loss: 0.000888743728864938\n",
      "Batch 6341/14851, Loss: 0.017292527481913567\n",
      "Batch 6342/14851, Loss: 0.018089445307850838\n",
      "Batch 6343/14851, Loss: 0.019392048940062523\n",
      "Batch 6344/14851, Loss: 0.03345772624015808\n",
      "Batch 6345/14851, Loss: 0.007096712943166494\n",
      "Batch 6346/14851, Loss: 0.0006312566692940891\n",
      "Batch 6347/14851, Loss: 0.0003052813408430666\n",
      "Batch 6348/14851, Loss: 0.025562269613146782\n",
      "Batch 6349/14851, Loss: 0.0031221683602780104\n",
      "Batch 6350/14851, Loss: 0.006454674992710352\n",
      "Batch 6351/14851, Loss: 0.0007547747227363288\n",
      "Batch 6352/14851, Loss: 0.0012340868124738336\n",
      "Batch 6353/14851, Loss: 0.0024651612620800734\n",
      "Batch 6354/14851, Loss: 0.006782160606235266\n",
      "Batch 6355/14851, Loss: 0.0448148250579834\n",
      "Batch 6356/14851, Loss: 0.0010518021881580353\n",
      "Batch 6357/14851, Loss: 0.003193237353116274\n",
      "Batch 6358/14851, Loss: 0.002251807600259781\n",
      "Batch 6359/14851, Loss: 0.005757524166256189\n",
      "Batch 6360/14851, Loss: 0.0009917430579662323\n",
      "Batch 6361/14851, Loss: 9.68774184002541e-05\n",
      "Batch 6362/14851, Loss: 0.07949970662593842\n",
      "Batch 6363/14851, Loss: 0.0009198921616189182\n",
      "Batch 6364/14851, Loss: 0.056055232882499695\n",
      "Batch 6365/14851, Loss: 0.011396117508411407\n",
      "Batch 6366/14851, Loss: 0.012661884538829327\n",
      "Batch 6367/14851, Loss: 0.0008758926996961236\n",
      "Batch 6368/14851, Loss: 0.00778351491317153\n",
      "Batch 6369/14851, Loss: 0.030400313436985016\n",
      "Batch 6370/14851, Loss: 0.03325733542442322\n",
      "Batch 6371/14851, Loss: 0.004293649923056364\n",
      "Batch 6372/14851, Loss: 0.03532973304390907\n",
      "Batch 6373/14851, Loss: 0.14934562146663666\n",
      "Batch 6374/14851, Loss: 0.0031794810201972723\n",
      "Batch 6375/14851, Loss: 0.03839066997170448\n",
      "Batch 6376/14851, Loss: 0.010991886258125305\n",
      "Batch 6377/14851, Loss: 0.025919517502188683\n",
      "Batch 6378/14851, Loss: 0.024745285511016846\n",
      "Batch 6379/14851, Loss: 0.0004956300253979862\n",
      "Batch 6380/14851, Loss: 0.0005314822192303836\n",
      "Batch 6381/14851, Loss: 0.0006053385441191494\n",
      "Batch 6382/14851, Loss: 0.004917384125292301\n",
      "Batch 6383/14851, Loss: 0.0051009259186685085\n",
      "Batch 6384/14851, Loss: 0.004616993013769388\n",
      "Batch 6385/14851, Loss: 0.04854142293334007\n",
      "Batch 6386/14851, Loss: 0.03478871285915375\n",
      "Batch 6387/14851, Loss: 0.03262341767549515\n",
      "Batch 6388/14851, Loss: 0.004019101615995169\n",
      "Batch 6389/14851, Loss: 0.02959805354475975\n",
      "Batch 6390/14851, Loss: 0.074558325111866\n",
      "Batch 6391/14851, Loss: 0.010528945364058018\n",
      "Batch 6392/14851, Loss: 0.0303358044475317\n",
      "Batch 6393/14851, Loss: 0.0922471210360527\n",
      "Batch 6394/14851, Loss: 0.0046143257059156895\n",
      "Batch 6395/14851, Loss: 0.008875228464603424\n",
      "Batch 6396/14851, Loss: 0.004796538036316633\n",
      "Batch 6397/14851, Loss: 0.021159440279006958\n",
      "Batch 6398/14851, Loss: 0.0006931808311492205\n",
      "Batch 6399/14851, Loss: 0.0023029905278235674\n",
      "Batch 6400/14851, Loss: 0.0046553597785532475\n",
      "Batch 6401/14851, Loss: 0.014554720371961594\n",
      "Batch 6402/14851, Loss: 0.017152609303593636\n",
      "Batch 6403/14851, Loss: 0.019980382174253464\n",
      "Batch 6404/14851, Loss: 0.010377257131040096\n",
      "Batch 6405/14851, Loss: 0.00033616149448789656\n",
      "Batch 6406/14851, Loss: 0.000524565577507019\n",
      "Batch 6407/14851, Loss: 0.03177480027079582\n",
      "Batch 6408/14851, Loss: 0.005239576566964388\n",
      "Batch 6409/14851, Loss: 0.03645402938127518\n",
      "Batch 6410/14851, Loss: 0.02006697840988636\n",
      "Batch 6411/14851, Loss: 0.0733460858464241\n",
      "Batch 6412/14851, Loss: 0.0031422663014382124\n",
      "Batch 6413/14851, Loss: 0.0148318475112319\n",
      "Batch 6414/14851, Loss: 0.0001467280089855194\n",
      "Batch 6415/14851, Loss: 0.0004783397016581148\n",
      "Batch 6416/14851, Loss: 0.0025424398481845856\n",
      "Batch 6417/14851, Loss: 0.0005909775500185788\n",
      "Batch 6418/14851, Loss: 0.00041277954005636275\n",
      "Batch 6419/14851, Loss: 0.01760183833539486\n",
      "Batch 6420/14851, Loss: 0.007965434342622757\n",
      "Batch 6421/14851, Loss: 0.011569440364837646\n",
      "Batch 6422/14851, Loss: 0.003276187926530838\n",
      "Batch 6423/14851, Loss: 0.009401530027389526\n",
      "Batch 6424/14851, Loss: 0.029260877519845963\n",
      "Batch 6425/14851, Loss: 0.07508393377065659\n",
      "Batch 6426/14851, Loss: 0.0025758217088878155\n",
      "Batch 6427/14851, Loss: 4.1664887248771265e-05\n",
      "Batch 6428/14851, Loss: 0.030799685046076775\n",
      "Batch 6429/14851, Loss: 0.00346981268376112\n",
      "Batch 6430/14851, Loss: 0.04386851191520691\n",
      "Batch 6431/14851, Loss: 0.03143077343702316\n",
      "Batch 6432/14851, Loss: 0.0018778691301122308\n",
      "Batch 6433/14851, Loss: 0.00035081832902505994\n",
      "Batch 6434/14851, Loss: 0.0035716083366423845\n",
      "Batch 6435/14851, Loss: 0.0005519191618077457\n",
      "Batch 6436/14851, Loss: 0.0018152246484532952\n",
      "Batch 6437/14851, Loss: 0.032844770699739456\n",
      "Batch 6438/14851, Loss: 0.03735841065645218\n",
      "Batch 6439/14851, Loss: 0.017590293660759926\n",
      "Batch 6440/14851, Loss: 0.05132623761892319\n",
      "Batch 6441/14851, Loss: 0.01115854736417532\n",
      "Batch 6442/14851, Loss: 0.01707579381763935\n",
      "Batch 6443/14851, Loss: 0.0019356422126293182\n",
      "Batch 6444/14851, Loss: 0.00012025485193589702\n",
      "Batch 6445/14851, Loss: 0.0053117964416742325\n",
      "Batch 6446/14851, Loss: 0.0004660598933696747\n",
      "Batch 6447/14851, Loss: 0.003939882852137089\n",
      "Batch 6448/14851, Loss: 0.020520249381661415\n",
      "Batch 6449/14851, Loss: 0.001218061544932425\n",
      "Batch 6450/14851, Loss: 0.0005898814415559173\n",
      "Batch 6451/14851, Loss: 0.0032038253266364336\n",
      "Batch 6452/14851, Loss: 0.019015008583664894\n",
      "Batch 6453/14851, Loss: 0.11740248650312424\n",
      "Batch 6454/14851, Loss: 0.0054579610005021095\n",
      "Batch 6455/14851, Loss: 0.00031088589457795024\n",
      "Batch 6456/14851, Loss: 0.022287849336862564\n",
      "Batch 6457/14851, Loss: 0.00031064325594343245\n",
      "Batch 6458/14851, Loss: 0.0009026117622852325\n",
      "Batch 6459/14851, Loss: 0.01234630960971117\n",
      "Batch 6460/14851, Loss: 0.007736027240753174\n",
      "Batch 6461/14851, Loss: 0.02021397463977337\n",
      "Batch 6462/14851, Loss: 0.00396916875615716\n",
      "Batch 6463/14851, Loss: 0.020446084439754486\n",
      "Batch 6464/14851, Loss: 0.004565918352454901\n",
      "Batch 6465/14851, Loss: 0.008567401207983494\n",
      "Batch 6466/14851, Loss: 0.014353464357554913\n",
      "Batch 6467/14851, Loss: 0.13919740915298462\n",
      "Batch 6468/14851, Loss: 0.01115767378360033\n",
      "Batch 6469/14851, Loss: 0.0009937187423929572\n",
      "Batch 6470/14851, Loss: 0.04546962305903435\n",
      "Batch 6471/14851, Loss: 0.01675810106098652\n",
      "Batch 6472/14851, Loss: 0.0028160386718809605\n",
      "Batch 6473/14851, Loss: 0.029605284333229065\n",
      "Batch 6474/14851, Loss: 0.003488923190161586\n",
      "Batch 6475/14851, Loss: 0.02022838592529297\n",
      "Batch 6476/14851, Loss: 0.009486760012805462\n",
      "Batch 6477/14851, Loss: 0.0022113241720944643\n",
      "Batch 6478/14851, Loss: 0.0001327681093243882\n",
      "Batch 6479/14851, Loss: 0.005881751421838999\n",
      "Batch 6480/14851, Loss: 0.012412376701831818\n",
      "Batch 6481/14851, Loss: 0.0007011989946477115\n",
      "Batch 6482/14851, Loss: 0.0024483571760356426\n",
      "Batch 6483/14851, Loss: 0.02445671707391739\n",
      "Batch 6484/14851, Loss: 0.009369445964694023\n",
      "Batch 6485/14851, Loss: 0.004218640271574259\n",
      "Batch 6486/14851, Loss: 0.008626066148281097\n",
      "Batch 6487/14851, Loss: 0.003696791362017393\n",
      "Batch 6488/14851, Loss: 0.14502233266830444\n",
      "Batch 6489/14851, Loss: 0.001782137667760253\n",
      "Batch 6490/14851, Loss: 0.00041556605719961226\n",
      "Batch 6491/14851, Loss: 0.02863570675253868\n",
      "Batch 6492/14851, Loss: 0.007642053067684174\n",
      "Batch 6493/14851, Loss: 0.0248709749430418\n",
      "Batch 6494/14851, Loss: 0.004820001311600208\n",
      "Batch 6495/14851, Loss: 0.000504491210449487\n",
      "Batch 6496/14851, Loss: 0.009571711532771587\n",
      "Batch 6497/14851, Loss: 0.001989441690966487\n",
      "Batch 6498/14851, Loss: 0.00018942604947369546\n",
      "Batch 6499/14851, Loss: 0.0017087472369894385\n",
      "Batch 6500/14851, Loss: 0.07951284199953079\n",
      "Batch 6501/14851, Loss: 0.03821157291531563\n",
      "Batch 6502/14851, Loss: 0.003722894936800003\n",
      "Batch 6503/14851, Loss: 0.028226934373378754\n",
      "Batch 6504/14851, Loss: 0.0011374627938494086\n",
      "Batch 6505/14851, Loss: 0.025177722796797752\n",
      "Skipping batch 6506 due to NaN loss\n",
      "Batch 6507/14851, Loss: nan\n",
      "Batch 6508/14851, Loss: 0.007830193266272545\n",
      "Batch 6509/14851, Loss: 0.006960859056562185\n",
      "Batch 6510/14851, Loss: 0.05885843187570572\n",
      "Batch 6511/14851, Loss: 0.021750373765826225\n",
      "Batch 6512/14851, Loss: 0.017106911167502403\n",
      "Batch 6513/14851, Loss: 0.03825553506612778\n",
      "Batch 6514/14851, Loss: 0.00016026322555262595\n",
      "Batch 6515/14851, Loss: 0.012237370014190674\n",
      "Batch 6516/14851, Loss: 0.0037656810600310564\n",
      "Batch 6517/14851, Loss: 0.0015684539685025811\n",
      "Batch 6518/14851, Loss: 0.01065878476947546\n",
      "Batch 6519/14851, Loss: 0.041739773005247116\n",
      "Batch 6520/14851, Loss: 0.030166860669851303\n",
      "Batch 6521/14851, Loss: 0.0008603595197200775\n",
      "Batch 6522/14851, Loss: 0.03908023610711098\n",
      "Batch 6523/14851, Loss: 0.0016042751958593726\n",
      "Batch 6524/14851, Loss: 0.008604436181485653\n",
      "Batch 6525/14851, Loss: 0.002942025661468506\n",
      "Batch 6526/14851, Loss: 0.07868358492851257\n",
      "Batch 6527/14851, Loss: 0.0006099281017668545\n",
      "Batch 6528/14851, Loss: 0.003262527287006378\n",
      "Batch 6529/14851, Loss: 0.03483879193663597\n",
      "Batch 6530/14851, Loss: 0.03087802417576313\n",
      "Batch 6531/14851, Loss: 0.014034549705684185\n",
      "Batch 6532/14851, Loss: 0.021476373076438904\n",
      "Batch 6533/14851, Loss: 0.01816553995013237\n",
      "Batch 6534/14851, Loss: 0.02495754510164261\n",
      "Batch 6535/14851, Loss: 0.000986631726846099\n",
      "Batch 6536/14851, Loss: 0.006862331181764603\n",
      "Batch 6537/14851, Loss: 0.0005329921841621399\n",
      "Batch 6538/14851, Loss: 0.019923135638237\n",
      "Batch 6539/14851, Loss: 0.018556544557213783\n",
      "Batch 6540/14851, Loss: 0.004334092140197754\n",
      "Batch 6541/14851, Loss: 0.005001586861908436\n",
      "Batch 6542/14851, Loss: 0.004895426332950592\n",
      "Batch 6543/14851, Loss: 0.01995864138007164\n",
      "Batch 6544/14851, Loss: 0.0353853665292263\n",
      "Batch 6545/14851, Loss: 0.0055258250795304775\n",
      "Batch 6546/14851, Loss: 0.002151449443772435\n",
      "Batch 6547/14851, Loss: 0.00013865530490875244\n",
      "Batch 6548/14851, Loss: 0.007722808048129082\n",
      "Batch 6549/14851, Loss: 0.0013650605687871575\n",
      "Batch 6550/14851, Loss: 0.0005631707608699799\n",
      "Batch 6551/14851, Loss: 0.011229786090552807\n",
      "Batch 6552/14851, Loss: 0.005249673034995794\n",
      "Batch 6553/14851, Loss: 0.0036354325711727142\n",
      "Batch 6554/14851, Loss: 0.025289034470915794\n",
      "Batch 6555/14851, Loss: 0.014751115813851357\n",
      "Batch 6556/14851, Loss: 0.023545831441879272\n",
      "Batch 6557/14851, Loss: 0.015073089860379696\n",
      "Batch 6558/14851, Loss: 0.04566003754734993\n",
      "Batch 6559/14851, Loss: 0.05078811198472977\n",
      "Batch 6560/14851, Loss: 0.058653127402067184\n",
      "Batch 6561/14851, Loss: 0.04317668825387955\n",
      "Batch 6562/14851, Loss: 0.0012441599974408746\n",
      "Batch 6563/14851, Loss: 0.018497401848435402\n",
      "Batch 6564/14851, Loss: 0.02535659447312355\n",
      "Batch 6565/14851, Loss: 0.021056048572063446\n",
      "Batch 6566/14851, Loss: 0.0582369789481163\n",
      "Batch 6567/14851, Loss: 0.004592714365571737\n",
      "Batch 6568/14851, Loss: 0.018390055745840073\n",
      "Batch 6569/14851, Loss: 0.012947172857820988\n",
      "Batch 6570/14851, Loss: 0.010191207751631737\n",
      "Batch 6571/14851, Loss: 0.005748899187892675\n",
      "Batch 6572/14851, Loss: 0.044689081609249115\n",
      "Batch 6573/14851, Loss: 0.01859160326421261\n",
      "Batch 6574/14851, Loss: 0.008123435080051422\n",
      "Batch 6575/14851, Loss: 0.004992633126676083\n",
      "Batch 6576/14851, Loss: 0.0328030101954937\n",
      "Batch 6577/14851, Loss: 0.02619910053908825\n",
      "Batch 6578/14851, Loss: 0.004028028342872858\n",
      "Batch 6579/14851, Loss: 0.026774344965815544\n",
      "Batch 6580/14851, Loss: 0.0022502329666167498\n",
      "Batch 6581/14851, Loss: 0.00765288295224309\n",
      "Batch 6582/14851, Loss: 0.008918351493775845\n",
      "Batch 6583/14851, Loss: 0.0015914278337731957\n",
      "Batch 6584/14851, Loss: 0.020853200927376747\n",
      "Batch 6585/14851, Loss: 0.004197835922241211\n",
      "Batch 6586/14851, Loss: 0.09964849054813385\n",
      "Batch 6587/14851, Loss: 0.08869355916976929\n",
      "Batch 6588/14851, Loss: 0.0058326334692537785\n",
      "Batch 6589/14851, Loss: 0.004858372267335653\n",
      "Batch 6590/14851, Loss: 0.0062608979642391205\n",
      "Batch 6591/14851, Loss: 0.00272311270236969\n",
      "Batch 6592/14851, Loss: 0.01478747371584177\n",
      "Batch 6593/14851, Loss: 0.021193373948335648\n",
      "Batch 6594/14851, Loss: 0.019847579300403595\n",
      "Batch 6595/14851, Loss: 0.0075465054251253605\n",
      "Batch 6596/14851, Loss: 0.0037779659032821655\n",
      "Batch 6597/14851, Loss: 0.0023337763268500566\n",
      "Batch 6598/14851, Loss: 0.004653637763112783\n",
      "Batch 6599/14851, Loss: 0.00871837418526411\n",
      "Batch 6600/14851, Loss: 0.01885147951543331\n",
      "Batch 6601/14851, Loss: 0.04567251354455948\n",
      "Batch 6602/14851, Loss: 0.0003052304091397673\n",
      "Batch 6603/14851, Loss: 0.005339416209608316\n",
      "Batch 6604/14851, Loss: 0.1361771523952484\n",
      "Batch 6605/14851, Loss: 0.06691090017557144\n",
      "Batch 6606/14851, Loss: 0.0034822833258658648\n",
      "Batch 6607/14851, Loss: 0.012949458323419094\n",
      "Batch 6608/14851, Loss: 0.0063686855137348175\n",
      "Batch 6609/14851, Loss: 0.11809571087360382\n",
      "Batch 6610/14851, Loss: 0.015339195728302002\n",
      "Batch 6611/14851, Loss: 0.001538701355457306\n",
      "Batch 6612/14851, Loss: 0.0012565316865220666\n",
      "Batch 6613/14851, Loss: 0.0005043819546699524\n",
      "Batch 6614/14851, Loss: 0.19058208167552948\n",
      "Batch 6615/14851, Loss: 0.0007236649398691952\n",
      "Batch 6616/14851, Loss: 0.0005948481266386807\n",
      "Batch 6617/14851, Loss: 0.015489148907363415\n",
      "Batch 6618/14851, Loss: 0.03342660143971443\n",
      "Batch 6619/14851, Loss: 0.00045834234333597124\n",
      "Batch 6620/14851, Loss: 0.00044092038297094405\n",
      "Batch 6621/14851, Loss: 0.0011438543442636728\n",
      "Batch 6622/14851, Loss: 0.015702519565820694\n",
      "Batch 6623/14851, Loss: 0.0002453414199408144\n",
      "Batch 6624/14851, Loss: 0.06696649640798569\n",
      "Batch 6625/14851, Loss: 0.010251066647469997\n",
      "Batch 6626/14851, Loss: 0.03503523766994476\n",
      "Batch 6627/14851, Loss: 0.007353737019002438\n",
      "Batch 6628/14851, Loss: 0.0008454409544356167\n",
      "Batch 6629/14851, Loss: 0.00305951084010303\n",
      "Batch 6630/14851, Loss: 0.020158646628260612\n",
      "Batch 6631/14851, Loss: 0.013971158303320408\n",
      "Batch 6632/14851, Loss: 0.002482133684679866\n",
      "Batch 6633/14851, Loss: 0.006167559418827295\n",
      "Batch 6634/14851, Loss: 0.00023006896662991494\n",
      "Batch 6635/14851, Loss: 0.0015068972716107965\n",
      "Batch 6636/14851, Loss: 0.031336843967437744\n",
      "Batch 6637/14851, Loss: 0.055402517318725586\n",
      "Batch 6638/14851, Loss: 0.07276389002799988\n",
      "Batch 6639/14851, Loss: 0.012932773679494858\n",
      "Batch 6640/14851, Loss: 0.026045499369502068\n",
      "Batch 6641/14851, Loss: 0.003499565413221717\n",
      "Batch 6642/14851, Loss: 0.008523121476173401\n",
      "Batch 6643/14851, Loss: 0.014365066774189472\n",
      "Batch 6644/14851, Loss: 0.003246437758207321\n",
      "Batch 6645/14851, Loss: 0.01369320135563612\n",
      "Batch 6646/14851, Loss: 0.09182106703519821\n",
      "Batch 6647/14851, Loss: 0.012195623479783535\n",
      "Batch 6648/14851, Loss: 0.011897246353328228\n",
      "Batch 6649/14851, Loss: 0.004033992532640696\n",
      "Batch 6650/14851, Loss: 0.008392466232180595\n",
      "Batch 6651/14851, Loss: 0.02929103933274746\n",
      "Batch 6652/14851, Loss: 0.0535062775015831\n",
      "Batch 6653/14851, Loss: 0.0027022685389965773\n",
      "Batch 6654/14851, Loss: 0.008280002512037754\n",
      "Batch 6655/14851, Loss: 0.0008724058861844242\n",
      "Batch 6656/14851, Loss: 0.0010097635677084327\n",
      "Batch 6657/14851, Loss: 0.00784121360629797\n",
      "Batch 6658/14851, Loss: 0.001414177822880447\n",
      "Batch 6659/14851, Loss: 0.006833747029304504\n",
      "Batch 6660/14851, Loss: 0.0032657899428159\n",
      "Batch 6661/14851, Loss: 0.0029732200782746077\n",
      "Batch 6662/14851, Loss: 0.03925097733736038\n",
      "Batch 6663/14851, Loss: 0.0011205064365640283\n",
      "Batch 6664/14851, Loss: 0.007398291490972042\n",
      "Batch 6665/14851, Loss: 0.002710537286475301\n",
      "Batch 6666/14851, Loss: 0.007931477390229702\n",
      "Batch 6667/14851, Loss: 0.033592965453863144\n",
      "Batch 6668/14851, Loss: 0.01942603476345539\n",
      "Batch 6669/14851, Loss: 0.0015730445738881826\n",
      "Batch 6670/14851, Loss: 0.03469782695174217\n",
      "Batch 6671/14851, Loss: 0.007192469201982021\n",
      "Batch 6672/14851, Loss: 0.011830992065370083\n",
      "Batch 6673/14851, Loss: 0.015489966608583927\n",
      "Batch 6674/14851, Loss: 0.0007871451671235263\n",
      "Batch 6675/14851, Loss: 0.040528103709220886\n",
      "Batch 6676/14851, Loss: 0.006637386977672577\n",
      "Batch 6677/14851, Loss: 0.0008616894483566284\n",
      "Batch 6678/14851, Loss: 0.00103808066342026\n",
      "Batch 6679/14851, Loss: 0.0207549799233675\n",
      "Batch 6680/14851, Loss: 0.004393938463181257\n",
      "Batch 6681/14851, Loss: 0.04548622667789459\n",
      "Batch 6682/14851, Loss: 0.005241870880126953\n",
      "Batch 6683/14851, Loss: 0.002298962092027068\n",
      "Batch 6684/14851, Loss: 0.024962732568383217\n",
      "Batch 6685/14851, Loss: 0.0046119107864797115\n",
      "Batch 6686/14851, Loss: 0.004452655091881752\n",
      "Batch 6687/14851, Loss: 0.0029139842372387648\n",
      "Batch 6688/14851, Loss: 0.0012654239544644952\n",
      "Batch 6689/14851, Loss: 0.04546487703919411\n",
      "Batch 6690/14851, Loss: 0.0006083709304220974\n",
      "Batch 6691/14851, Loss: 0.00657166913151741\n",
      "Batch 6692/14851, Loss: 0.007813000120222569\n",
      "Batch 6693/14851, Loss: 0.019774582237005234\n",
      "Batch 6694/14851, Loss: 0.030964670702815056\n",
      "Batch 6695/14851, Loss: 0.011032234877347946\n",
      "Batch 6696/14851, Loss: 0.1224559098482132\n",
      "Batch 6697/14851, Loss: 0.008574512787163258\n",
      "Batch 6698/14851, Loss: 0.011729585938155651\n",
      "Batch 6699/14851, Loss: 0.015059186145663261\n",
      "Batch 6700/14851, Loss: 0.014621353708207607\n",
      "Batch 6701/14851, Loss: 0.006983102764934301\n",
      "Batch 6702/14851, Loss: 0.0261637344956398\n",
      "Batch 6703/14851, Loss: 0.030300535261631012\n",
      "Batch 6704/14851, Loss: 0.0003972339036408812\n",
      "Batch 6705/14851, Loss: 0.029127756133675575\n",
      "Batch 6706/14851, Loss: 0.015219115652143955\n",
      "Batch 6707/14851, Loss: 0.009257365949451923\n",
      "Batch 6708/14851, Loss: 0.022784844040870667\n",
      "Batch 6709/14851, Loss: 0.03491657227277756\n",
      "Batch 6710/14851, Loss: 0.009223521687090397\n",
      "Batch 6711/14851, Loss: 0.04897002503275871\n",
      "Batch 6712/14851, Loss: 0.0024022581055760384\n",
      "Batch 6713/14851, Loss: 0.0034651507157832384\n",
      "Batch 6714/14851, Loss: 0.040654998272657394\n",
      "Batch 6715/14851, Loss: 0.0012324502458795905\n",
      "Batch 6716/14851, Loss: 0.007085166405886412\n",
      "Batch 6717/14851, Loss: 0.022399235516786575\n",
      "Batch 6718/14851, Loss: 0.034280553460121155\n",
      "Batch 6719/14851, Loss: 0.0037275219801813364\n",
      "Batch 6720/14851, Loss: 0.007757632527500391\n",
      "Batch 6721/14851, Loss: 0.005625690799206495\n",
      "Batch 6722/14851, Loss: 0.004163868725299835\n",
      "Batch 6723/14851, Loss: 0.002219272078946233\n",
      "Batch 6724/14851, Loss: 0.010439835488796234\n",
      "Batch 6725/14851, Loss: 0.01564679481089115\n",
      "Batch 6726/14851, Loss: 0.00013327722263056785\n",
      "Batch 6727/14851, Loss: 0.004482449498027563\n",
      "Batch 6728/14851, Loss: 0.0004417896270751953\n",
      "Batch 6729/14851, Loss: 0.002995238872244954\n",
      "Batch 6730/14851, Loss: 0.006329737603664398\n",
      "Batch 6731/14851, Loss: 0.01847551390528679\n",
      "Batch 6732/14851, Loss: 0.0421595498919487\n",
      "Batch 6733/14851, Loss: 0.0005285118822939694\n",
      "Batch 6734/14851, Loss: 0.00653679808601737\n",
      "Batch 6735/14851, Loss: 0.01738513447344303\n",
      "Batch 6736/14851, Loss: 0.014692786149680614\n",
      "Batch 6737/14851, Loss: 0.00042005343129858375\n",
      "Batch 6738/14851, Loss: 0.049541208893060684\n",
      "Batch 6739/14851, Loss: 0.2736990749835968\n",
      "Batch 6740/14851, Loss: 0.19623707234859467\n",
      "Batch 6741/14851, Loss: 0.0010445552179589868\n",
      "Batch 6742/14851, Loss: 0.0317864790558815\n",
      "Batch 6743/14851, Loss: 0.017083993181586266\n",
      "Batch 6744/14851, Loss: 0.052707426249980927\n",
      "Batch 6745/14851, Loss: 0.026198193430900574\n",
      "Batch 6746/14851, Loss: 0.0005892614717595279\n",
      "Batch 6747/14851, Loss: 0.003353903768584132\n",
      "Batch 6748/14851, Loss: 0.004848700016736984\n",
      "Batch 6749/14851, Loss: 9.352589404443279e-05\n",
      "Batch 6750/14851, Loss: 0.03025718778371811\n",
      "Batch 6751/14851, Loss: 0.04032972827553749\n",
      "Batch 6752/14851, Loss: 0.01209890004247427\n",
      "Batch 6753/14851, Loss: 0.018243946135044098\n",
      "Batch 6754/14851, Loss: 0.01203728374093771\n",
      "Batch 6755/14851, Loss: 0.0021318658255040646\n",
      "Batch 6756/14851, Loss: 0.00030287797562777996\n",
      "Batch 6757/14851, Loss: 0.0031117808539420366\n",
      "Batch 6758/14851, Loss: 0.0015690586296841502\n",
      "Batch 6759/14851, Loss: 0.006909823510795832\n",
      "Batch 6760/14851, Loss: 0.01593554951250553\n",
      "Batch 6761/14851, Loss: 0.0008854468469507992\n",
      "Batch 6762/14851, Loss: 0.0059327492490410805\n",
      "Batch 6763/14851, Loss: 0.00029425197863020003\n",
      "Batch 6764/14851, Loss: 0.007441367022693157\n",
      "Batch 6765/14851, Loss: 0.0417923629283905\n",
      "Batch 6766/14851, Loss: 0.00470593199133873\n",
      "Batch 6767/14851, Loss: 0.0029342707712203264\n",
      "Batch 6768/14851, Loss: 0.010440262034535408\n",
      "Batch 6769/14851, Loss: 0.030519163236021996\n",
      "Batch 6770/14851, Loss: 0.00043427571654319763\n",
      "Batch 6771/14851, Loss: 0.018441883847117424\n",
      "Batch 6772/14851, Loss: 0.00865868665277958\n",
      "Batch 6773/14851, Loss: 0.01762610487639904\n",
      "Batch 6774/14851, Loss: 0.001830387394875288\n",
      "Batch 6775/14851, Loss: 0.004803492687642574\n",
      "Batch 6776/14851, Loss: 0.005064622964709997\n",
      "Batch 6777/14851, Loss: 0.019572611898183823\n",
      "Batch 6778/14851, Loss: 0.01588151790201664\n",
      "Batch 6779/14851, Loss: 0.006388361100107431\n",
      "Batch 6780/14851, Loss: 0.0013388574589043856\n",
      "Batch 6781/14851, Loss: 0.007091250270605087\n",
      "Batch 6782/14851, Loss: 0.0049589090049266815\n",
      "Batch 6783/14851, Loss: 0.010697930119931698\n",
      "Batch 6784/14851, Loss: 0.02119337022304535\n",
      "Batch 6785/14851, Loss: 0.0005292740534059703\n",
      "Batch 6786/14851, Loss: 0.04539800062775612\n",
      "Batch 6787/14851, Loss: 0.005610001739114523\n",
      "Batch 6788/14851, Loss: 0.018285434693098068\n",
      "Batch 6789/14851, Loss: 0.05305418372154236\n",
      "Batch 6790/14851, Loss: 0.000434044748544693\n",
      "Batch 6791/14851, Loss: 0.012771195732057095\n",
      "Batch 6792/14851, Loss: 0.0037944414652884007\n",
      "Batch 6793/14851, Loss: 0.022062355652451515\n",
      "Batch 6794/14851, Loss: 0.03775179386138916\n",
      "Batch 6795/14851, Loss: 0.009785697795450687\n",
      "Batch 6796/14851, Loss: 0.008855105377733707\n",
      "Batch 6797/14851, Loss: 0.004757954739034176\n",
      "Batch 6798/14851, Loss: 0.00245232624001801\n",
      "Batch 6799/14851, Loss: 0.013999479822814465\n",
      "Batch 6800/14851, Loss: 0.001127738505601883\n",
      "Batch 6801/14851, Loss: 0.003933206666260958\n",
      "Batch 6802/14851, Loss: 0.0003326932492200285\n",
      "Batch 6803/14851, Loss: 0.004096955060958862\n",
      "Batch 6804/14851, Loss: 0.005312813445925713\n",
      "Batch 6805/14851, Loss: 0.000254540384048596\n",
      "Batch 6806/14851, Loss: 0.11450065672397614\n",
      "Batch 6807/14851, Loss: 0.0025518902111798525\n",
      "Batch 6808/14851, Loss: 0.0010130066657438874\n",
      "Batch 6809/14851, Loss: 0.013217848725616932\n",
      "Batch 6810/14851, Loss: 0.009881786070764065\n",
      "Batch 6811/14851, Loss: 0.010922419838607311\n",
      "Batch 6812/14851, Loss: 0.009527432732284069\n",
      "Batch 6813/14851, Loss: 0.0011580834398046136\n",
      "Batch 6814/14851, Loss: 0.013153014704585075\n",
      "Batch 6815/14851, Loss: 0.0011342178331688046\n",
      "Batch 6816/14851, Loss: 0.0012528077932074666\n",
      "Batch 6817/14851, Loss: 0.03766360506415367\n",
      "Batch 6818/14851, Loss: 0.0002555487153586\n",
      "Batch 6819/14851, Loss: 0.008365160785615444\n",
      "Batch 6820/14851, Loss: 0.020476365461945534\n",
      "Batch 6821/14851, Loss: 0.0012565477518364787\n",
      "Batch 6822/14851, Loss: 0.007427981123328209\n",
      "Batch 6823/14851, Loss: 0.018490049988031387\n",
      "Batch 6824/14851, Loss: 0.0012769687455147505\n",
      "Batch 6825/14851, Loss: 0.03687179833650589\n",
      "Batch 6826/14851, Loss: 0.01368775311857462\n",
      "Batch 6827/14851, Loss: 0.002511652885004878\n",
      "Batch 6828/14851, Loss: 0.002894376637414098\n",
      "Batch 6829/14851, Loss: 0.0019290931522846222\n",
      "Batch 6830/14851, Loss: 0.013375752605497837\n",
      "Batch 6831/14851, Loss: 0.002251242520287633\n",
      "Batch 6832/14851, Loss: 0.00034549334668554366\n",
      "Batch 6833/14851, Loss: 9.703511750558391e-05\n",
      "Batch 6834/14851, Loss: 0.00046586370444856584\n",
      "Batch 6835/14851, Loss: 0.0012272708117961884\n",
      "Batch 6836/14851, Loss: 0.0006822508876211941\n",
      "Batch 6837/14851, Loss: 0.009116312488913536\n",
      "Skipping batch 6838 due to NaN loss\n",
      "Batch 6839/14851, Loss: nan\n",
      "Batch 6840/14851, Loss: 0.001380186527967453\n",
      "Batch 6841/14851, Loss: 0.0018215900054201484\n",
      "Batch 6842/14851, Loss: 0.0012910825898870826\n",
      "Batch 6843/14851, Loss: 0.0166635662317276\n",
      "Batch 6844/14851, Loss: 0.007021981291472912\n",
      "Batch 6845/14851, Loss: 0.00749083561822772\n",
      "Batch 6846/14851, Loss: 0.0010096393525600433\n",
      "Batch 6847/14851, Loss: 0.0006727427244186401\n",
      "Batch 6848/14851, Loss: 0.028240755200386047\n",
      "Batch 6849/14851, Loss: 0.0004571846511680633\n",
      "Batch 6850/14851, Loss: 0.0005312685971148312\n",
      "Batch 6851/14851, Loss: 0.005944834090769291\n",
      "Batch 6852/14851, Loss: 0.0005997493863105774\n",
      "Batch 6853/14851, Loss: 0.00032004466629587114\n",
      "Batch 6854/14851, Loss: 0.010619972832500935\n",
      "Batch 6855/14851, Loss: 0.0016196088399738073\n",
      "Batch 6856/14851, Loss: 0.0034145910758525133\n",
      "Batch 6857/14851, Loss: 0.0037077665328979492\n",
      "Batch 6858/14851, Loss: 0.0009429541532881558\n",
      "Batch 6859/14851, Loss: 0.00019353504467289895\n",
      "Batch 6860/14851, Loss: 0.001443028450012207\n",
      "Batch 6861/14851, Loss: 0.0173663143068552\n",
      "Batch 6862/14851, Loss: 0.0006854090024717152\n",
      "Batch 6863/14851, Loss: 0.00021422776626423\n",
      "Batch 6864/14851, Loss: 0.008202552795410156\n",
      "Batch 6865/14851, Loss: 0.00045757865882478654\n",
      "Batch 6866/14851, Loss: 0.026247384026646614\n",
      "Batch 6867/14851, Loss: 0.00010102366650244221\n",
      "Batch 6868/14851, Loss: 0.001089639961719513\n",
      "Batch 6869/14851, Loss: 0.0011208545183762908\n",
      "Batch 6870/14851, Loss: 0.0020799122285097837\n",
      "Batch 6871/14851, Loss: 0.03530484810471535\n",
      "Batch 6872/14851, Loss: 0.003917224705219269\n",
      "Batch 6873/14851, Loss: 0.0015693902969360352\n",
      "Batch 6874/14851, Loss: 0.0001289347856072709\n",
      "Batch 6875/14851, Loss: 0.019917668774724007\n",
      "Batch 6876/14851, Loss: 0.00535764591768384\n",
      "Batch 6877/14851, Loss: 3.495563942124136e-05\n",
      "Batch 6878/14851, Loss: 0.003470506053417921\n",
      "Batch 6879/14851, Loss: 0.0003197602927684784\n",
      "Batch 6880/14851, Loss: 0.0007727122865617275\n",
      "Batch 6881/14851, Loss: 0.007405319716781378\n",
      "Batch 6882/14851, Loss: 0.010971305891871452\n",
      "Batch 6883/14851, Loss: 0.0011767186224460602\n",
      "Batch 6884/14851, Loss: 0.012913831509649754\n",
      "Batch 6885/14851, Loss: 0.0002861917018890381\n",
      "Batch 6886/14851, Loss: 0.008506041020154953\n",
      "Batch 6887/14851, Loss: 0.06181693822145462\n",
      "Batch 6888/14851, Loss: 0.0020320091862231493\n",
      "Batch 6889/14851, Loss: 0.0010651078773662448\n",
      "Batch 6890/14851, Loss: 0.003609565319493413\n",
      "Batch 6891/14851, Loss: 0.009857827797532082\n",
      "Batch 6892/14851, Loss: 0.026480847969651222\n",
      "Batch 6893/14851, Loss: 0.001370440935716033\n",
      "Batch 6894/14851, Loss: 0.05449487641453743\n",
      "Batch 6895/14851, Loss: 0.006309247110038996\n",
      "Batch 6896/14851, Loss: 0.00029961889958940446\n",
      "Batch 6897/14851, Loss: 0.0020352776627987623\n",
      "Batch 6898/14851, Loss: 0.0001338869333267212\n",
      "Batch 6899/14851, Loss: 0.04294474050402641\n",
      "Batch 6900/14851, Loss: 0.002744048833847046\n",
      "Batch 6901/14851, Loss: 0.01634582318365574\n",
      "Batch 6902/14851, Loss: 0.0022293440997600555\n",
      "Batch 6903/14851, Loss: 0.03936593234539032\n",
      "Batch 6904/14851, Loss: 0.008119150996208191\n",
      "Batch 6905/14851, Loss: 0.004598882049322128\n",
      "Batch 6906/14851, Loss: 0.013198846019804478\n",
      "Batch 6907/14851, Loss: 0.006530566141009331\n",
      "Batch 6908/14851, Loss: 0.0006004560273140669\n",
      "Batch 6909/14851, Loss: 0.0012008199701085687\n",
      "Batch 6910/14851, Loss: 0.05467890202999115\n",
      "Batch 6911/14851, Loss: 0.0023352380376309156\n",
      "Batch 6912/14851, Loss: 0.004804383963346481\n",
      "Batch 6913/14851, Loss: 0.027345404028892517\n",
      "Batch 6914/14851, Loss: 0.00010712817311286926\n",
      "Batch 6915/14851, Loss: 0.0036575424019247293\n",
      "Batch 6916/14851, Loss: 0.029097547754645348\n",
      "Batch 6917/14851, Loss: 0.0006221451913006604\n",
      "Batch 6918/14851, Loss: 0.0005217455327510834\n",
      "Batch 6919/14851, Loss: 0.002510373480618\n",
      "Batch 6920/14851, Loss: 0.009117900393903255\n",
      "Batch 6921/14851, Loss: 0.008182504214346409\n",
      "Batch 6922/14851, Loss: 0.0011118365218862891\n",
      "Batch 6923/14851, Loss: 0.0005526269669644535\n",
      "Batch 6924/14851, Loss: 0.018346821889281273\n",
      "Batch 6925/14851, Loss: 0.00031564669916406274\n",
      "Batch 6926/14851, Loss: 0.0010848641395568848\n",
      "Batch 6927/14851, Loss: 0.0006292040343396366\n",
      "Batch 6928/14851, Loss: 0.014275600202381611\n",
      "Batch 6929/14851, Loss: 0.0004833970160689205\n",
      "Batch 6930/14851, Loss: 0.026991596445441246\n",
      "Batch 6931/14851, Loss: 0.00475181033834815\n",
      "Batch 6932/14851, Loss: 0.017018333077430725\n",
      "Batch 6933/14851, Loss: 0.023600773885846138\n",
      "Batch 6934/14851, Loss: 0.0009812252828851342\n",
      "Batch 6935/14851, Loss: 0.0018978278385475278\n",
      "Batch 6936/14851, Loss: 0.0005436409846879542\n",
      "Batch 6937/14851, Loss: 0.0013859878527000546\n",
      "Batch 6938/14851, Loss: 0.010696476325392723\n",
      "Batch 6939/14851, Loss: 0.00886519718915224\n",
      "Batch 6940/14851, Loss: 0.03448379412293434\n",
      "Batch 6941/14851, Loss: 0.10611709207296371\n",
      "Batch 6942/14851, Loss: 0.0011697422014549375\n",
      "Batch 6943/14851, Loss: 0.0011917089577764273\n",
      "Batch 6944/14851, Loss: 0.03977697715163231\n",
      "Batch 6945/14851, Loss: 0.20212408900260925\n",
      "Batch 6946/14851, Loss: 0.02655293047428131\n",
      "Batch 6947/14851, Loss: 2.2823611288913526e-05\n",
      "Batch 6948/14851, Loss: 0.008449286222457886\n",
      "Batch 6949/14851, Loss: 0.00017755974840838462\n",
      "Batch 6950/14851, Loss: 0.00034941607736982405\n",
      "Batch 6951/14851, Loss: 0.10017018765211105\n",
      "Batch 6952/14851, Loss: 0.004458246286958456\n",
      "Batch 6953/14851, Loss: 0.04325469583272934\n",
      "Batch 6954/14851, Loss: 0.013776594772934914\n",
      "Batch 6955/14851, Loss: 0.0008480611140839756\n",
      "Batch 6956/14851, Loss: 0.0009716649656184018\n",
      "Batch 6957/14851, Loss: 0.01433007512241602\n",
      "Batch 6958/14851, Loss: 0.008870269171893597\n",
      "Batch 6959/14851, Loss: 0.03607839718461037\n",
      "Batch 6960/14851, Loss: 0.10257050395011902\n",
      "Batch 6961/14851, Loss: 0.0017048679292201996\n",
      "Batch 6962/14851, Loss: 0.05138574168086052\n",
      "Batch 6963/14851, Loss: 0.044634707272052765\n",
      "Batch 6964/14851, Loss: 0.0012979755410924554\n",
      "Batch 6965/14851, Loss: 0.0006229840219020844\n",
      "Batch 6966/14851, Loss: 0.059454478323459625\n",
      "Batch 6967/14851, Loss: 0.0005581838195212185\n",
      "Batch 6968/14851, Loss: 0.0295433197170496\n",
      "Batch 6969/14851, Loss: 0.0023559778928756714\n",
      "Batch 6970/14851, Loss: 0.0008286808733828366\n",
      "Batch 6971/14851, Loss: 0.02135283499956131\n",
      "Batch 6972/14851, Loss: 0.03246203809976578\n",
      "Batch 6973/14851, Loss: 0.005785086657851934\n",
      "Batch 6974/14851, Loss: 0.025131864473223686\n",
      "Batch 6975/14851, Loss: 0.0010567443678155541\n",
      "Batch 6976/14851, Loss: 0.004994621966034174\n",
      "Batch 6977/14851, Loss: 0.009275969117879868\n",
      "Batch 6978/14851, Loss: 0.03828777000308037\n",
      "Batch 6979/14851, Loss: 0.001664958894252777\n",
      "Batch 6980/14851, Loss: 0.0057756975293159485\n",
      "Batch 6981/14851, Loss: 0.00477822357788682\n",
      "Batch 6982/14851, Loss: 0.013935734517872334\n",
      "Batch 6983/14851, Loss: 0.011954877525568008\n",
      "Batch 6984/14851, Loss: 0.0030349839944392443\n",
      "Batch 6985/14851, Loss: 0.004691386595368385\n",
      "Batch 6986/14851, Loss: 0.03012404590845108\n",
      "Batch 6987/14851, Loss: 0.007170563563704491\n",
      "Batch 6988/14851, Loss: 0.008438891731202602\n",
      "Batch 6989/14851, Loss: 0.001388662843964994\n",
      "Batch 6990/14851, Loss: 0.014645393937826157\n",
      "Batch 6991/14851, Loss: 0.006782816257327795\n",
      "Batch 6992/14851, Loss: 0.015632355585694313\n",
      "Batch 6993/14851, Loss: 0.0007663319702260196\n",
      "Batch 6994/14851, Loss: 0.013951761648058891\n",
      "Batch 6995/14851, Loss: 0.013887415640056133\n",
      "Batch 6996/14851, Loss: 0.01869460754096508\n",
      "Batch 6997/14851, Loss: 0.03822040185332298\n",
      "Batch 6998/14851, Loss: 0.00509960250928998\n",
      "Batch 6999/14851, Loss: 0.001766291563399136\n",
      "Batch 7000/14851, Loss: 0.021809330210089684\n",
      "Batch 7001/14851, Loss: 0.0027926750481128693\n",
      "Batch 7002/14851, Loss: 0.0026904556434601545\n",
      "Batch 7003/14851, Loss: 0.029179897159337997\n",
      "Batch 7004/14851, Loss: 0.010299545712769032\n",
      "Batch 7005/14851, Loss: 0.003649592399597168\n",
      "Batch 7006/14851, Loss: 0.18446652591228485\n",
      "Batch 7007/14851, Loss: 0.010164600796997547\n",
      "Batch 7008/14851, Loss: 0.02458069659769535\n",
      "Batch 7009/14851, Loss: 0.005360345356166363\n",
      "Batch 7010/14851, Loss: 0.018087897449731827\n",
      "Batch 7011/14851, Loss: 0.001866490114480257\n",
      "Batch 7012/14851, Loss: 0.006986070424318314\n",
      "Batch 7013/14851, Loss: 0.0040497928857803345\n",
      "Batch 7014/14851, Loss: 0.005016533192247152\n",
      "Batch 7015/14851, Loss: 0.06446099281311035\n",
      "Batch 7016/14851, Loss: 0.005420259665697813\n",
      "Batch 7017/14851, Loss: 0.026051556691527367\n",
      "Batch 7018/14851, Loss: 0.001520120888017118\n",
      "Batch 7019/14851, Loss: 0.004382993094623089\n",
      "Batch 7020/14851, Loss: 0.012828699313104153\n",
      "Batch 7021/14851, Loss: 0.0007943485979922116\n",
      "Batch 7022/14851, Loss: 0.0008232236141338944\n",
      "Batch 7023/14851, Loss: 0.006186785642057657\n",
      "Batch 7024/14851, Loss: 0.014816196635365486\n",
      "Batch 7025/14851, Loss: 0.010968693532049656\n",
      "Batch 7026/14851, Loss: 0.007218861486762762\n",
      "Batch 7027/14851, Loss: 0.0011028603184968233\n",
      "Batch 7028/14851, Loss: 0.025252338498830795\n",
      "Batch 7029/14851, Loss: 0.012896240688860416\n",
      "Batch 7030/14851, Loss: 0.03488263860344887\n",
      "Batch 7031/14851, Loss: 0.008659864775836468\n",
      "Batch 7032/14851, Loss: 0.05532851815223694\n",
      "Batch 7033/14851, Loss: 0.01729411631822586\n",
      "Batch 7034/14851, Loss: 0.011797868646681309\n",
      "Batch 7035/14851, Loss: 0.01228982675820589\n",
      "Batch 7036/14851, Loss: 0.024820461869239807\n",
      "Batch 7037/14851, Loss: 0.0005483714048750699\n",
      "Batch 7038/14851, Loss: 0.0018151527037844062\n",
      "Batch 7039/14851, Loss: 0.00413868110626936\n",
      "Batch 7040/14851, Loss: 0.026427945122122765\n",
      "Batch 7041/14851, Loss: 0.0005051245098002255\n",
      "Batch 7042/14851, Loss: 0.009824778884649277\n",
      "Batch 7043/14851, Loss: 0.011533797718584538\n",
      "Batch 7044/14851, Loss: 0.00975782796740532\n",
      "Batch 7045/14851, Loss: 0.009585306979715824\n",
      "Batch 7046/14851, Loss: 0.007548979017883539\n",
      "Batch 7047/14851, Loss: 0.00494544580578804\n",
      "Batch 7048/14851, Loss: 0.003762659849599004\n",
      "Batch 7049/14851, Loss: 0.013749011792242527\n",
      "Batch 7050/14851, Loss: 0.01140329148620367\n",
      "Batch 7051/14851, Loss: 0.003916887100785971\n",
      "Batch 7052/14851, Loss: 0.0003624012169893831\n",
      "Batch 7053/14851, Loss: 0.05113606154918671\n",
      "Batch 7054/14851, Loss: 0.0014240286545827985\n",
      "Batch 7055/14851, Loss: 0.014658727683126926\n",
      "Batch 7056/14851, Loss: 0.0619630366563797\n",
      "Batch 7057/14851, Loss: 0.003232664195820689\n",
      "Batch 7058/14851, Loss: 0.009946633130311966\n",
      "Batch 7059/14851, Loss: 0.02400578372180462\n",
      "Batch 7060/14851, Loss: 0.014790813438594341\n",
      "Batch 7061/14851, Loss: 0.0024536673445254564\n",
      "Batch 7062/14851, Loss: 0.00022091592836659402\n",
      "Batch 7063/14851, Loss: 0.053470563143491745\n",
      "Batch 7064/14851, Loss: 0.0027092837262898684\n",
      "Batch 7065/14851, Loss: 0.006460866425186396\n",
      "Batch 7066/14851, Loss: 0.009875569492578506\n",
      "Batch 7067/14851, Loss: 0.0007307405467145145\n",
      "Batch 7068/14851, Loss: 0.0022431272082030773\n",
      "Batch 7069/14851, Loss: 0.0010787637438625097\n",
      "Batch 7070/14851, Loss: 0.002796904416754842\n",
      "Batch 7071/14851, Loss: 0.005790545605123043\n",
      "Batch 7072/14851, Loss: 0.00015328700828831643\n",
      "Batch 7073/14851, Loss: 0.025747453793883324\n",
      "Batch 7074/14851, Loss: 0.006268984172493219\n",
      "Batch 7075/14851, Loss: 0.00812629796564579\n",
      "Batch 7076/14851, Loss: 0.03844998776912689\n",
      "Batch 7077/14851, Loss: 0.0008993111550807953\n",
      "Batch 7078/14851, Loss: 0.0004052942094858736\n",
      "Batch 7079/14851, Loss: 0.02188403718173504\n",
      "Batch 7080/14851, Loss: 0.005230551119893789\n",
      "Batch 7081/14851, Loss: 0.002242774935439229\n",
      "Batch 7082/14851, Loss: 0.0016561659285798669\n",
      "Batch 7083/14851, Loss: 0.003262355225160718\n",
      "Batch 7084/14851, Loss: 0.03199838846921921\n",
      "Batch 7085/14851, Loss: 0.007549259811639786\n",
      "Batch 7086/14851, Loss: 0.00046614930033683777\n",
      "Batch 7087/14851, Loss: 0.036395445466041565\n",
      "Batch 7088/14851, Loss: 0.024020081385970116\n",
      "Batch 7089/14851, Loss: 0.0006956122815608978\n",
      "Batch 7090/14851, Loss: 0.00023352976131718606\n",
      "Batch 7091/14851, Loss: 0.021515853703022003\n",
      "Batch 7092/14851, Loss: 0.03144530951976776\n",
      "Batch 7093/14851, Loss: 0.0013110028812661767\n",
      "Batch 7094/14851, Loss: 0.0011677356669679284\n",
      "Batch 7095/14851, Loss: 0.012642170302569866\n",
      "Batch 7096/14851, Loss: 0.00013807167124468833\n",
      "Batch 7097/14851, Loss: 0.03991236537694931\n",
      "Batch 7098/14851, Loss: 6.16610050201416e-05\n",
      "Batch 7099/14851, Loss: 0.0005934971268288791\n",
      "Batch 7100/14851, Loss: 0.05289975181221962\n",
      "Batch 7101/14851, Loss: 0.0036962127778679132\n",
      "Batch 7102/14851, Loss: 0.0012568148085847497\n",
      "Batch 7103/14851, Loss: 0.00833409558981657\n",
      "Batch 7104/14851, Loss: 0.002590849995613098\n",
      "Batch 7105/14851, Loss: 0.04361222684383392\n",
      "Batch 7106/14851, Loss: 0.00016706573660485446\n",
      "Batch 7107/14851, Loss: 0.013940812088549137\n",
      "Batch 7108/14851, Loss: 0.0016055852174758911\n",
      "Batch 7109/14851, Loss: 0.0013622104888781905\n",
      "Batch 7110/14851, Loss: 0.002522223861888051\n",
      "Batch 7111/14851, Loss: 0.004019053187221289\n",
      "Batch 7112/14851, Loss: 0.01120838150382042\n",
      "Batch 7113/14851, Loss: 0.0003761497209779918\n",
      "Batch 7114/14851, Loss: 0.0020848324056714773\n",
      "Batch 7115/14851, Loss: 0.005181711167097092\n",
      "Batch 7116/14851, Loss: 0.0010518600465729833\n",
      "Batch 7117/14851, Loss: 0.033983323723077774\n",
      "Batch 7118/14851, Loss: 0.03792906552553177\n",
      "Batch 7119/14851, Loss: 0.005247429944574833\n",
      "Batch 7120/14851, Loss: 0.0013426403747871518\n",
      "Batch 7121/14851, Loss: 0.0010778667638078332\n",
      "Batch 7122/14851, Loss: 0.027554508298635483\n",
      "Batch 7123/14851, Loss: 0.003444929840043187\n",
      "Batch 7124/14851, Loss: 0.00041648373007774353\n",
      "Batch 7125/14851, Loss: 0.0074049620889127254\n",
      "Batch 7126/14851, Loss: 0.0009422679431736469\n",
      "Batch 7127/14851, Loss: 0.09200241416692734\n",
      "Batch 7128/14851, Loss: 0.032246850430965424\n",
      "Batch 7129/14851, Loss: 0.0021160689648240805\n",
      "Batch 7130/14851, Loss: 0.014950888231396675\n",
      "Batch 7131/14851, Loss: 0.005687632132321596\n",
      "Batch 7132/14851, Loss: 0.0011096311500295997\n",
      "Batch 7133/14851, Loss: 0.002493447158485651\n",
      "Batch 7134/14851, Loss: 0.012293730862438679\n",
      "Batch 7135/14851, Loss: 0.0027946073096245527\n",
      "Batch 7136/14851, Loss: 0.023732496425509453\n",
      "Batch 7137/14851, Loss: 0.022701723501086235\n",
      "Batch 7138/14851, Loss: 0.010644543915987015\n",
      "Batch 7139/14851, Loss: 0.005689888261258602\n",
      "Batch 7140/14851, Loss: 0.001982440473511815\n",
      "Batch 7141/14851, Loss: 0.000796247913967818\n",
      "Batch 7142/14851, Loss: 0.0059213899075984955\n",
      "Batch 7143/14851, Loss: 0.016682112589478493\n",
      "Batch 7144/14851, Loss: 0.02153174951672554\n",
      "Batch 7145/14851, Loss: 0.007323063910007477\n",
      "Batch 7146/14851, Loss: 0.017212415114045143\n",
      "Batch 7147/14851, Loss: 0.0077008153311908245\n",
      "Batch 7148/14851, Loss: 0.0026172094512730837\n",
      "Batch 7149/14851, Loss: 0.07543940842151642\n",
      "Batch 7150/14851, Loss: 0.0009580373880453408\n",
      "Batch 7151/14851, Loss: 0.002379176439717412\n",
      "Batch 7152/14851, Loss: 0.0001640845148358494\n",
      "Batch 7153/14851, Loss: 0.005821842700242996\n",
      "Batch 7154/14851, Loss: 0.031450916081666946\n",
      "Batch 7155/14851, Loss: 0.010483881458640099\n",
      "Batch 7156/14851, Loss: 0.0031826442573219538\n",
      "Batch 7157/14851, Loss: 0.003632231382653117\n",
      "Batch 7158/14851, Loss: 0.01312891487032175\n",
      "Batch 7159/14851, Loss: 9.211773431161419e-05\n",
      "Batch 7160/14851, Loss: 0.001969646429643035\n",
      "Batch 7161/14851, Loss: 0.015186426229774952\n",
      "Batch 7162/14851, Loss: 0.0004880490305367857\n",
      "Batch 7163/14851, Loss: 0.06967440992593765\n",
      "Batch 7164/14851, Loss: 0.0006414775853045285\n",
      "Batch 7165/14851, Loss: 0.0005993013037368655\n",
      "Batch 7166/14851, Loss: 0.00022021390032023191\n",
      "Batch 7167/14851, Loss: 0.0004371782997623086\n",
      "Batch 7168/14851, Loss: 0.03765672817826271\n",
      "Batch 7169/14851, Loss: 0.00010451966954860836\n",
      "Batch 7170/14851, Loss: 0.005101991817355156\n",
      "Batch 7171/14851, Loss: 0.0040597147308290005\n",
      "Batch 7172/14851, Loss: 0.00042268758988939226\n",
      "Batch 7173/14851, Loss: 0.003279430093243718\n",
      "Batch 7174/14851, Loss: 0.0068044583313167095\n",
      "Batch 7175/14851, Loss: 0.013798415660858154\n",
      "Batch 7176/14851, Loss: 0.00013715277600567788\n",
      "Batch 7177/14851, Loss: 0.021436506882309914\n",
      "Batch 7178/14851, Loss: 0.0003088874218519777\n",
      "Batch 7179/14851, Loss: 0.008323808200657368\n",
      "Batch 7180/14851, Loss: 0.013140669092535973\n",
      "Batch 7181/14851, Loss: 0.007964002899825573\n",
      "Batch 7182/14851, Loss: 0.009819732047617435\n",
      "Batch 7183/14851, Loss: 0.0008062347769737244\n",
      "Batch 7184/14851, Loss: 0.013711857609450817\n",
      "Batch 7185/14851, Loss: 0.00044018527842126787\n",
      "Batch 7186/14851, Loss: 0.004532327875494957\n",
      "Batch 7187/14851, Loss: 0.0007499284693039954\n",
      "Batch 7188/14851, Loss: 0.002612417796626687\n",
      "Batch 7189/14851, Loss: 0.0024691284634172916\n",
      "Batch 7190/14851, Loss: 0.0011010939488187432\n",
      "Batch 7191/14851, Loss: 0.0020842691883444786\n",
      "Batch 7192/14851, Loss: 0.0007766745984554291\n",
      "Batch 7193/14851, Loss: 0.009705981239676476\n",
      "Batch 7194/14851, Loss: 0.007457836996763945\n",
      "Batch 7195/14851, Loss: 0.011131415143609047\n",
      "Batch 7196/14851, Loss: 0.02550876885652542\n",
      "Batch 7197/14851, Loss: 0.04319261759519577\n",
      "Batch 7198/14851, Loss: 0.000711267173755914\n",
      "Batch 7199/14851, Loss: 0.02531690150499344\n",
      "Batch 7200/14851, Loss: 0.011625241488218307\n",
      "Batch 7201/14851, Loss: 0.0013943227240815759\n",
      "Batch 7202/14851, Loss: 0.023578017950057983\n",
      "Batch 7203/14851, Loss: 0.004056437406688929\n",
      "Batch 7204/14851, Loss: 0.020267454907298088\n",
      "Batch 7205/14851, Loss: 0.0032885908149182796\n",
      "Batch 7206/14851, Loss: 0.014378326945006847\n",
      "Batch 7207/14851, Loss: 0.019933121278882027\n",
      "Batch 7208/14851, Loss: 0.01033878419548273\n",
      "Batch 7209/14851, Loss: 0.06201475113630295\n",
      "Batch 7210/14851, Loss: 0.07129950076341629\n",
      "Batch 7211/14851, Loss: 9.005516767501831e-05\n",
      "Batch 7212/14851, Loss: 0.021380765363574028\n",
      "Batch 7213/14851, Loss: 0.0030417863745242357\n",
      "Batch 7214/14851, Loss: 0.0036501388531178236\n",
      "Batch 7215/14851, Loss: 0.003719551023095846\n",
      "Batch 7216/14851, Loss: 0.005139967426657677\n",
      "Batch 7217/14851, Loss: 1.3946245417173486e-05\n",
      "Batch 7218/14851, Loss: 0.0042063700966537\n",
      "Batch 7219/14851, Loss: 0.019157791510224342\n",
      "Batch 7220/14851, Loss: 4.86522912979126e-05\n",
      "Batch 7221/14851, Loss: 0.013760806992650032\n",
      "Batch 7222/14851, Loss: 0.00177697092294693\n",
      "Batch 7223/14851, Loss: 0.025247875601053238\n",
      "Batch 7224/14851, Loss: 0.04816913604736328\n",
      "Batch 7225/14851, Loss: 0.0008437857031822205\n",
      "Batch 7226/14851, Loss: 0.0024130356032401323\n",
      "Batch 7227/14851, Loss: 0.0016060926718637347\n",
      "Batch 7228/14851, Loss: 0.0008597788400948048\n",
      "Batch 7229/14851, Loss: 0.052531078457832336\n",
      "Batch 7230/14851, Loss: 0.0052180043421685696\n",
      "Batch 7231/14851, Loss: 0.026063255965709686\n",
      "Batch 7232/14851, Loss: 0.0009143066708929837\n",
      "Batch 7233/14851, Loss: 0.04684945195913315\n",
      "Batch 7234/14851, Loss: 0.044994112104177475\n",
      "Batch 7235/14851, Loss: 0.0004855195584241301\n",
      "Batch 7236/14851, Loss: 0.019121313467621803\n",
      "Batch 7237/14851, Loss: 0.0034297406673431396\n",
      "Batch 7238/14851, Loss: 0.0022608477156609297\n",
      "Batch 7239/14851, Loss: 0.0006089868838898838\n",
      "Batch 7240/14851, Loss: 0.0001062105075106956\n",
      "Batch 7241/14851, Loss: 0.056828565895557404\n",
      "Batch 7242/14851, Loss: 0.043517228215932846\n",
      "Batch 7243/14851, Loss: 0.0019070899579674006\n",
      "Batch 7244/14851, Loss: 0.0002444883284624666\n",
      "Batch 7245/14851, Loss: 0.006585943978279829\n",
      "Batch 7246/14851, Loss: 0.0009492089156992733\n",
      "Batch 7247/14851, Loss: 0.0003166621027048677\n",
      "Batch 7248/14851, Loss: 0.0041091046296060085\n",
      "Batch 7249/14851, Loss: 6.447980558732525e-05\n",
      "Batch 7250/14851, Loss: 0.004341380205005407\n",
      "Batch 7251/14851, Loss: 0.0036450412590056658\n",
      "Batch 7252/14851, Loss: 0.0021480496507138014\n",
      "Batch 7253/14851, Loss: 0.015264463610947132\n",
      "Batch 7254/14851, Loss: 0.006809792015701532\n",
      "Batch 7255/14851, Loss: 0.0372704342007637\n",
      "Batch 7256/14851, Loss: 0.003410405945032835\n",
      "Batch 7257/14851, Loss: 0.008963271975517273\n",
      "Batch 7258/14851, Loss: 0.004909060895442963\n",
      "Batch 7259/14851, Loss: 0.0003085148928221315\n",
      "Batch 7260/14851, Loss: 0.00016519799828529358\n",
      "Batch 7261/14851, Loss: 0.007553279399871826\n",
      "Batch 7262/14851, Loss: 0.04842589795589447\n",
      "Batch 7263/14851, Loss: 0.02400330826640129\n",
      "Batch 7264/14851, Loss: 0.0010513340821489692\n",
      "Batch 7265/14851, Loss: 0.010455950163304806\n",
      "Batch 7266/14851, Loss: 4.965190964867361e-05\n",
      "Batch 7267/14851, Loss: 0.002624478191137314\n",
      "Batch 7268/14851, Loss: 0.017796361818909645\n",
      "Batch 7269/14851, Loss: 0.07977055758237839\n",
      "Batch 7270/14851, Loss: 0.03358250856399536\n",
      "Batch 7271/14851, Loss: 0.02194790542125702\n",
      "Batch 7272/14851, Loss: 0.005678082350641489\n",
      "Batch 7273/14851, Loss: 0.008832015097141266\n",
      "Batch 7274/14851, Loss: 0.011824742890894413\n",
      "Batch 7275/14851, Loss: 0.0017642790917307138\n",
      "Batch 7276/14851, Loss: 0.00011181334411958233\n",
      "Batch 7277/14851, Loss: 0.009004691615700722\n",
      "Batch 7278/14851, Loss: 0.047700848430395126\n",
      "Batch 7279/14851, Loss: 0.005235167220234871\n",
      "Batch 7280/14851, Loss: 0.006371042225509882\n",
      "Batch 7281/14851, Loss: 0.003540363162755966\n",
      "Batch 7282/14851, Loss: 0.01570255309343338\n",
      "Batch 7283/14851, Loss: 0.0010339008877053857\n",
      "Batch 7284/14851, Loss: 0.06179283559322357\n",
      "Batch 7285/14851, Loss: 0.04852716252207756\n",
      "Batch 7286/14851, Loss: 0.006669755559414625\n",
      "Batch 7287/14851, Loss: 0.011993867345154285\n",
      "Batch 7288/14851, Loss: 0.001329126418568194\n",
      "Batch 7289/14851, Loss: 0.04441576451063156\n",
      "Batch 7290/14851, Loss: 0.010461746715009212\n",
      "Batch 7291/14851, Loss: 0.0009830296039581299\n",
      "Batch 7292/14851, Loss: 0.0009604630176909268\n",
      "Batch 7293/14851, Loss: 0.0003613556327763945\n",
      "Batch 7294/14851, Loss: 0.0051460606046020985\n",
      "Batch 7295/14851, Loss: 0.03151621297001839\n",
      "Batch 7296/14851, Loss: 0.012603791430592537\n",
      "Batch 7297/14851, Loss: 0.02673696167767048\n",
      "Batch 7298/14851, Loss: 0.010624698363244534\n",
      "Batch 7299/14851, Loss: 0.0009416614775545895\n",
      "Batch 7300/14851, Loss: 0.0008964408771134913\n",
      "Batch 7301/14851, Loss: 0.0001646578311920166\n",
      "Batch 7302/14851, Loss: 0.003607679158449173\n",
      "Batch 7303/14851, Loss: 0.0002828290162142366\n",
      "Batch 7304/14851, Loss: 0.0281795933842659\n",
      "Batch 7305/14851, Loss: 0.00020597875118255615\n",
      "Batch 7306/14851, Loss: 0.011352182365953922\n",
      "Batch 7307/14851, Loss: 0.009490146301686764\n",
      "Batch 7308/14851, Loss: 0.006323252338916063\n",
      "Batch 7309/14851, Loss: 0.004992256406694651\n",
      "Batch 7310/14851, Loss: 0.0038856908213347197\n",
      "Batch 7311/14851, Loss: 0.0009985846700146794\n",
      "Batch 7312/14851, Loss: 0.005478908307850361\n",
      "Batch 7313/14851, Loss: 0.059160273522138596\n",
      "Batch 7314/14851, Loss: 2.929071524704341e-05\n",
      "Batch 7315/14851, Loss: 0.0007382651092484593\n",
      "Batch 7316/14851, Loss: 0.0028540927451103926\n",
      "Batch 7317/14851, Loss: 0.042003288865089417\n",
      "Batch 7318/14851, Loss: 0.053837571293115616\n",
      "Batch 7319/14851, Loss: 0.028344083577394485\n",
      "Batch 7320/14851, Loss: 0.09422309696674347\n",
      "Batch 7321/14851, Loss: 0.014933423139154911\n",
      "Batch 7322/14851, Loss: 0.008536378853023052\n",
      "Batch 7323/14851, Loss: 0.0005816941265948117\n",
      "Batch 7324/14851, Loss: 0.006820587906986475\n",
      "Batch 7325/14851, Loss: 0.021231459453701973\n",
      "Batch 7326/14851, Loss: 0.037341147661209106\n",
      "Batch 7327/14851, Loss: 0.04696042463183403\n",
      "Batch 7328/14851, Loss: 0.002987031126394868\n",
      "Batch 7329/14851, Loss: 0.02790261059999466\n",
      "Batch 7330/14851, Loss: 0.02838560938835144\n",
      "Batch 7331/14851, Loss: 0.0029501505196094513\n",
      "Batch 7332/14851, Loss: 0.0017696780851110816\n",
      "Batch 7333/14851, Loss: 0.0020629973150789738\n",
      "Batch 7334/14851, Loss: 0.006665083114057779\n",
      "Batch 7335/14851, Loss: 0.019191592931747437\n",
      "Batch 7336/14851, Loss: 0.01672428473830223\n",
      "Batch 7337/14851, Loss: 0.01240153331309557\n",
      "Batch 7338/14851, Loss: 8.142615115502849e-05\n",
      "Batch 7339/14851, Loss: 0.004746049176901579\n",
      "Batch 7340/14851, Loss: 0.007417360786348581\n",
      "Batch 7341/14851, Loss: 0.0006605428061448038\n",
      "Batch 7342/14851, Loss: 0.010439659468829632\n",
      "Batch 7343/14851, Loss: 0.06425423175096512\n",
      "Batch 7344/14851, Loss: 0.0021203074138611555\n",
      "Batch 7345/14851, Loss: 0.0059378258883953094\n",
      "Batch 7346/14851, Loss: 0.02949950285255909\n",
      "Batch 7347/14851, Loss: 0.00033831968903541565\n",
      "Batch 7348/14851, Loss: 0.0077000949531793594\n",
      "Batch 7349/14851, Loss: 0.002270879689604044\n",
      "Batch 7350/14851, Loss: 0.0012580460170283914\n",
      "Batch 7351/14851, Loss: 0.021491359919309616\n",
      "Batch 7352/14851, Loss: 0.0002624727785587311\n",
      "Batch 7353/14851, Loss: 0.0105228740721941\n",
      "Batch 7354/14851, Loss: 0.026847166940569878\n",
      "Batch 7355/14851, Loss: 0.020410174503922462\n",
      "Batch 7356/14851, Loss: 0.0034488774836063385\n",
      "Batch 7357/14851, Loss: 0.03270597383379936\n",
      "Batch 7358/14851, Loss: 0.0017480904934927821\n",
      "Batch 7359/14851, Loss: 0.0012211725115776062\n",
      "Batch 7360/14851, Loss: 0.0029821444768458605\n",
      "Batch 7361/14851, Loss: 0.013816643506288528\n",
      "Batch 7362/14851, Loss: 0.003967691212892532\n",
      "Batch 7363/14851, Loss: 0.006817513611167669\n",
      "Batch 7364/14851, Loss: 0.006334961391985416\n",
      "Batch 7365/14851, Loss: 0.003982966300100088\n",
      "Batch 7366/14851, Loss: 0.014397704042494297\n",
      "Batch 7367/14851, Loss: 0.003143342910334468\n",
      "Batch 7368/14851, Loss: 5.8259814977645874e-05\n",
      "Batch 7369/14851, Loss: 0.0006043133907951415\n",
      "Batch 7370/14851, Loss: 0.0006926804780960083\n",
      "Batch 7371/14851, Loss: 0.03135053440928459\n",
      "Batch 7372/14851, Loss: 5.804126340080984e-05\n",
      "Batch 7373/14851, Loss: 0.03951428085565567\n",
      "Batch 7374/14851, Loss: 0.0002296815364388749\n",
      "Batch 7375/14851, Loss: 0.012066581286489964\n",
      "Batch 7376/14851, Loss: 0.006362042389810085\n",
      "Batch 7377/14851, Loss: 0.0012232139706611633\n",
      "Batch 7378/14851, Loss: 0.03072734735906124\n",
      "Batch 7379/14851, Loss: 0.003157304599881172\n",
      "Batch 7380/14851, Loss: 0.007264220621436834\n",
      "Batch 7381/14851, Loss: 0.0027127438224852085\n",
      "Batch 7382/14851, Loss: 0.014488096348941326\n",
      "Batch 7383/14851, Loss: 0.00015060479927342385\n",
      "Batch 7384/14851, Loss: 0.0025121646467596292\n",
      "Batch 7385/14851, Loss: 0.0006579893524758518\n",
      "Batch 7386/14851, Loss: 6.912648677825928e-05\n",
      "Batch 7387/14851, Loss: 0.004390326794236898\n",
      "Batch 7388/14851, Loss: 0.015538991428911686\n",
      "Batch 7389/14851, Loss: 0.00277043622918427\n",
      "Batch 7390/14851, Loss: 0.013423863798379898\n",
      "Batch 7391/14851, Loss: 0.0021255251485854387\n",
      "Batch 7392/14851, Loss: 0.009692464955151081\n",
      "Batch 7393/14851, Loss: 0.00024931877851486206\n",
      "Batch 7394/14851, Loss: 0.0007929814164526761\n",
      "Batch 7395/14851, Loss: 0.003458658466115594\n",
      "Batch 7396/14851, Loss: 0.12213144451379776\n",
      "Batch 7397/14851, Loss: 0.010701044462621212\n",
      "Batch 7398/14851, Loss: 0.0024833667557686567\n",
      "Batch 7399/14851, Loss: 7.566934073111042e-05\n",
      "Batch 7400/14851, Loss: 0.0012452775845304132\n",
      "Batch 7401/14851, Loss: 0.0061225793324410915\n",
      "Batch 7402/14851, Loss: 0.0012426999164745212\n",
      "Batch 7403/14851, Loss: 0.001186008215881884\n",
      "Batch 7404/14851, Loss: 0.02795780636370182\n",
      "Batch 7405/14851, Loss: 0.0009597688913345337\n",
      "Batch 7406/14851, Loss: 0.00043312774505466223\n",
      "Batch 7407/14851, Loss: 0.0013410337269306183\n",
      "Batch 7408/14851, Loss: 7.12995752110146e-05\n",
      "Batch 7409/14851, Loss: 0.023399505764245987\n",
      "Batch 7410/14851, Loss: 0.000529089302290231\n",
      "Batch 7411/14851, Loss: 0.0014552697539329529\n",
      "Batch 7412/14851, Loss: 0.004993317183107138\n",
      "Batch 7413/14851, Loss: 0.0039545688778162\n",
      "Batch 7414/14851, Loss: 0.013910490088164806\n",
      "Batch 7415/14851, Loss: 0.0016880581388249993\n",
      "Batch 7416/14851, Loss: 0.0041646454483270645\n",
      "Batch 7417/14851, Loss: 0.0003958754241466522\n",
      "Batch 7418/14851, Loss: 0.000904801010619849\n",
      "Batch 7419/14851, Loss: 0.004156982060521841\n",
      "Batch 7420/14851, Loss: 0.0035233572125434875\n",
      "Batch 7421/14851, Loss: 0.01535623986274004\n",
      "Batch 7422/14851, Loss: 0.021644802764058113\n",
      "Batch 7423/14851, Loss: 0.0016432234551757574\n",
      "Batch 7424/14851, Loss: 0.0004546046257019043\n",
      "Batch 7425/14851, Loss: 0.0026891182642430067\n",
      "Batch 7426/14851, Loss: 0.015336684882640839\n",
      "Batch 7427/14851, Loss: 0.032109204679727554\n",
      "Batch 7428/14851, Loss: 0.020745322108268738\n",
      "Batch 7429/14851, Loss: 0.01008328702300787\n",
      "Batch 7430/14851, Loss: 0.027581721544265747\n",
      "Batch 7431/14851, Loss: 0.0013210383476689458\n",
      "Batch 7432/14851, Loss: 0.0048111313953995705\n",
      "Batch 7433/14851, Loss: 0.00018290306616108865\n",
      "Batch 7434/14851, Loss: 0.012027080170810223\n",
      "Batch 7435/14851, Loss: 0.006541998125612736\n",
      "Batch 7436/14851, Loss: 0.030727768316864967\n",
      "Batch 7437/14851, Loss: 0.018168307840824127\n",
      "Batch 7438/14851, Loss: 0.025409117341041565\n",
      "Batch 7439/14851, Loss: 0.0006888322532176971\n",
      "Batch 7440/14851, Loss: 0.07388512045145035\n",
      "Batch 7441/14851, Loss: 0.001665562973357737\n",
      "Batch 7442/14851, Loss: 0.004932763054966927\n",
      "Batch 7443/14851, Loss: 3.914286571671255e-05\n",
      "Batch 7444/14851, Loss: 0.0025994291063398123\n",
      "Batch 7445/14851, Loss: 0.00027211630367673934\n",
      "Batch 7446/14851, Loss: 0.010213014669716358\n",
      "Batch 7447/14851, Loss: 0.0002888292074203491\n",
      "Batch 7448/14851, Loss: 0.009938674978911877\n",
      "Batch 7449/14851, Loss: 0.018100235611200333\n",
      "Batch 7450/14851, Loss: 0.0006852634251117706\n",
      "Batch 7451/14851, Loss: 0.0013512721052393317\n",
      "Batch 7452/14851, Loss: 0.02757427655160427\n",
      "Batch 7453/14851, Loss: 0.03922666981816292\n",
      "Batch 7454/14851, Loss: 0.007163753267377615\n",
      "Batch 7455/14851, Loss: 0.0012055863626301289\n",
      "Batch 7456/14851, Loss: 0.010547949001193047\n",
      "Batch 7457/14851, Loss: 0.00549005251377821\n",
      "Batch 7458/14851, Loss: 0.0005258197779767215\n",
      "Batch 7459/14851, Loss: 0.01978289522230625\n",
      "Batch 7460/14851, Loss: 0.00698331231251359\n",
      "Batch 7461/14851, Loss: 0.03302166238427162\n",
      "Batch 7462/14851, Loss: 0.00044219070696271956\n",
      "Batch 7463/14851, Loss: 0.023642268031835556\n",
      "Batch 7464/14851, Loss: 0.0013836584985256195\n",
      "Batch 7465/14851, Loss: 0.013178962282836437\n",
      "Batch 7466/14851, Loss: 0.011617672629654408\n",
      "Batch 7467/14851, Loss: 0.000548510521184653\n",
      "Batch 7468/14851, Loss: 0.028034409508109093\n",
      "Batch 7469/14851, Loss: 0.010542872361838818\n",
      "Batch 7470/14851, Loss: 0.003926895558834076\n",
      "Batch 7471/14851, Loss: 0.007881224155426025\n",
      "Batch 7472/14851, Loss: 0.006305665709078312\n",
      "Batch 7473/14851, Loss: 0.0007203705608844757\n",
      "Batch 7474/14851, Loss: 0.03243246302008629\n",
      "Batch 7475/14851, Loss: 0.039339371025562286\n",
      "Batch 7476/14851, Loss: 0.000958815508056432\n",
      "Batch 7477/14851, Loss: 0.013797462917864323\n",
      "Batch 7478/14851, Loss: 0.00034888958907686174\n",
      "Batch 7479/14851, Loss: 0.009723459370434284\n",
      "Batch 7480/14851, Loss: 0.0020566496532410383\n",
      "Batch 7481/14851, Loss: 0.0009246627450920641\n",
      "Batch 7482/14851, Loss: 0.03716108202934265\n",
      "Batch 7483/14851, Loss: 0.062312815338373184\n",
      "Batch 7484/14851, Loss: 0.010877641849219799\n",
      "Batch 7485/14851, Loss: 0.00030694901943206787\n",
      "Batch 7486/14851, Loss: 0.021557899191975594\n",
      "Batch 7487/14851, Loss: 0.04871238023042679\n",
      "Batch 7488/14851, Loss: 0.0046167499385774136\n",
      "Batch 7489/14851, Loss: 0.002521861344575882\n",
      "Batch 7490/14851, Loss: 0.0023693267721682787\n",
      "Batch 7491/14851, Loss: 8.809814607957378e-05\n",
      "Batch 7492/14851, Loss: 0.0034990080166608095\n",
      "Batch 7493/14851, Loss: 0.0018754502525553107\n",
      "Batch 7494/14851, Loss: 0.003893803572282195\n",
      "Batch 7495/14851, Loss: 0.007544341497123241\n",
      "Batch 7496/14851, Loss: 0.0024884033482521772\n",
      "Batch 7497/14851, Loss: 0.006496513728052378\n",
      "Batch 7498/14851, Loss: 0.020308008417487144\n",
      "Batch 7499/14851, Loss: 0.000141192227602005\n",
      "Batch 7500/14851, Loss: 0.00038985288119874895\n",
      "Batch 7501/14851, Loss: 0.00041089579463005066\n",
      "Batch 7502/14851, Loss: 0.00022559364151675254\n",
      "Batch 7503/14851, Loss: 0.0028613682370632887\n",
      "Batch 7504/14851, Loss: 0.00035060569643974304\n",
      "Batch 7505/14851, Loss: 0.00035223306622356176\n",
      "Batch 7506/14851, Loss: 0.003870015498250723\n",
      "Batch 7507/14851, Loss: 0.00689366739243269\n",
      "Batch 7508/14851, Loss: 0.0002792477607727051\n",
      "Batch 7509/14851, Loss: 0.0006202287622727454\n",
      "Batch 7510/14851, Loss: 0.027267854660749435\n",
      "Batch 7511/14851, Loss: 0.031298745423555374\n",
      "Batch 7512/14851, Loss: 0.000839362561237067\n",
      "Batch 7513/14851, Loss: 0.039873555302619934\n",
      "Batch 7514/14851, Loss: 0.01647392474114895\n",
      "Batch 7515/14851, Loss: 0.009084836579859257\n",
      "Batch 7516/14851, Loss: 0.018050264567136765\n",
      "Batch 7517/14851, Loss: 0.0005691474070772529\n",
      "Batch 7518/14851, Loss: 0.00940344762057066\n",
      "Batch 7519/14851, Loss: 0.0007106351549737155\n",
      "Batch 7520/14851, Loss: 0.01690588891506195\n",
      "Batch 7521/14851, Loss: 0.010004855692386627\n",
      "Batch 7522/14851, Loss: 0.010202878154814243\n",
      "Batch 7523/14851, Loss: 0.013704014010727406\n",
      "Batch 7524/14851, Loss: 0.0009818089893087745\n",
      "Batch 7525/14851, Loss: 0.002213023602962494\n",
      "Batch 7526/14851, Loss: 0.06705693155527115\n",
      "Batch 7527/14851, Loss: 0.0012176731834188104\n",
      "Batch 7528/14851, Loss: 0.000908173737116158\n",
      "Batch 7529/14851, Loss: 0.017904339358210564\n",
      "Batch 7530/14851, Loss: 0.006130605470389128\n",
      "Batch 7531/14851, Loss: 0.010824024677276611\n",
      "Batch 7532/14851, Loss: 0.0007411788101308048\n",
      "Batch 7533/14851, Loss: 0.007932505570352077\n",
      "Batch 7534/14851, Loss: 0.03344223275780678\n",
      "Batch 7535/14851, Loss: 0.0037974652368575335\n",
      "Batch 7536/14851, Loss: 0.0004463878576643765\n",
      "Batch 7537/14851, Loss: 0.00026027983403764665\n",
      "Batch 7538/14851, Loss: 0.0017744252691045403\n",
      "Batch 7539/14851, Loss: 0.00039259716868400574\n",
      "Batch 7540/14851, Loss: 0.00038935491465963423\n",
      "Batch 7541/14851, Loss: 0.01548923458904028\n",
      "Batch 7542/14851, Loss: 0.007837793789803982\n",
      "Batch 7543/14851, Loss: 0.0015473089879378676\n",
      "Batch 7544/14851, Loss: 0.001287899212911725\n",
      "Batch 7545/14851, Loss: 0.008050771430134773\n",
      "Batch 7546/14851, Loss: 0.043534282594919205\n",
      "Batch 7547/14851, Loss: 0.024564852938055992\n",
      "Batch 7548/14851, Loss: 0.0014048026641830802\n",
      "Batch 7549/14851, Loss: 0.0017058601370081306\n",
      "Batch 7550/14851, Loss: 0.0021474300883710384\n",
      "Batch 7551/14851, Loss: 8.05544332251884e-05\n",
      "Batch 7552/14851, Loss: 0.017526576295495033\n",
      "Batch 7553/14851, Loss: 0.00801907666027546\n",
      "Batch 7554/14851, Loss: 0.05022391676902771\n",
      "Batch 7555/14851, Loss: 0.0024211567360907793\n",
      "Batch 7556/14851, Loss: 0.039738550782203674\n",
      "Batch 7557/14851, Loss: 0.0047571975737810135\n",
      "Batch 7558/14851, Loss: 0.0036800613161176443\n",
      "Batch 7559/14851, Loss: 0.0054218340665102005\n",
      "Batch 7560/14851, Loss: 0.020634939894080162\n",
      "Batch 7561/14851, Loss: 0.0005839131772518158\n",
      "Batch 7562/14851, Loss: 0.0194783303886652\n",
      "Batch 7563/14851, Loss: 0.01988627016544342\n",
      "Batch 7564/14851, Loss: 0.014788084663450718\n",
      "Batch 7565/14851, Loss: 0.019845861941576004\n",
      "Batch 7566/14851, Loss: 0.01992943324148655\n",
      "Batch 7567/14851, Loss: 0.0012467826018109918\n",
      "Batch 7568/14851, Loss: 0.013783223927021027\n",
      "Batch 7569/14851, Loss: 0.0184971671551466\n",
      "Batch 7570/14851, Loss: 0.004989346954971552\n",
      "Batch 7571/14851, Loss: 0.0031670245807617903\n",
      "Batch 7572/14851, Loss: 0.008876443840563297\n",
      "Batch 7573/14851, Loss: 0.04734070599079132\n",
      "Batch 7574/14851, Loss: 0.002813610015437007\n",
      "Batch 7575/14851, Loss: 0.03662589192390442\n",
      "Batch 7576/14851, Loss: 0.012253941036760807\n",
      "Batch 7577/14851, Loss: 0.0036792655009776354\n",
      "Batch 7578/14851, Loss: 0.0024049554485827684\n",
      "Batch 7579/14851, Loss: 0.004483051598072052\n",
      "Batch 7580/14851, Loss: 0.007429210003465414\n",
      "Batch 7581/14851, Loss: 0.004901856184005737\n",
      "Batch 7582/14851, Loss: 0.0009282715618610382\n",
      "Batch 7583/14851, Loss: 0.003621985437348485\n",
      "Batch 7584/14851, Loss: 0.028483903035521507\n",
      "Batch 7585/14851, Loss: 0.00961825530976057\n",
      "Batch 7586/14851, Loss: 0.021122783422470093\n",
      "Batch 7587/14851, Loss: 0.027755815535783768\n",
      "Batch 7588/14851, Loss: 0.008561489172279835\n",
      "Batch 7589/14851, Loss: 0.00023076796787790954\n",
      "Batch 7590/14851, Loss: 0.008547011762857437\n",
      "Batch 7591/14851, Loss: 0.024755902588367462\n",
      "Batch 7592/14851, Loss: 0.014200807549059391\n",
      "Batch 7593/14851, Loss: 0.046670883893966675\n",
      "Batch 7594/14851, Loss: 0.0024238568730652332\n",
      "Batch 7595/14851, Loss: 0.005825650412589312\n",
      "Batch 7596/14851, Loss: 0.028453325852751732\n",
      "Batch 7597/14851, Loss: 0.04189018905162811\n",
      "Batch 7598/14851, Loss: 0.05095100402832031\n",
      "Batch 7599/14851, Loss: 0.030791275203227997\n",
      "Batch 7600/14851, Loss: 0.004076045006513596\n",
      "Batch 7601/14851, Loss: 0.0005197232239879668\n",
      "Batch 7602/14851, Loss: 0.0002457474765833467\n",
      "Batch 7603/14851, Loss: 0.0005378735368140042\n",
      "Batch 7604/14851, Loss: 0.014186449348926544\n",
      "Batch 7605/14851, Loss: 0.02260904386639595\n",
      "Batch 7606/14851, Loss: 0.047667358070611954\n",
      "Batch 7607/14851, Loss: 0.0060349502600729465\n",
      "Batch 7608/14851, Loss: 0.0009471277589909732\n",
      "Batch 7609/14851, Loss: 0.02640610747039318\n",
      "Batch 7610/14851, Loss: 0.0011596381664276123\n",
      "Batch 7611/14851, Loss: 0.03387286514043808\n",
      "Batch 7612/14851, Loss: 0.025344496592879295\n",
      "Batch 7613/14851, Loss: 0.00429510185495019\n",
      "Batch 7614/14851, Loss: 0.014975973404943943\n",
      "Batch 7615/14851, Loss: 0.004737710114568472\n",
      "Batch 7616/14851, Loss: 0.019316505640745163\n",
      "Batch 7617/14851, Loss: 0.0014482364058494568\n",
      "Batch 7618/14851, Loss: 0.0011648846557363868\n",
      "Batch 7619/14851, Loss: 0.034672047942876816\n",
      "Batch 7620/14851, Loss: 0.005072145257145166\n",
      "Batch 7621/14851, Loss: 0.004969173576682806\n",
      "Batch 7622/14851, Loss: 0.005690647754818201\n",
      "Batch 7623/14851, Loss: 0.03280268609523773\n",
      "Batch 7624/14851, Loss: 0.0018164539942517877\n",
      "Batch 7625/14851, Loss: 0.0019286387832835317\n",
      "Batch 7626/14851, Loss: 0.0008426482672803104\n",
      "Batch 7627/14851, Loss: 0.01363256387412548\n",
      "Batch 7628/14851, Loss: 0.001993659883737564\n",
      "Batch 7629/14851, Loss: 0.012522436678409576\n",
      "Batch 7630/14851, Loss: 0.00038256371044553816\n",
      "Batch 7631/14851, Loss: 0.0005894104833714664\n",
      "Batch 7632/14851, Loss: 0.020707538351416588\n",
      "Batch 7633/14851, Loss: 3.065665441681631e-05\n",
      "Batch 7634/14851, Loss: 0.0026073132175952196\n",
      "Batch 7635/14851, Loss: 0.034682027995586395\n",
      "Batch 7636/14851, Loss: 0.001212379545904696\n",
      "Batch 7637/14851, Loss: 0.037813346832990646\n",
      "Batch 7638/14851, Loss: 0.001364565105177462\n",
      "Batch 7639/14851, Loss: 0.00017970676708500832\n",
      "Batch 7640/14851, Loss: 0.00016155217599589378\n",
      "Batch 7641/14851, Loss: 0.00039849927998147905\n",
      "Batch 7642/14851, Loss: 0.0020993079524487257\n",
      "Batch 7643/14851, Loss: 0.001868095132522285\n",
      "Batch 7644/14851, Loss: 0.05729498341679573\n",
      "Batch 7645/14851, Loss: 0.008242116309702396\n",
      "Batch 7646/14851, Loss: 0.00030346596031449735\n",
      "Batch 7647/14851, Loss: 0.011790059506893158\n",
      "Batch 7648/14851, Loss: 0.0015698045026510954\n",
      "Batch 7649/14851, Loss: 0.0009262909297831357\n",
      "Batch 7650/14851, Loss: 0.005088973790407181\n",
      "Batch 7651/14851, Loss: 0.00485761184245348\n",
      "Batch 7652/14851, Loss: 0.00040410831570625305\n",
      "Batch 7653/14851, Loss: 0.001517999917268753\n",
      "Batch 7654/14851, Loss: 0.02995905838906765\n",
      "Batch 7655/14851, Loss: 0.001167137990705669\n",
      "Batch 7656/14851, Loss: 0.018931185826659203\n",
      "Batch 7657/14851, Loss: 0.002561363624408841\n",
      "Batch 7658/14851, Loss: 0.0015260741347447038\n",
      "Batch 7659/14851, Loss: 0.000361795217031613\n",
      "Batch 7660/14851, Loss: 0.0026285324711352587\n",
      "Batch 7661/14851, Loss: 0.0013115754118189216\n",
      "Batch 7662/14851, Loss: 0.07812143862247467\n",
      "Batch 7663/14851, Loss: 0.00204307958483696\n",
      "Batch 7664/14851, Loss: 0.0037245426792651415\n",
      "Batch 7665/14851, Loss: 0.0008397537167184055\n",
      "Batch 7666/14851, Loss: 0.03444454446434975\n",
      "Batch 7667/14851, Loss: 0.000731276988517493\n",
      "Batch 7668/14851, Loss: 0.00030150017119012773\n",
      "Batch 7669/14851, Loss: 0.0025145206600427628\n",
      "Batch 7670/14851, Loss: 0.003909275867044926\n",
      "Batch 7671/14851, Loss: 0.003147655399516225\n",
      "Batch 7672/14851, Loss: 0.013917505741119385\n",
      "Batch 7673/14851, Loss: 0.0005096184904687107\n",
      "Batch 7674/14851, Loss: 0.0015185370575636625\n",
      "Batch 7675/14851, Loss: 0.0020628964994102716\n",
      "Batch 7676/14851, Loss: 0.0013471407582983375\n",
      "Batch 7677/14851, Loss: 0.0038153226487338543\n",
      "Batch 7678/14851, Loss: 0.002458658767864108\n",
      "Batch 7679/14851, Loss: 0.01934775896370411\n",
      "Batch 7680/14851, Loss: 0.00247616576962173\n",
      "Batch 7681/14851, Loss: 0.0009571498376317322\n",
      "Batch 7682/14851, Loss: 0.00018186867237091064\n",
      "Batch 7683/14851, Loss: 0.0005074914079159498\n",
      "Batch 7684/14851, Loss: 0.05124550685286522\n",
      "Batch 7685/14851, Loss: 0.00363582163117826\n",
      "Batch 7686/14851, Loss: 0.00370349851436913\n",
      "Batch 7687/14851, Loss: 0.016293006017804146\n",
      "Batch 7688/14851, Loss: 0.03655995428562164\n",
      "Batch 7689/14851, Loss: 0.024198055267333984\n",
      "Batch 7690/14851, Loss: 0.0037807465996593237\n",
      "Batch 7691/14851, Loss: 0.02008894830942154\n",
      "Batch 7692/14851, Loss: 0.00012435764074325562\n",
      "Batch 7693/14851, Loss: 0.0012054559774696827\n",
      "Batch 7694/14851, Loss: 0.0031166684348136187\n",
      "Batch 7695/14851, Loss: 0.0018922450253739953\n",
      "Batch 7696/14851, Loss: 0.0014065044233575463\n",
      "Batch 7697/14851, Loss: 0.009459265507757664\n",
      "Batch 7698/14851, Loss: 0.0154861556366086\n",
      "Batch 7699/14851, Loss: 0.004172849468886852\n",
      "Batch 7700/14851, Loss: 0.0004535230400506407\n",
      "Batch 7701/14851, Loss: 0.00033252962748520076\n",
      "Batch 7702/14851, Loss: 0.030078008770942688\n",
      "Batch 7703/14851, Loss: 0.0031929092947393656\n",
      "Batch 7704/14851, Loss: 0.02977699786424637\n",
      "Batch 7705/14851, Loss: 0.003333811881020665\n",
      "Batch 7706/14851, Loss: 0.0016214983770623803\n",
      "Batch 7707/14851, Loss: 0.027025651186704636\n",
      "Batch 7708/14851, Loss: 0.004405506420880556\n",
      "Batch 7709/14851, Loss: 0.0002806695702020079\n",
      "Batch 7710/14851, Loss: 0.0008438490331172943\n",
      "Batch 7711/14851, Loss: 0.0016527920961380005\n",
      "Batch 7712/14851, Loss: 8.6414314864669e-05\n",
      "Batch 7713/14851, Loss: 0.001272702938877046\n",
      "Batch 7714/14851, Loss: 0.000964463921263814\n",
      "Batch 7715/14851, Loss: 0.014075552113354206\n",
      "Batch 7716/14851, Loss: 0.0001811981201171875\n",
      "Batch 7717/14851, Loss: 0.03243718668818474\n",
      "Batch 7718/14851, Loss: 0.00347021478228271\n",
      "Batch 7719/14851, Loss: 0.0013196913059800863\n",
      "Batch 7720/14851, Loss: 0.0016393562545999885\n",
      "Batch 7721/14851, Loss: 0.006077077239751816\n",
      "Batch 7722/14851, Loss: 0.0009432993829250336\n",
      "Batch 7723/14851, Loss: 0.0007523360545746982\n",
      "Batch 7724/14851, Loss: 0.00038884551031515\n",
      "Batch 7725/14851, Loss: 0.026923969388008118\n",
      "Batch 7726/14851, Loss: 0.001694314181804657\n",
      "Batch 7727/14851, Loss: 0.003054329426959157\n",
      "Batch 7728/14851, Loss: 0.003627726109698415\n",
      "Batch 7729/14851, Loss: 0.005075978115200996\n",
      "Batch 7730/14851, Loss: 0.0007671815110370517\n",
      "Batch 7731/14851, Loss: 0.007165386341512203\n",
      "Batch 7732/14851, Loss: 0.0005940832197666168\n",
      "Batch 7733/14851, Loss: 0.02430494874715805\n",
      "Batch 7734/14851, Loss: 0.000600536644924432\n",
      "Batch 7735/14851, Loss: 0.0009226699476130307\n",
      "Batch 7736/14851, Loss: 0.022833513095974922\n",
      "Batch 7737/14851, Loss: 0.009392855688929558\n",
      "Batch 7738/14851, Loss: 0.0005766861140727997\n",
      "Batch 7739/14851, Loss: 0.012741303071379662\n",
      "Batch 7740/14851, Loss: 0.04457657411694527\n",
      "Batch 7741/14851, Loss: 0.018166903406381607\n",
      "Batch 7742/14851, Loss: 0.005790930241346359\n",
      "Batch 7743/14851, Loss: 0.0856417641043663\n",
      "Batch 7744/14851, Loss: 0.004935475531965494\n",
      "Batch 7745/14851, Loss: 0.02649252489209175\n",
      "Batch 7746/14851, Loss: 0.007432477083057165\n",
      "Batch 7747/14851, Loss: 0.015451798215508461\n",
      "Batch 7748/14851, Loss: 0.0018189874244853854\n",
      "Batch 7749/14851, Loss: 7.483487570425496e-05\n",
      "Batch 7750/14851, Loss: 0.0022800390142947435\n",
      "Batch 7751/14851, Loss: 0.001885689445771277\n",
      "Batch 7752/14851, Loss: 0.0005848690634593368\n",
      "Batch 7753/14851, Loss: 0.03344094380736351\n",
      "Batch 7754/14851, Loss: 0.0027567308861762285\n",
      "Batch 7755/14851, Loss: 0.0005624294281005859\n",
      "Batch 7756/14851, Loss: 0.0026705898344516754\n",
      "Batch 7757/14851, Loss: 0.007180198561400175\n",
      "Batch 7758/14851, Loss: 0.0004211390914861113\n",
      "Batch 7759/14851, Loss: 0.002275788923725486\n",
      "Batch 7760/14851, Loss: 0.00014505659055430442\n",
      "Batch 7761/14851, Loss: 0.04939284920692444\n",
      "Batch 7762/14851, Loss: 0.012942853383719921\n",
      "Batch 7763/14851, Loss: 0.031427882611751556\n",
      "Batch 7764/14851, Loss: 0.0009304073173552752\n",
      "Batch 7765/14851, Loss: 0.005297647323459387\n",
      "Batch 7766/14851, Loss: 0.003674004226922989\n",
      "Batch 7767/14851, Loss: 0.05248324200510979\n",
      "Batch 7768/14851, Loss: 0.00013690318155568093\n",
      "Batch 7769/14851, Loss: 0.0017785988748073578\n",
      "Batch 7770/14851, Loss: 0.011683841235935688\n",
      "Batch 7771/14851, Loss: 0.00197524088434875\n",
      "Batch 7772/14851, Loss: 0.03332383930683136\n",
      "Batch 7773/14851, Loss: 0.00021956861019134521\n",
      "Batch 7774/14851, Loss: 0.0012793389614671469\n",
      "Batch 7775/14851, Loss: 0.00020015612244606018\n",
      "Batch 7776/14851, Loss: 0.005850059445947409\n",
      "Batch 7777/14851, Loss: 0.0018033197848126292\n",
      "Batch 7778/14851, Loss: 0.00020715594291687012\n",
      "Batch 7779/14851, Loss: 0.01527015957981348\n",
      "Batch 7780/14851, Loss: 0.004249684978276491\n",
      "Batch 7781/14851, Loss: 0.009069214574992657\n",
      "Batch 7782/14851, Loss: 0.00015603129577357322\n",
      "Batch 7783/14851, Loss: 0.04422156512737274\n",
      "Batch 7784/14851, Loss: 0.0010558629874140024\n",
      "Batch 7785/14851, Loss: 0.0011456323554739356\n",
      "Batch 7786/14851, Loss: 0.00036993747926317155\n",
      "Batch 7787/14851, Loss: 0.0035183499567210674\n",
      "Batch 7788/14851, Loss: 0.013051054440438747\n",
      "Batch 7789/14851, Loss: 0.00015417982649523765\n",
      "Batch 7790/14851, Loss: 0.003321116091683507\n",
      "Batch 7791/14851, Loss: 0.000176413610461168\n",
      "Batch 7792/14851, Loss: 0.0062278639525175095\n",
      "Batch 7793/14851, Loss: 0.031139807775616646\n",
      "Batch 7794/14851, Loss: 0.0002681612968444824\n",
      "Batch 7795/14851, Loss: 0.0029046968556940556\n",
      "Batch 7796/14851, Loss: 0.002572152763605118\n",
      "Batch 7797/14851, Loss: 0.0016592462779954076\n",
      "Batch 7798/14851, Loss: 0.0011295373551547527\n",
      "Batch 7799/14851, Loss: 0.008669658564031124\n",
      "Batch 7800/14851, Loss: 0.0007324510952457786\n",
      "Batch 7801/14851, Loss: 0.02209066040813923\n",
      "Batch 7802/14851, Loss: 0.0001406788796884939\n",
      "Batch 7803/14851, Loss: 0.04614577069878578\n",
      "Batch 7804/14851, Loss: 0.0005072539788670838\n",
      "Batch 7805/14851, Loss: 0.06048843264579773\n",
      "Batch 7806/14851, Loss: 0.04996166750788689\n",
      "Batch 7807/14851, Loss: 0.0030562281608581543\n",
      "Batch 7808/14851, Loss: 0.043239377439022064\n",
      "Batch 7809/14851, Loss: 0.0028948145918548107\n",
      "Batch 7810/14851, Loss: 0.04215939715504646\n",
      "Batch 7811/14851, Loss: 0.004542889539152384\n",
      "Batch 7812/14851, Loss: 0.010714028961956501\n",
      "Batch 7813/14851, Loss: 0.02267204225063324\n",
      "Batch 7814/14851, Loss: 0.0001652340142754838\n",
      "Batch 7815/14851, Loss: 0.006073088850826025\n",
      "Batch 7816/14851, Loss: 0.025034602731466293\n",
      "Batch 7817/14851, Loss: 0.0021435278467833996\n",
      "Batch 7818/14851, Loss: 0.0005355117027647793\n",
      "Batch 7819/14851, Loss: 0.011154761537909508\n",
      "Batch 7820/14851, Loss: 0.03023790940642357\n",
      "Batch 7821/14851, Loss: 0.0012822260614484549\n",
      "Batch 7822/14851, Loss: 0.00046473866677843034\n",
      "Batch 7823/14851, Loss: 0.006117289420217276\n",
      "Batch 7824/14851, Loss: 0.017276404425501823\n",
      "Batch 7825/14851, Loss: 0.0043185800313949585\n",
      "Batch 7826/14851, Loss: 0.0019490346312522888\n",
      "Batch 7827/14851, Loss: 0.004944000858813524\n",
      "Batch 7828/14851, Loss: 0.0058260452933609486\n",
      "Batch 7829/14851, Loss: 0.0066628605127334595\n",
      "Batch 7830/14851, Loss: 0.007608370389789343\n",
      "Batch 7831/14851, Loss: 0.014544242061674595\n",
      "Batch 7832/14851, Loss: 0.02016657218337059\n",
      "Batch 7833/14851, Loss: 0.0017913045594468713\n",
      "Batch 7834/14851, Loss: 0.002641110448166728\n",
      "Batch 7835/14851, Loss: 0.05939552187919617\n",
      "Batch 7836/14851, Loss: 0.005449304822832346\n",
      "Batch 7837/14851, Loss: 0.0064779710955917835\n",
      "Batch 7838/14851, Loss: 0.008149870671331882\n",
      "Batch 7839/14851, Loss: 0.0004797068831976503\n",
      "Batch 7840/14851, Loss: 0.0027910470962524414\n",
      "Batch 7841/14851, Loss: 0.009807483293116093\n",
      "Batch 7842/14851, Loss: 0.0044445558451116085\n",
      "Batch 7843/14851, Loss: 0.02809920534491539\n",
      "Batch 7844/14851, Loss: 0.010496684350073338\n",
      "Batch 7845/14851, Loss: 0.006669191177934408\n",
      "Batch 7846/14851, Loss: 0.0006099790334701538\n",
      "Batch 7847/14851, Loss: 0.04230087623000145\n",
      "Batch 7848/14851, Loss: 0.12543030083179474\n",
      "Batch 7849/14851, Loss: 0.0014464916894212365\n",
      "Batch 7850/14851, Loss: 0.0018293143948540092\n",
      "Batch 7851/14851, Loss: 0.0006015983526594937\n",
      "Batch 7852/14851, Loss: 0.004513373598456383\n",
      "Batch 7853/14851, Loss: 0.00101827725302428\n",
      "Batch 7854/14851, Loss: 0.02088039554655552\n",
      "Batch 7855/14851, Loss: 0.02083657681941986\n",
      "Batch 7856/14851, Loss: 0.0050026727840304375\n",
      "Batch 7857/14851, Loss: 0.011462380178272724\n",
      "Batch 7858/14851, Loss: 0.001340708346106112\n",
      "Batch 7859/14851, Loss: 0.012324242852628231\n",
      "Batch 7860/14851, Loss: 0.0005527213215827942\n",
      "Batch 7861/14851, Loss: 0.0006033505196683109\n",
      "Batch 7862/14851, Loss: 0.012135923840105534\n",
      "Batch 7863/14851, Loss: 0.0011543730506673455\n",
      "Batch 7864/14851, Loss: 0.05522628128528595\n",
      "Batch 7865/14851, Loss: 0.016565846279263496\n",
      "Batch 7866/14851, Loss: 0.00021177281450945884\n",
      "Batch 7867/14851, Loss: 0.0020985056180506945\n",
      "Batch 7868/14851, Loss: 0.0005272995913401246\n",
      "Batch 7869/14851, Loss: 0.005277134478092194\n",
      "Batch 7870/14851, Loss: 0.0013790192315354943\n",
      "Batch 7871/14851, Loss: 0.011441407725214958\n",
      "Batch 7872/14851, Loss: 0.001029712613672018\n",
      "Batch 7873/14851, Loss: 0.0015865643508732319\n",
      "Batch 7874/14851, Loss: 0.002481164177879691\n",
      "Batch 7875/14851, Loss: 0.00948833953589201\n",
      "Batch 7876/14851, Loss: 0.014842013828456402\n",
      "Batch 7877/14851, Loss: 0.020752402022480965\n",
      "Batch 7878/14851, Loss: 0.0006931076641194522\n",
      "Batch 7879/14851, Loss: 0.0007839566678740084\n",
      "Batch 7880/14851, Loss: 0.00355728343129158\n",
      "Batch 7881/14851, Loss: 0.0005627113278023899\n",
      "Batch 7882/14851, Loss: 0.040534958243370056\n",
      "Batch 7883/14851, Loss: 0.0148400217294693\n",
      "Batch 7884/14851, Loss: 0.02007773332297802\n",
      "Batch 7885/14851, Loss: 0.013820052146911621\n",
      "Batch 7886/14851, Loss: 0.0229556392878294\n",
      "Batch 7887/14851, Loss: 0.02548748068511486\n",
      "Batch 7888/14851, Loss: 0.029469186440110207\n",
      "Batch 7889/14851, Loss: 0.020350608974695206\n",
      "Batch 7890/14851, Loss: 0.05309564620256424\n",
      "Batch 7891/14851, Loss: 0.009372328408062458\n",
      "Batch 7892/14851, Loss: 0.03712897375226021\n",
      "Batch 7893/14851, Loss: 0.0006002212758176029\n",
      "Batch 7894/14851, Loss: 0.002567703602835536\n",
      "Batch 7895/14851, Loss: 0.07312659174203873\n",
      "Batch 7896/14851, Loss: 0.0027011865749955177\n",
      "Batch 7897/14851, Loss: 0.012620780616998672\n",
      "Batch 7898/14851, Loss: 0.007615313399583101\n",
      "Batch 7899/14851, Loss: 0.0009314852650277317\n",
      "Batch 7900/14851, Loss: 0.002416191389784217\n",
      "Batch 7901/14851, Loss: 0.0032523113768547773\n",
      "Batch 7902/14851, Loss: 0.010738163255155087\n",
      "Batch 7903/14851, Loss: 0.010308127850294113\n",
      "Batch 7904/14851, Loss: 0.015141783282160759\n",
      "Batch 7905/14851, Loss: 0.07446913421154022\n",
      "Batch 7906/14851, Loss: 0.023921623826026917\n",
      "Batch 7907/14851, Loss: 0.00026460117078386247\n",
      "Batch 7908/14851, Loss: 0.00929016899317503\n",
      "Batch 7909/14851, Loss: 0.02289375476539135\n",
      "Batch 7910/14851, Loss: 0.0036158040165901184\n",
      "Batch 7911/14851, Loss: 0.0023733724374324083\n",
      "Batch 7912/14851, Loss: 0.01991659589111805\n",
      "Batch 7913/14851, Loss: 0.012251828797161579\n",
      "Batch 7914/14851, Loss: 0.0011473707854747772\n",
      "Batch 7915/14851, Loss: 0.0014411236625164747\n",
      "Batch 7916/14851, Loss: 0.0008946445886977017\n",
      "Batch 7917/14851, Loss: 0.06207805499434471\n",
      "Batch 7918/14851, Loss: 0.007144213654100895\n",
      "Batch 7919/14851, Loss: 0.017496980726718903\n",
      "Batch 7920/14851, Loss: 0.00028077265596948564\n",
      "Batch 7921/14851, Loss: 0.0032426256220787764\n",
      "Batch 7922/14851, Loss: 0.0008457787334918976\n",
      "Batch 7923/14851, Loss: 0.010423797182738781\n",
      "Batch 7924/14851, Loss: 0.00010110189759870991\n",
      "Batch 7925/14851, Loss: 0.0013347817584872246\n",
      "Batch 7926/14851, Loss: 0.0005237199366092682\n",
      "Batch 7927/14851, Loss: 0.004448323976248503\n",
      "Batch 7928/14851, Loss: 0.0002977301774080843\n",
      "Batch 7929/14851, Loss: 0.002448245882987976\n",
      "Batch 7930/14851, Loss: 0.0034441016614437103\n",
      "Batch 7931/14851, Loss: 0.00064849853515625\n",
      "Batch 7932/14851, Loss: 0.011292087845504284\n",
      "Batch 7933/14851, Loss: 0.0014469897141680121\n",
      "Batch 7934/14851, Loss: 0.006147986277937889\n",
      "Batch 7935/14851, Loss: 0.0026304598432034254\n",
      "Batch 7936/14851, Loss: 0.04520487040281296\n",
      "Batch 7937/14851, Loss: 0.002129255561158061\n",
      "Batch 7938/14851, Loss: 0.013465079478919506\n",
      "Batch 7939/14851, Loss: 0.003833673195913434\n",
      "Batch 7940/14851, Loss: 0.004545640200376511\n",
      "Batch 7941/14851, Loss: 0.0015018853591755033\n",
      "Batch 7942/14851, Loss: 0.000706307590007782\n",
      "Batch 7943/14851, Loss: 0.009740989655256271\n",
      "Batch 7944/14851, Loss: 0.004546889569610357\n",
      "Batch 7945/14851, Loss: 0.03343944624066353\n",
      "Batch 7946/14851, Loss: 0.0029732882976531982\n",
      "Batch 7947/14851, Loss: 0.0006151844863779843\n",
      "Batch 7948/14851, Loss: 0.02821715921163559\n",
      "Batch 7949/14851, Loss: 0.004986308515071869\n",
      "Batch 7950/14851, Loss: 0.041179705411195755\n",
      "Batch 7951/14851, Loss: 0.0006271153688430786\n",
      "Batch 7952/14851, Loss: 0.010843713767826557\n",
      "Batch 7953/14851, Loss: 0.009949195198714733\n",
      "Batch 7954/14851, Loss: 0.003904307959601283\n",
      "Batch 7955/14851, Loss: 0.00023704518389422446\n",
      "Batch 7956/14851, Loss: 0.0007833455456420779\n",
      "Batch 7957/14851, Loss: 0.005044323857873678\n",
      "Batch 7958/14851, Loss: 0.005137322004884481\n",
      "Batch 7959/14851, Loss: 0.008432137779891491\n",
      "Batch 7960/14851, Loss: 0.001179871498607099\n",
      "Batch 7961/14851, Loss: 0.02243066392838955\n",
      "Batch 7962/14851, Loss: 0.01099140290170908\n",
      "Batch 7963/14851, Loss: 0.012475993484258652\n",
      "Batch 7964/14851, Loss: 0.0001961253583431244\n",
      "Batch 7965/14851, Loss: 0.013228422962129116\n",
      "Batch 7966/14851, Loss: 0.002047317335382104\n",
      "Batch 7967/14851, Loss: 0.0009748924057930708\n",
      "Batch 7968/14851, Loss: 0.0028883630875498056\n",
      "Batch 7969/14851, Loss: 0.0464845634996891\n",
      "Batch 7970/14851, Loss: 0.0026091639883816242\n",
      "Batch 7971/14851, Loss: 0.00015339504170697182\n",
      "Batch 7972/14851, Loss: 0.0008686098153702915\n",
      "Batch 7973/14851, Loss: 0.0028959724586457014\n",
      "Batch 7974/14851, Loss: 0.031028006225824356\n",
      "Batch 7975/14851, Loss: 0.054654836654663086\n",
      "Batch 7976/14851, Loss: 0.004145529121160507\n",
      "Batch 7977/14851, Loss: 0.005674052983522415\n",
      "Batch 7978/14851, Loss: 0.0011522421846166253\n",
      "Batch 7979/14851, Loss: 0.00019022822380065918\n",
      "Batch 7980/14851, Loss: 0.012490407563745975\n",
      "Batch 7981/14851, Loss: 0.007134494837373495\n",
      "Batch 7982/14851, Loss: 0.03799067437648773\n",
      "Batch 7983/14851, Loss: 0.004471573047339916\n",
      "Batch 7984/14851, Loss: 0.009209333918988705\n",
      "Batch 7985/14851, Loss: 0.0007318832795135677\n",
      "Batch 7986/14851, Loss: 0.013373440131545067\n",
      "Batch 7987/14851, Loss: 0.0029720801394432783\n",
      "Batch 7988/14851, Loss: 0.01123554166406393\n",
      "Batch 7989/14851, Loss: 0.0033244926016777754\n",
      "Batch 7990/14851, Loss: 0.01599009335041046\n",
      "Batch 7991/14851, Loss: 0.006448942236602306\n",
      "Batch 7992/14851, Loss: 0.003643774427473545\n",
      "Batch 7993/14851, Loss: 0.0003209201095160097\n",
      "Batch 7994/14851, Loss: 0.012303183786571026\n",
      "Batch 7995/14851, Loss: 0.000730369589291513\n",
      "Batch 7996/14851, Loss: 0.0005310177803039551\n",
      "Batch 7997/14851, Loss: 0.04876888543367386\n",
      "Batch 7998/14851, Loss: 0.03767655789852142\n",
      "Batch 7999/14851, Loss: 0.0013307109475135803\n",
      "Batch 8000/14851, Loss: 0.027174659073352814\n",
      "Batch 8001/14851, Loss: 0.0054036835208535194\n",
      "Batch 8002/14851, Loss: 0.0019358806312084198\n",
      "Batch 8003/14851, Loss: 0.031077321618795395\n",
      "Batch 8004/14851, Loss: 9.167442476609722e-05\n",
      "Batch 8005/14851, Loss: 0.0023699484299868345\n",
      "Batch 8006/14851, Loss: 0.0008008805452845991\n",
      "Batch 8007/14851, Loss: 0.0015811725752428174\n",
      "Batch 8008/14851, Loss: 0.001834765076637268\n",
      "Batch 8009/14851, Loss: 0.011781369335949421\n",
      "Batch 8010/14851, Loss: 0.011188043281435966\n",
      "Batch 8011/14851, Loss: 0.0007713213562965393\n",
      "Batch 8012/14851, Loss: 0.038071777671575546\n",
      "Batch 8013/14851, Loss: 0.0013939155032858253\n",
      "Batch 8014/14851, Loss: 0.0011216917773708701\n",
      "Batch 8015/14851, Loss: 0.003383106319233775\n",
      "Batch 8016/14851, Loss: 0.0011180104920640588\n",
      "Batch 8017/14851, Loss: 0.008730722591280937\n",
      "Batch 8018/14851, Loss: 0.01988067850470543\n",
      "Batch 8019/14851, Loss: 0.0002738088369369507\n",
      "Batch 8020/14851, Loss: 0.009694666601717472\n",
      "Batch 8021/14851, Loss: 0.003459144150838256\n",
      "Batch 8022/14851, Loss: 0.013678030110895634\n",
      "Batch 8023/14851, Loss: 0.015944644808769226\n",
      "Batch 8024/14851, Loss: 0.048385217785835266\n",
      "Batch 8025/14851, Loss: 0.004174614790827036\n",
      "Batch 8026/14851, Loss: 0.0018763653934001923\n",
      "Batch 8027/14851, Loss: 0.000823474139906466\n",
      "Batch 8028/14851, Loss: 0.001117618172429502\n",
      "Batch 8029/14851, Loss: 0.035494837909936905\n",
      "Batch 8030/14851, Loss: 0.002100412966683507\n",
      "Batch 8031/14851, Loss: 0.050344474613666534\n",
      "Batch 8032/14851, Loss: 0.014968532137572765\n",
      "Batch 8033/14851, Loss: 0.006366064306348562\n",
      "Batch 8034/14851, Loss: 0.0031099289190024137\n",
      "Batch 8035/14851, Loss: 0.011428902857005596\n",
      "Batch 8036/14851, Loss: 0.07031912356615067\n",
      "Batch 8037/14851, Loss: 0.001298488350585103\n",
      "Batch 8038/14851, Loss: 0.0008560823043808341\n",
      "Batch 8039/14851, Loss: 0.03756135329604149\n",
      "Batch 8040/14851, Loss: 0.0006757304072380066\n",
      "Batch 8041/14851, Loss: 0.04723542928695679\n",
      "Batch 8042/14851, Loss: 0.006095811724662781\n",
      "Batch 8043/14851, Loss: 0.03497142717242241\n",
      "Batch 8044/14851, Loss: 0.0003245496191084385\n",
      "Batch 8045/14851, Loss: 0.04330885037779808\n",
      "Batch 8046/14851, Loss: 0.0004901513457298279\n",
      "Batch 8047/14851, Loss: 0.0020358215551823378\n",
      "Batch 8048/14851, Loss: 0.03432077169418335\n",
      "Batch 8049/14851, Loss: 0.005116607062518597\n",
      "Batch 8050/14851, Loss: 0.0020824475213885307\n",
      "Batch 8051/14851, Loss: 0.0005432454054243863\n",
      "Batch 8052/14851, Loss: 0.04520115256309509\n",
      "Batch 8053/14851, Loss: 0.01646491512656212\n",
      "Batch 8054/14851, Loss: 0.048291705548763275\n",
      "Batch 8055/14851, Loss: 0.0008973638177849352\n",
      "Batch 8056/14851, Loss: 0.007326476741582155\n",
      "Batch 8057/14851, Loss: 0.001566485851071775\n",
      "Batch 8058/14851, Loss: 0.0018019415438175201\n",
      "Batch 8059/14851, Loss: 0.010335547849535942\n",
      "Batch 8060/14851, Loss: 0.014206057414412498\n",
      "Batch 8061/14851, Loss: 0.004471931140869856\n",
      "Batch 8062/14851, Loss: 0.0007342013414017856\n",
      "Batch 8063/14851, Loss: 0.0005633048713207245\n",
      "Batch 8064/14851, Loss: 0.002287987619638443\n",
      "Batch 8065/14851, Loss: 0.017751626670360565\n",
      "Batch 8066/14851, Loss: 0.007362955249845982\n",
      "Batch 8067/14851, Loss: 0.00034838542342185974\n",
      "Batch 8068/14851, Loss: 0.003643633099272847\n",
      "Batch 8069/14851, Loss: 0.04414130002260208\n",
      "Batch 8070/14851, Loss: 0.018344657495617867\n",
      "Batch 8071/14851, Loss: 0.0005552980001084507\n",
      "Batch 8072/14851, Loss: 0.019684085622429848\n",
      "Batch 8073/14851, Loss: 0.0011839395156130195\n",
      "Batch 8074/14851, Loss: 0.0008770637214183807\n",
      "Batch 8075/14851, Loss: 0.0023296053986996412\n",
      "Batch 8076/14851, Loss: 0.0020693952683359385\n",
      "Batch 8077/14851, Loss: 0.005888078827410936\n",
      "Batch 8078/14851, Loss: 0.0018582029733806849\n",
      "Batch 8079/14851, Loss: 0.00924728438258171\n",
      "Batch 8080/14851, Loss: 0.031054576858878136\n",
      "Batch 8081/14851, Loss: 0.008814848028123379\n",
      "Batch 8082/14851, Loss: 0.04854031652212143\n",
      "Batch 8083/14851, Loss: 0.001139227650128305\n",
      "Batch 8084/14851, Loss: 0.011696184054017067\n",
      "Batch 8085/14851, Loss: 0.00030938288546167314\n",
      "Batch 8086/14851, Loss: 0.018717298284173012\n",
      "Batch 8087/14851, Loss: 0.001138940453529358\n",
      "Batch 8088/14851, Loss: 0.0006062264437787235\n",
      "Batch 8089/14851, Loss: 0.004472322762012482\n",
      "Batch 8090/14851, Loss: 0.005311017856001854\n",
      "Batch 8091/14851, Loss: 0.0006279696244746447\n",
      "Batch 8092/14851, Loss: 0.0027070045471191406\n",
      "Batch 8093/14851, Loss: 0.02877611480653286\n",
      "Batch 8094/14851, Loss: 0.03752506524324417\n",
      "Batch 8095/14851, Loss: 0.0014868242433294654\n",
      "Batch 8096/14851, Loss: 0.0024674066808074713\n",
      "Batch 8097/14851, Loss: 0.0006679520010948181\n",
      "Batch 8098/14851, Loss: 0.0009307712316513062\n",
      "Batch 8099/14851, Loss: 0.02291821874678135\n",
      "Batch 8100/14851, Loss: 0.0025875503197312355\n",
      "Batch 8101/14851, Loss: 0.005132489837706089\n",
      "Batch 8102/14851, Loss: 0.001009418279863894\n",
      "Batch 8103/14851, Loss: 0.016539111733436584\n",
      "Batch 8104/14851, Loss: 0.014226282946765423\n",
      "Batch 8105/14851, Loss: 0.023824593052268028\n",
      "Batch 8106/14851, Loss: 0.03010249137878418\n",
      "Batch 8107/14851, Loss: 0.0027683128137141466\n",
      "Batch 8108/14851, Loss: 0.009066340513527393\n",
      "Batch 8109/14851, Loss: 0.007422959432005882\n",
      "Batch 8110/14851, Loss: 0.0061513748951256275\n",
      "Batch 8111/14851, Loss: 0.0010826861253008246\n",
      "Batch 8112/14851, Loss: 0.00505427410826087\n",
      "Batch 8113/14851, Loss: 0.017502842471003532\n",
      "Batch 8114/14851, Loss: 0.005318338517099619\n",
      "Batch 8115/14851, Loss: 0.07611691206693649\n",
      "Batch 8116/14851, Loss: 0.00042029470205307007\n",
      "Batch 8117/14851, Loss: 0.03416607156395912\n",
      "Batch 8118/14851, Loss: 0.03256460651755333\n",
      "Batch 8119/14851, Loss: 0.016091754660010338\n",
      "Batch 8120/14851, Loss: 0.007520474959164858\n",
      "Batch 8121/14851, Loss: 0.04106326028704643\n",
      "Batch 8122/14851, Loss: 0.0031692027114331722\n",
      "Batch 8123/14851, Loss: 0.006165838334709406\n",
      "Batch 8124/14851, Loss: 0.0078542809933424\n",
      "Batch 8125/14851, Loss: 0.001293681561946869\n",
      "Batch 8126/14851, Loss: 0.0017845717957243323\n",
      "Batch 8127/14851, Loss: 0.0005460381507873535\n",
      "Batch 8128/14851, Loss: 0.003690282581374049\n",
      "Batch 8129/14851, Loss: 0.012451200745999813\n",
      "Batch 8130/14851, Loss: 0.0065717329271137714\n",
      "Batch 8131/14851, Loss: 0.019599447026848793\n",
      "Batch 8132/14851, Loss: 0.005217375699430704\n",
      "Batch 8133/14851, Loss: 0.03831769898533821\n",
      "Batch 8134/14851, Loss: 0.01499431487172842\n",
      "Batch 8135/14851, Loss: 0.0029424051754176617\n",
      "Batch 8136/14851, Loss: 0.024131035432219505\n",
      "Batch 8137/14851, Loss: 0.001526505220681429\n",
      "Batch 8138/14851, Loss: 0.012620841152966022\n",
      "Batch 8139/14851, Loss: 0.0008413679897785187\n",
      "Batch 8140/14851, Loss: 0.04846786707639694\n",
      "Batch 8141/14851, Loss: 0.000380660523660481\n",
      "Batch 8142/14851, Loss: 0.01323715690523386\n",
      "Batch 8143/14851, Loss: 0.0013869424583390355\n",
      "Batch 8144/14851, Loss: 0.013164027594029903\n",
      "Batch 8145/14851, Loss: 0.0009553630952723324\n",
      "Batch 8146/14851, Loss: 0.0007640925468876958\n",
      "Batch 8147/14851, Loss: 0.0003143673238810152\n",
      "Batch 8148/14851, Loss: 0.00032646828913129866\n",
      "Batch 8149/14851, Loss: 0.014995422214269638\n",
      "Batch 8150/14851, Loss: 0.011818052269518375\n",
      "Batch 8151/14851, Loss: 0.0009535906137898564\n",
      "Batch 8152/14851, Loss: 0.0013982774689793587\n",
      "Batch 8153/14851, Loss: 0.00023848563432693481\n",
      "Batch 8154/14851, Loss: 0.0532035194337368\n",
      "Batch 8155/14851, Loss: 0.036018867045640945\n",
      "Batch 8156/14851, Loss: 0.0046941670589149\n",
      "Batch 8157/14851, Loss: 0.04522997513413429\n",
      "Batch 8158/14851, Loss: 0.016699785366654396\n",
      "Batch 8159/14851, Loss: 0.004416653420776129\n",
      "Batch 8160/14851, Loss: 0.0007289859349839389\n",
      "Batch 8161/14851, Loss: 0.0025126647669821978\n",
      "Batch 8162/14851, Loss: 0.03942197188735008\n",
      "Batch 8163/14851, Loss: 0.0013772249221801758\n",
      "Batch 8164/14851, Loss: 0.002910597948357463\n",
      "Batch 8165/14851, Loss: 0.0005660181050188839\n",
      "Batch 8166/14851, Loss: 0.03235599398612976\n",
      "Batch 8167/14851, Loss: 0.002585911424830556\n",
      "Batch 8168/14851, Loss: 0.004732417408376932\n",
      "Batch 8169/14851, Loss: 0.02143477462232113\n",
      "Batch 8170/14851, Loss: 0.00017284353089053184\n",
      "Batch 8171/14851, Loss: 0.003450897755101323\n",
      "Batch 8172/14851, Loss: 0.05476631224155426\n",
      "Batch 8173/14851, Loss: 0.022983865812420845\n",
      "Batch 8174/14851, Loss: 0.0020252650137990713\n",
      "Batch 8175/14851, Loss: 0.0015244769165292382\n",
      "Batch 8176/14851, Loss: 0.04140166565775871\n",
      "Batch 8177/14851, Loss: 0.0019193676998838782\n",
      "Batch 8178/14851, Loss: 0.0026057634968310595\n",
      "Batch 8179/14851, Loss: 0.10277926921844482\n",
      "Batch 8180/14851, Loss: 0.004229184240102768\n",
      "Batch 8181/14851, Loss: 0.004590640310198069\n",
      "Batch 8182/14851, Loss: 0.0032957124058157206\n",
      "Batch 8183/14851, Loss: 0.0025772626977413893\n",
      "Batch 8184/14851, Loss: 0.0010301843285560608\n",
      "Batch 8185/14851, Loss: 0.03220526501536369\n",
      "Batch 8186/14851, Loss: 0.01132958848029375\n",
      "Batch 8187/14851, Loss: 0.013947189785540104\n",
      "Batch 8188/14851, Loss: 0.027080176398158073\n",
      "Batch 8189/14851, Loss: 0.011118999682366848\n",
      "Batch 8190/14851, Loss: 0.002622794359922409\n",
      "Batch 8191/14851, Loss: 0.0053486754186451435\n",
      "Batch 8192/14851, Loss: 0.010265238583087921\n",
      "Batch 8193/14851, Loss: 0.022382929921150208\n",
      "Batch 8194/14851, Loss: 0.015443033538758755\n",
      "Batch 8195/14851, Loss: 0.03411834314465523\n",
      "Batch 8196/14851, Loss: 0.0022711881902068853\n",
      "Batch 8197/14851, Loss: 0.0021705366671085358\n",
      "Batch 8198/14851, Loss: 0.007097605615854263\n",
      "Batch 8199/14851, Loss: 0.0007415376603603363\n",
      "Batch 8200/14851, Loss: 0.0015469182981178164\n",
      "Batch 8201/14851, Loss: 0.0012762509286403656\n",
      "Batch 8202/14851, Loss: 0.0004737501440104097\n",
      "Batch 8203/14851, Loss: 0.001987600000575185\n",
      "Batch 8204/14851, Loss: 0.002421945333480835\n",
      "Batch 8205/14851, Loss: 0.006765086203813553\n",
      "Batch 8206/14851, Loss: 0.0020598433911800385\n",
      "Batch 8207/14851, Loss: 0.005126084201037884\n",
      "Batch 8208/14851, Loss: 0.006172154098749161\n",
      "Batch 8209/14851, Loss: 0.004234601743519306\n",
      "Batch 8210/14851, Loss: 0.003938962239772081\n",
      "Batch 8211/14851, Loss: 0.029863718897104263\n",
      "Batch 8212/14851, Loss: 0.000731324718799442\n",
      "Batch 8213/14851, Loss: 0.008802075870335102\n",
      "Batch 8214/14851, Loss: 0.0012511919485405087\n",
      "Batch 8215/14851, Loss: 0.0008453291957266629\n",
      "Batch 8216/14851, Loss: 0.010256580077111721\n",
      "Batch 8217/14851, Loss: 0.001861319993622601\n",
      "Batch 8218/14851, Loss: 0.01076041255146265\n",
      "Batch 8219/14851, Loss: 0.0007385524804703891\n",
      "Batch 8220/14851, Loss: 0.0009739299421198666\n",
      "Batch 8221/14851, Loss: 0.0006003839080221951\n",
      "Batch 8222/14851, Loss: 0.006619274616241455\n",
      "Batch 8223/14851, Loss: 0.004259235691279173\n",
      "Batch 8224/14851, Loss: 0.00029144808650016785\n",
      "Batch 8225/14851, Loss: 0.03455232456326485\n",
      "Batch 8226/14851, Loss: 0.034141987562179565\n",
      "Batch 8227/14851, Loss: 0.0030852872878313065\n",
      "Batch 8228/14851, Loss: 0.013173634186387062\n",
      "Batch 8229/14851, Loss: 0.00460072373971343\n",
      "Batch 8230/14851, Loss: 0.01872013509273529\n",
      "Batch 8231/14851, Loss: 0.03206393122673035\n",
      "Batch 8232/14851, Loss: 0.004256053362041712\n",
      "Batch 8233/14851, Loss: 0.009961511008441448\n",
      "Batch 8234/14851, Loss: 0.0014396505430340767\n",
      "Batch 8235/14851, Loss: 0.00575260678306222\n",
      "Batch 8236/14851, Loss: 0.00035038465284742415\n",
      "Batch 8237/14851, Loss: 0.011401639319956303\n",
      "Batch 8238/14851, Loss: 0.0009683817625045776\n",
      "Batch 8239/14851, Loss: 0.04375709593296051\n",
      "Batch 8240/14851, Loss: 0.04304758459329605\n",
      "Batch 8241/14851, Loss: 0.001687895506620407\n",
      "Batch 8242/14851, Loss: 0.0030645665246993303\n",
      "Batch 8243/14851, Loss: 0.017910024151206017\n",
      "Batch 8244/14851, Loss: 0.024135081097483635\n",
      "Batch 8245/14851, Loss: 0.017390534281730652\n",
      "Batch 8246/14851, Loss: 0.027992822229862213\n",
      "Batch 8247/14851, Loss: 0.0009669053251855075\n",
      "Batch 8248/14851, Loss: 0.0029501852113753557\n",
      "Batch 8249/14851, Loss: 0.014544856734573841\n",
      "Batch 8250/14851, Loss: 0.039714083075523376\n",
      "Batch 8251/14851, Loss: 0.0007813998381607234\n",
      "Batch 8252/14851, Loss: 0.0039626359939575195\n",
      "Batch 8253/14851, Loss: 0.0055374642834067345\n",
      "Batch 8254/14851, Loss: 0.008487655781209469\n",
      "Batch 8255/14851, Loss: 0.05253337323665619\n",
      "Batch 8256/14851, Loss: 0.026375794783234596\n",
      "Batch 8257/14851, Loss: 0.0005497398669831455\n",
      "Batch 8258/14851, Loss: 0.00023811230494175106\n",
      "Batch 8259/14851, Loss: 0.022025801241397858\n",
      "Batch 8260/14851, Loss: 0.0004237790999468416\n",
      "Batch 8261/14851, Loss: 0.007647945079952478\n",
      "Batch 8262/14851, Loss: 0.008584636263549328\n",
      "Batch 8263/14851, Loss: 0.003251111600548029\n",
      "Batch 8264/14851, Loss: 5.5825959861977026e-05\n",
      "Batch 8265/14851, Loss: 0.017081527039408684\n",
      "Batch 8266/14851, Loss: 0.00519245769828558\n",
      "Batch 8267/14851, Loss: 0.0016731955111026764\n",
      "Batch 8268/14851, Loss: 0.01806807704269886\n",
      "Batch 8269/14851, Loss: 0.014184080064296722\n",
      "Batch 8270/14851, Loss: 0.00018424044537823647\n",
      "Batch 8271/14851, Loss: 0.0039672753773629665\n",
      "Batch 8272/14851, Loss: 0.06816281378269196\n",
      "Batch 8273/14851, Loss: 0.009517183527350426\n",
      "Batch 8274/14851, Loss: 0.004396796226501465\n",
      "Batch 8275/14851, Loss: 0.00722242146730423\n",
      "Batch 8276/14851, Loss: 0.003617313224822283\n",
      "Batch 8277/14851, Loss: 0.010950368829071522\n",
      "Batch 8278/14851, Loss: 0.015481166541576385\n",
      "Batch 8279/14851, Loss: 0.007552288472652435\n",
      "Batch 8280/14851, Loss: 0.0012123212218284607\n",
      "Batch 8281/14851, Loss: 0.02560262382030487\n",
      "Batch 8282/14851, Loss: 0.000539194792509079\n",
      "Batch 8283/14851, Loss: 0.0039982786402106285\n",
      "Batch 8284/14851, Loss: 0.0028535265009850264\n",
      "Batch 8285/14851, Loss: 0.00010426963126519695\n",
      "Batch 8286/14851, Loss: 0.0007791817188262939\n",
      "Batch 8287/14851, Loss: 0.0220840685069561\n",
      "Batch 8288/14851, Loss: 0.001970755634829402\n",
      "Batch 8289/14851, Loss: 0.05712157115340233\n",
      "Batch 8290/14851, Loss: 0.01354757510125637\n",
      "Batch 8291/14851, Loss: 0.0036026190500706434\n",
      "Batch 8292/14851, Loss: 0.006861862260848284\n",
      "Batch 8293/14851, Loss: 0.005317112430930138\n",
      "Batch 8294/14851, Loss: 0.016137825325131416\n",
      "Batch 8295/14851, Loss: 0.0007040277123451233\n",
      "Batch 8296/14851, Loss: 0.00012232984590809792\n",
      "Batch 8297/14851, Loss: 0.019895315170288086\n",
      "Batch 8298/14851, Loss: 0.05151527002453804\n",
      "Batch 8299/14851, Loss: 0.025116730481386185\n",
      "Batch 8300/14851, Loss: 0.0007685672026127577\n",
      "Batch 8301/14851, Loss: 0.016882462427020073\n",
      "Batch 8302/14851, Loss: 0.013879033736884594\n",
      "Batch 8303/14851, Loss: 0.0024122518952935934\n",
      "Batch 8304/14851, Loss: 0.0018195932498201728\n",
      "Batch 8305/14851, Loss: 0.006316410377621651\n",
      "Batch 8306/14851, Loss: 0.006683058105409145\n",
      "Batch 8307/14851, Loss: 0.003906931262463331\n",
      "Batch 8308/14851, Loss: 0.001226914580911398\n",
      "Batch 8309/14851, Loss: 0.03189578652381897\n",
      "Batch 8310/14851, Loss: 0.015199230052530766\n",
      "Batch 8311/14851, Loss: 0.015931513160467148\n",
      "Batch 8312/14851, Loss: 0.008378979749977589\n",
      "Batch 8313/14851, Loss: 0.008595267310738564\n",
      "Batch 8314/14851, Loss: 0.0002805727126542479\n",
      "Batch 8315/14851, Loss: 0.0007832410628907382\n",
      "Batch 8316/14851, Loss: 0.013936815783381462\n",
      "Batch 8317/14851, Loss: 0.010194829665124416\n",
      "Batch 8318/14851, Loss: 0.021016454324126244\n",
      "Batch 8319/14851, Loss: 0.006456378381699324\n",
      "Batch 8320/14851, Loss: 0.012626947835087776\n",
      "Batch 8321/14851, Loss: 0.0002463045821059495\n",
      "Batch 8322/14851, Loss: 0.02458415925502777\n",
      "Batch 8323/14851, Loss: 0.0002332627773284912\n",
      "Batch 8324/14851, Loss: 0.026695650070905685\n",
      "Batch 8325/14851, Loss: 0.008378964848816395\n",
      "Batch 8326/14851, Loss: 0.005552577320486307\n",
      "Batch 8327/14851, Loss: 0.0022032547276467085\n",
      "Batch 8328/14851, Loss: 0.005052173510193825\n",
      "Batch 8329/14851, Loss: 0.005028094165027142\n",
      "Batch 8330/14851, Loss: 0.0489845797419548\n",
      "Batch 8331/14851, Loss: 0.0011684931814670563\n",
      "Batch 8332/14851, Loss: 0.019498595967888832\n",
      "Batch 8333/14851, Loss: 0.0062074498273432255\n",
      "Batch 8334/14851, Loss: 0.003462147433310747\n",
      "Batch 8335/14851, Loss: 0.038797102868556976\n",
      "Batch 8336/14851, Loss: 0.005053154658526182\n",
      "Batch 8337/14851, Loss: 0.05745945870876312\n",
      "Batch 8338/14851, Loss: 0.0011137572582811117\n",
      "Batch 8339/14851, Loss: 0.0012020753929391503\n",
      "Batch 8340/14851, Loss: 0.010389656759798527\n",
      "Batch 8341/14851, Loss: 0.0015469416975975037\n",
      "Batch 8342/14851, Loss: 0.009521759115159512\n",
      "Batch 8343/14851, Loss: 0.0054595377296209335\n",
      "Batch 8344/14851, Loss: 0.01113867200911045\n",
      "Batch 8345/14851, Loss: 0.0016359752044081688\n",
      "Batch 8346/14851, Loss: 0.011647271923720837\n",
      "Batch 8347/14851, Loss: 0.010302462615072727\n",
      "Batch 8348/14851, Loss: 0.006692529655992985\n",
      "Batch 8349/14851, Loss: 0.00209588254801929\n",
      "Batch 8350/14851, Loss: 0.011057893745601177\n",
      "Batch 8351/14851, Loss: 0.007055125664919615\n",
      "Batch 8352/14851, Loss: 0.0023691377136856318\n",
      "Batch 8353/14851, Loss: 0.04947228357195854\n",
      "Batch 8354/14851, Loss: 0.04428079351782799\n",
      "Batch 8355/14851, Loss: 0.019653337076306343\n",
      "Batch 8356/14851, Loss: 0.0010037360480055213\n",
      "Batch 8357/14851, Loss: 0.011332002468407154\n",
      "Batch 8358/14851, Loss: 0.0015909274807199836\n",
      "Batch 8359/14851, Loss: 0.002439451403915882\n",
      "Batch 8360/14851, Loss: 0.002902725012972951\n",
      "Batch 8361/14851, Loss: 0.015264943242073059\n",
      "Batch 8362/14851, Loss: 0.008047785609960556\n",
      "Batch 8363/14851, Loss: 0.006820108741521835\n",
      "Batch 8364/14851, Loss: 0.05065019801259041\n",
      "Batch 8365/14851, Loss: 0.021916180849075317\n",
      "Batch 8366/14851, Loss: 0.00037806606269441545\n",
      "Batch 8367/14851, Loss: 0.026369264349341393\n",
      "Batch 8368/14851, Loss: 0.005628469865769148\n",
      "Batch 8369/14851, Loss: 0.004644520115107298\n",
      "Batch 8370/14851, Loss: 0.0002733458240982145\n",
      "Batch 8371/14851, Loss: 0.0020327058155089617\n",
      "Batch 8372/14851, Loss: 0.0015519621083512902\n",
      "Batch 8373/14851, Loss: 0.03297169879078865\n",
      "Batch 8374/14851, Loss: 0.0011533076176419854\n",
      "Batch 8375/14851, Loss: 0.0030251741409301758\n",
      "Batch 8376/14851, Loss: 0.00830127764493227\n",
      "Batch 8377/14851, Loss: 0.003287973115220666\n",
      "Batch 8378/14851, Loss: 0.024222586303949356\n",
      "Batch 8379/14851, Loss: 0.0007131758029572666\n",
      "Batch 8380/14851, Loss: 0.0016794124385342002\n",
      "Batch 8381/14851, Loss: 0.0004126615822315216\n",
      "Batch 8382/14851, Loss: 0.0055437334813177586\n",
      "Batch 8383/14851, Loss: 0.00266882567666471\n",
      "Batch 8384/14851, Loss: 0.0013876334996894002\n",
      "Batch 8385/14851, Loss: 0.0002764592645689845\n",
      "Batch 8386/14851, Loss: 0.0003044443728867918\n",
      "Batch 8387/14851, Loss: 0.01704445108771324\n",
      "Batch 8388/14851, Loss: 0.005239959806203842\n",
      "Batch 8389/14851, Loss: 0.006374054122716188\n",
      "Batch 8390/14851, Loss: 0.0028925854712724686\n",
      "Batch 8391/14851, Loss: 0.0017604479799047112\n",
      "Batch 8392/14851, Loss: 0.023439299315214157\n",
      "Batch 8393/14851, Loss: 0.001640400500036776\n",
      "Batch 8394/14851, Loss: 4.556402564048767e-05\n",
      "Batch 8395/14851, Loss: 0.0006917690043337643\n",
      "Batch 8396/14851, Loss: 0.005061882082372904\n",
      "Batch 8397/14851, Loss: 0.008100378327071667\n",
      "Batch 8398/14851, Loss: 0.038851261138916016\n",
      "Batch 8399/14851, Loss: 0.02428087592124939\n",
      "Batch 8400/14851, Loss: 0.01469747256487608\n",
      "Batch 8401/14851, Loss: 0.004086960107088089\n",
      "Batch 8402/14851, Loss: 0.001064194948412478\n",
      "Batch 8403/14851, Loss: 0.031401824206113815\n",
      "Batch 8404/14851, Loss: 0.00039923438453115523\n",
      "Batch 8405/14851, Loss: 0.07961615920066833\n",
      "Batch 8406/14851, Loss: 0.011040456593036652\n",
      "Batch 8407/14851, Loss: 0.007414385676383972\n",
      "Batch 8408/14851, Loss: 0.006564401090145111\n",
      "Batch 8409/14851, Loss: 0.0012153820134699345\n",
      "Batch 8410/14851, Loss: 0.0002527932228986174\n",
      "Batch 8411/14851, Loss: 0.04553595557808876\n",
      "Batch 8412/14851, Loss: 0.0002990029752254486\n",
      "Batch 8413/14851, Loss: 0.0006650450523011386\n",
      "Batch 8414/14851, Loss: 0.0027086532209068537\n",
      "Batch 8415/14851, Loss: 0.0016910011181607842\n",
      "Batch 8416/14851, Loss: 0.05612453445792198\n",
      "Batch 8417/14851, Loss: 0.00044434890151023865\n",
      "Batch 8418/14851, Loss: 0.0015082642203196883\n",
      "Batch 8419/14851, Loss: 0.0002723522484302521\n",
      "Batch 8420/14851, Loss: 0.00020115822553634644\n",
      "Batch 8421/14851, Loss: 0.0018973177066072822\n",
      "Batch 8422/14851, Loss: 0.0002817213535308838\n",
      "Batch 8423/14851, Loss: 0.0027570053935050964\n",
      "Batch 8424/14851, Loss: 0.01213027536869049\n",
      "Batch 8425/14851, Loss: 0.004120815545320511\n",
      "Batch 8426/14851, Loss: 0.0028238287195563316\n",
      "Batch 8427/14851, Loss: 0.001057364046573639\n",
      "Batch 8428/14851, Loss: 0.0005849190638400614\n",
      "Batch 8429/14851, Loss: 0.01820572279393673\n",
      "Batch 8430/14851, Loss: 0.03698011487722397\n",
      "Batch 8431/14851, Loss: 0.0008619613945484161\n",
      "Batch 8432/14851, Loss: 0.035427335649728775\n",
      "Batch 8433/14851, Loss: 0.0016829634550958872\n",
      "Batch 8434/14851, Loss: 0.024425450712442398\n",
      "Batch 8435/14851, Loss: 0.004921930376440287\n",
      "Batch 8436/14851, Loss: 0.00010917583858827129\n",
      "Batch 8437/14851, Loss: 2.44105849560583e-05\n",
      "Batch 8438/14851, Loss: 0.0006055329577066004\n",
      "Batch 8439/14851, Loss: 0.022196834906935692\n",
      "Batch 8440/14851, Loss: 0.00032642236328683794\n",
      "Batch 8441/14851, Loss: 0.0010420149192214012\n",
      "Batch 8442/14851, Loss: 0.0016297250986099243\n",
      "Batch 8443/14851, Loss: 0.0008611516677774489\n",
      "Batch 8444/14851, Loss: 0.01998722553253174\n",
      "Batch 8445/14851, Loss: 0.002506434917449951\n",
      "Batch 8446/14851, Loss: 0.02879628725349903\n",
      "Batch 8447/14851, Loss: 0.04635420814156532\n",
      "Batch 8448/14851, Loss: 9.888534987112507e-05\n",
      "Batch 8449/14851, Loss: 0.005757813807576895\n",
      "Batch 8450/14851, Loss: 0.013384460471570492\n",
      "Batch 8451/14851, Loss: 0.0017995996167883277\n",
      "Batch 8452/14851, Loss: 0.007877243682742119\n",
      "Batch 8453/14851, Loss: 0.0710107758641243\n",
      "Batch 8454/14851, Loss: 0.0019413623958826065\n",
      "Batch 8455/14851, Loss: 0.0008449070155620575\n",
      "Batch 8456/14851, Loss: 0.00043006037594750524\n",
      "Batch 8457/14851, Loss: 0.00024902945733629167\n",
      "Batch 8458/14851, Loss: 0.002474214881658554\n",
      "Batch 8459/14851, Loss: 0.051432300359010696\n",
      "Batch 8460/14851, Loss: 0.005758368875831366\n",
      "Batch 8461/14851, Loss: 0.001969254110008478\n",
      "Batch 8462/14851, Loss: 0.005580209195613861\n",
      "Batch 8463/14851, Loss: 0.029465466737747192\n",
      "Batch 8464/14851, Loss: 0.037470053881406784\n",
      "Batch 8465/14851, Loss: 0.0015308159636333585\n",
      "Batch 8466/14851, Loss: 0.0015670520951971412\n",
      "Batch 8467/14851, Loss: 0.006582456640899181\n",
      "Batch 8468/14851, Loss: 0.0008533098152838647\n",
      "Batch 8469/14851, Loss: 0.00934325810521841\n",
      "Batch 8470/14851, Loss: 0.0008599336142651737\n",
      "Batch 8471/14851, Loss: 0.01769562065601349\n",
      "Batch 8472/14851, Loss: 0.0007163811824284494\n",
      "Batch 8473/14851, Loss: 0.00305933877825737\n",
      "Batch 8474/14851, Loss: 0.0220039039850235\n",
      "Batch 8475/14851, Loss: 0.0033115174155682325\n",
      "Batch 8476/14851, Loss: 0.006588853895664215\n",
      "Batch 8477/14851, Loss: 0.0009849992347881198\n",
      "Batch 8478/14851, Loss: 0.006605842150747776\n",
      "Batch 8479/14851, Loss: 0.0047006444074213505\n",
      "Batch 8480/14851, Loss: 0.010040526278316975\n",
      "Batch 8481/14851, Loss: 0.038825761526823044\n",
      "Batch 8482/14851, Loss: 0.002374861855059862\n",
      "Batch 8483/14851, Loss: 0.0007664263248443604\n",
      "Batch 8484/14851, Loss: 0.0008982159197330475\n",
      "Batch 8485/14851, Loss: 0.0003043661417905241\n",
      "Batch 8486/14851, Loss: 0.011352721601724625\n",
      "Batch 8487/14851, Loss: 0.008607462979853153\n",
      "Batch 8488/14851, Loss: 0.00011666988575598225\n",
      "Batch 8489/14851, Loss: 0.0012882923474535346\n",
      "Batch 8490/14851, Loss: 0.005525745917111635\n",
      "Batch 8491/14851, Loss: 0.0003446464834269136\n",
      "Batch 8492/14851, Loss: 0.00028779724380001426\n",
      "Batch 8493/14851, Loss: 0.0002017083315877244\n",
      "Batch 8494/14851, Loss: 0.008123715408146381\n",
      "Batch 8495/14851, Loss: 0.005487034562975168\n",
      "Batch 8496/14851, Loss: 0.0009834319353103638\n",
      "Batch 8497/14851, Loss: 0.0032778382301330566\n",
      "Batch 8498/14851, Loss: 0.000803312927018851\n",
      "Batch 8499/14851, Loss: 0.0015932614915072918\n",
      "Batch 8500/14851, Loss: 0.0004914775490760803\n",
      "Batch 8501/14851, Loss: 0.0008753836154937744\n",
      "Batch 8502/14851, Loss: 0.0019541357178241014\n",
      "Batch 8503/14851, Loss: 0.04931970685720444\n",
      "Batch 8504/14851, Loss: 0.049954209476709366\n",
      "Batch 8505/14851, Loss: 0.01012219674885273\n",
      "Batch 8506/14851, Loss: 0.009787052869796753\n",
      "Batch 8507/14851, Loss: 0.00146658590529114\n",
      "Batch 8508/14851, Loss: 0.029542770236730576\n",
      "Batch 8509/14851, Loss: 0.030283067375421524\n",
      "Batch 8510/14851, Loss: 0.0004603204724844545\n",
      "Batch 8511/14851, Loss: 0.0004457223985809833\n",
      "Batch 8512/14851, Loss: 0.025130189955234528\n",
      "Batch 8513/14851, Loss: 0.0009144338546320796\n",
      "Batch 8514/14851, Loss: 0.0009562654886394739\n",
      "Batch 8515/14851, Loss: 6.168708205223083e-05\n",
      "Batch 8516/14851, Loss: 0.001322631142102182\n",
      "Batch 8517/14851, Loss: 0.023929351940751076\n",
      "Batch 8518/14851, Loss: 0.0007968917489051819\n",
      "Batch 8519/14851, Loss: 0.003374395426362753\n",
      "Batch 8520/14851, Loss: 0.0005936966044828296\n",
      "Batch 8521/14851, Loss: 0.00086875882698223\n",
      "Batch 8522/14851, Loss: 0.006988638546317816\n",
      "Batch 8523/14851, Loss: 0.0005189205403439701\n",
      "Batch 8524/14851, Loss: 0.001946919015608728\n",
      "Batch 8525/14851, Loss: 0.0001308557839365676\n",
      "Batch 8526/14851, Loss: 0.00201619416475296\n",
      "Batch 8527/14851, Loss: 0.043835241347551346\n",
      "Batch 8528/14851, Loss: 0.001045726239681244\n",
      "Batch 8529/14851, Loss: 0.007625239435583353\n",
      "Batch 8530/14851, Loss: 0.00402187742292881\n",
      "Batch 8531/14851, Loss: 0.0027739275246858597\n",
      "Batch 8532/14851, Loss: 0.0008649165974929929\n",
      "Batch 8533/14851, Loss: 0.00047253817319869995\n",
      "Batch 8534/14851, Loss: 0.0011086998274549842\n",
      "Batch 8535/14851, Loss: 0.00014219175500329584\n",
      "Batch 8536/14851, Loss: 0.010577389039099216\n",
      "Batch 8537/14851, Loss: 0.00022351091320160776\n",
      "Batch 8538/14851, Loss: 0.0001373117120238021\n",
      "Batch 8539/14851, Loss: 0.008400035090744495\n",
      "Batch 8540/14851, Loss: 0.001852379529736936\n",
      "Batch 8541/14851, Loss: 0.000866996415425092\n",
      "Batch 8542/14851, Loss: 0.04457825422286987\n",
      "Batch 8543/14851, Loss: 0.035166747868061066\n",
      "Batch 8544/14851, Loss: 0.00029314676066860557\n",
      "Batch 8545/14851, Loss: 0.04986908659338951\n",
      "Batch 8546/14851, Loss: 0.00761561281979084\n",
      "Batch 8547/14851, Loss: 0.002570018172264099\n",
      "Batch 8548/14851, Loss: 0.003954791929572821\n",
      "Batch 8549/14851, Loss: 0.00035024932003580034\n",
      "Batch 8550/14851, Loss: 0.0030028526671230793\n",
      "Batch 8551/14851, Loss: 0.0012810260523110628\n",
      "Batch 8552/14851, Loss: 0.03908848762512207\n",
      "Batch 8553/14851, Loss: 0.01161196455359459\n",
      "Batch 8554/14851, Loss: 0.004129492677748203\n",
      "Batch 8555/14851, Loss: 0.0005441182875074446\n",
      "Batch 8556/14851, Loss: 0.042194634675979614\n",
      "Batch 8557/14851, Loss: 0.010610995814204216\n",
      "Batch 8558/14851, Loss: 0.002846683142706752\n",
      "Batch 8559/14851, Loss: 0.05249490216374397\n",
      "Batch 8560/14851, Loss: 0.006905175745487213\n",
      "Batch 8561/14851, Loss: 0.008434589020907879\n",
      "Batch 8562/14851, Loss: 0.0013996424386277795\n",
      "Batch 8563/14851, Loss: 0.0012264387914910913\n",
      "Batch 8564/14851, Loss: 0.04057817533612251\n",
      "Batch 8565/14851, Loss: 1.0752429261629004e-05\n",
      "Batch 8566/14851, Loss: 0.008438102900981903\n",
      "Batch 8567/14851, Loss: 5.299225449562073e-05\n",
      "Batch 8568/14851, Loss: 0.0694902315735817\n",
      "Batch 8569/14851, Loss: 0.024701276794075966\n",
      "Batch 8570/14851, Loss: 0.0038678261917084455\n",
      "Batch 8571/14851, Loss: 0.055294059216976166\n",
      "Batch 8572/14851, Loss: 0.008150854147970676\n",
      "Batch 8573/14851, Loss: 0.0033625576179474592\n",
      "Batch 8574/14851, Loss: 0.0002133573143510148\n",
      "Batch 8575/14851, Loss: 0.0028277423698455095\n",
      "Batch 8576/14851, Loss: 0.006734674330800772\n",
      "Batch 8577/14851, Loss: 0.003912726882845163\n",
      "Batch 8578/14851, Loss: 0.028003405779600143\n",
      "Batch 8579/14851, Loss: 0.0722341239452362\n",
      "Batch 8580/14851, Loss: 0.00013100977230351418\n",
      "Batch 8581/14851, Loss: 0.009748207405209541\n",
      "Batch 8582/14851, Loss: 0.0019859347958117723\n",
      "Batch 8583/14851, Loss: 0.00048117959522642195\n",
      "Batch 8584/14851, Loss: 0.062448255717754364\n",
      "Batch 8585/14851, Loss: 0.0102776437997818\n",
      "Batch 8586/14851, Loss: 0.01832468993961811\n",
      "Batch 8587/14851, Loss: 0.0005296121817082167\n",
      "Batch 8588/14851, Loss: 0.0003987687232438475\n",
      "Batch 8589/14851, Loss: 0.0009432993829250336\n",
      "Batch 8590/14851, Loss: 0.010063561610877514\n",
      "Batch 8591/14851, Loss: 0.0013816766440868378\n",
      "Batch 8592/14851, Loss: 0.0036899547558277845\n",
      "Batch 8593/14851, Loss: 0.00019592295575421304\n",
      "Batch 8594/14851, Loss: 0.00010299807036062703\n",
      "Batch 8595/14851, Loss: 0.0005362711963243783\n",
      "Batch 8596/14851, Loss: 0.03913106396794319\n",
      "Batch 8597/14851, Loss: 9.083375334739685e-05\n",
      "Batch 8598/14851, Loss: 0.010109568946063519\n",
      "Batch 8599/14851, Loss: 0.019775254651904106\n",
      "Batch 8600/14851, Loss: 0.09220235794782639\n",
      "Batch 8601/14851, Loss: 0.026462500914931297\n",
      "Batch 8602/14851, Loss: 0.0001372992992401123\n",
      "Batch 8603/14851, Loss: 0.035670436918735504\n",
      "Batch 8604/14851, Loss: 0.006671290844678879\n",
      "Batch 8605/14851, Loss: 0.00023362661886494607\n",
      "Batch 8606/14851, Loss: 0.030464069917798042\n",
      "Batch 8607/14851, Loss: 0.0020615842659026384\n",
      "Batch 8608/14851, Loss: 4.6795856178505346e-05\n",
      "Batch 8609/14851, Loss: 0.0002353017480345443\n",
      "Batch 8610/14851, Loss: 0.01251502986997366\n",
      "Batch 8611/14851, Loss: 0.003505877684801817\n",
      "Batch 8612/14851, Loss: 5.869070810149424e-05\n",
      "Batch 8613/14851, Loss: 0.004713783506304026\n",
      "Batch 8614/14851, Loss: 0.00019733111548703164\n",
      "Batch 8615/14851, Loss: 0.0056833201088011265\n",
      "Batch 8616/14851, Loss: 0.03974897414445877\n",
      "Batch 8617/14851, Loss: 0.001462601125240326\n",
      "Batch 8618/14851, Loss: 0.00578903267160058\n",
      "Batch 8619/14851, Loss: 7.669875776628032e-05\n",
      "Batch 8620/14851, Loss: 0.0016615850618109107\n",
      "Batch 8621/14851, Loss: 0.007749441545456648\n",
      "Batch 8622/14851, Loss: 0.034545183181762695\n",
      "Batch 8623/14851, Loss: 0.0003541051410138607\n",
      "Batch 8624/14851, Loss: 0.00388244423083961\n",
      "Batch 8625/14851, Loss: 0.0011895435163751245\n",
      "Batch 8626/14851, Loss: 0.0012886114418506622\n",
      "Batch 8627/14851, Loss: 0.018753692507743835\n",
      "Batch 8628/14851, Loss: 0.002454472007229924\n",
      "Batch 8629/14851, Loss: 0.008772040717303753\n",
      "Batch 8630/14851, Loss: 0.0036499735433608294\n",
      "Batch 8631/14851, Loss: 0.0022234260104596615\n",
      "Batch 8632/14851, Loss: 0.07366088032722473\n",
      "Batch 8633/14851, Loss: 0.0026317909359931946\n",
      "Batch 8634/14851, Loss: 0.012147565372288227\n",
      "Batch 8635/14851, Loss: 0.0033242946956306696\n",
      "Batch 8636/14851, Loss: 0.0004450616834219545\n",
      "Batch 8637/14851, Loss: 0.0006414116360247135\n",
      "Batch 8638/14851, Loss: 0.0012993622804060578\n",
      "Batch 8639/14851, Loss: 0.0006151069537736475\n",
      "Batch 8640/14851, Loss: 0.0016645751893520355\n",
      "Batch 8641/14851, Loss: 0.001312199980020523\n",
      "Batch 8642/14851, Loss: 0.0013616556534543633\n",
      "Batch 8643/14851, Loss: 0.01989300735294819\n",
      "Batch 8644/14851, Loss: 0.0008542102877981961\n",
      "Batch 8645/14851, Loss: 0.0038789843674749136\n",
      "Batch 8646/14851, Loss: 0.0006557069718837738\n",
      "Batch 8647/14851, Loss: 0.006132680922746658\n",
      "Batch 8648/14851, Loss: 0.001276595052331686\n",
      "Batch 8649/14851, Loss: 0.015752872452139854\n",
      "Batch 8650/14851, Loss: 0.014415724202990532\n",
      "Batch 8651/14851, Loss: 0.029688308015465736\n",
      "Batch 8652/14851, Loss: 0.0026370605919510126\n",
      "Batch 8653/14851, Loss: 0.006129617337137461\n",
      "Batch 8654/14851, Loss: 0.0011448285076767206\n",
      "Batch 8655/14851, Loss: 0.013637606985867023\n",
      "Batch 8656/14851, Loss: 0.002386145293712616\n",
      "Batch 8657/14851, Loss: 0.003624500008299947\n",
      "Batch 8658/14851, Loss: 0.001141903456300497\n",
      "Batch 8659/14851, Loss: 0.00455109030008316\n",
      "Batch 8660/14851, Loss: 0.012241733260452747\n",
      "Batch 8661/14851, Loss: 0.0018052136292681098\n",
      "Batch 8662/14851, Loss: 0.0005220025777816772\n",
      "Batch 8663/14851, Loss: 0.020322861149907112\n",
      "Batch 8664/14851, Loss: 0.0050928774289786816\n",
      "Batch 8665/14851, Loss: 0.039283040910959244\n",
      "Batch 8666/14851, Loss: 0.0005789442220702767\n",
      "Batch 8667/14851, Loss: 0.0004042262735310942\n",
      "Batch 8668/14851, Loss: 0.001860248507000506\n",
      "Batch 8669/14851, Loss: 0.04729609563946724\n",
      "Batch 8670/14851, Loss: 0.004193377215415239\n",
      "Batch 8671/14851, Loss: 0.02429860644042492\n",
      "Batch 8672/14851, Loss: 0.02632228285074234\n",
      "Batch 8673/14851, Loss: 0.01713082753121853\n",
      "Batch 8674/14851, Loss: 0.016758279874920845\n",
      "Batch 8675/14851, Loss: 0.035080838948488235\n",
      "Batch 8676/14851, Loss: 0.0022443048655986786\n",
      "Batch 8677/14851, Loss: 0.0013249146286398172\n",
      "Batch 8678/14851, Loss: 0.000969666987657547\n",
      "Batch 8679/14851, Loss: 0.0005011893808841705\n",
      "Batch 8680/14851, Loss: 0.009167942218482494\n",
      "Batch 8681/14851, Loss: 0.0012141229817643762\n",
      "Batch 8682/14851, Loss: 0.020809968933463097\n",
      "Batch 8683/14851, Loss: 0.0029927354771643877\n",
      "Batch 8684/14851, Loss: 0.029331589117646217\n",
      "Batch 8685/14851, Loss: 0.0012951580574736\n",
      "Batch 8686/14851, Loss: 0.00027890747878700495\n",
      "Batch 8687/14851, Loss: 0.05828840658068657\n",
      "Batch 8688/14851, Loss: 0.05704404413700104\n",
      "Batch 8689/14851, Loss: 0.002170601161196828\n",
      "Batch 8690/14851, Loss: 0.0038284356705844402\n",
      "Batch 8691/14851, Loss: 0.00019627313304226846\n",
      "Batch 8692/14851, Loss: 0.0008524938020855188\n",
      "Batch 8693/14851, Loss: 0.0020769736729562283\n",
      "Batch 8694/14851, Loss: 0.0007637987728230655\n",
      "Batch 8695/14851, Loss: 0.026648493483662605\n",
      "Batch 8696/14851, Loss: 0.002144609810784459\n",
      "Batch 8697/14851, Loss: 0.019747093319892883\n",
      "Batch 8698/14851, Loss: 0.01079038716852665\n",
      "Batch 8699/14851, Loss: 0.004027216229587793\n",
      "Batch 8700/14851, Loss: 0.001112271100282669\n",
      "Batch 8701/14851, Loss: 0.0033600677270442247\n",
      "Batch 8702/14851, Loss: 0.03383737429976463\n",
      "Batch 8703/14851, Loss: 0.0027395763900130987\n",
      "Batch 8704/14851, Loss: 0.00391816021874547\n",
      "Batch 8705/14851, Loss: 0.0016379437875002623\n",
      "Batch 8706/14851, Loss: 0.0024526428896933794\n",
      "Batch 8707/14851, Loss: 0.003515331307426095\n",
      "Batch 8708/14851, Loss: 0.029605478048324585\n",
      "Batch 8709/14851, Loss: 0.0007573148468509316\n",
      "Batch 8710/14851, Loss: 0.00021758426737505943\n",
      "Batch 8711/14851, Loss: 0.0014120307750999928\n",
      "Batch 8712/14851, Loss: 0.001135053695179522\n",
      "Batch 8713/14851, Loss: 0.0016370924422517419\n",
      "Batch 8714/14851, Loss: 0.007484517991542816\n",
      "Batch 8715/14851, Loss: 0.0026401628274470568\n",
      "Batch 8716/14851, Loss: 0.04400620982050896\n",
      "Batch 8717/14851, Loss: 0.049355585128068924\n",
      "Batch 8718/14851, Loss: 0.016210269182920456\n",
      "Batch 8719/14851, Loss: 0.012728303670883179\n",
      "Batch 8720/14851, Loss: 0.0013283217558637261\n",
      "Batch 8721/14851, Loss: 0.0006089852540753782\n",
      "Batch 8722/14851, Loss: 0.0012256315676495433\n",
      "Batch 8723/14851, Loss: 0.00029161301790736616\n",
      "Batch 8724/14851, Loss: 0.008105078712105751\n",
      "Batch 8725/14851, Loss: 0.0006866169278509915\n",
      "Batch 8726/14851, Loss: 0.022433655336499214\n",
      "Batch 8727/14851, Loss: 0.004657653160393238\n",
      "Batch 8728/14851, Loss: 0.0064233290031552315\n",
      "Batch 8729/14851, Loss: 0.019931331276893616\n",
      "Batch 8730/14851, Loss: 0.019402040168642998\n",
      "Batch 8731/14851, Loss: 0.0025171313900500536\n",
      "Batch 8732/14851, Loss: 0.00035625198506750166\n",
      "Batch 8733/14851, Loss: 0.00763176754117012\n",
      "Batch 8734/14851, Loss: 0.0006882771849632263\n",
      "Batch 8735/14851, Loss: 0.0009588673710823059\n",
      "Batch 8736/14851, Loss: 0.0011831240262836218\n",
      "Batch 8737/14851, Loss: 0.0373111292719841\n",
      "Batch 8738/14851, Loss: 0.00024395808577537537\n",
      "Batch 8739/14851, Loss: 0.0066184792667627335\n",
      "Batch 8740/14851, Loss: 0.02890181541442871\n",
      "Batch 8741/14851, Loss: 0.018567929044365883\n",
      "Batch 8742/14851, Loss: 0.007337714545428753\n",
      "Batch 8743/14851, Loss: 0.05283301696181297\n",
      "Batch 8744/14851, Loss: 0.0012430613860487938\n",
      "Batch 8745/14851, Loss: 0.0010195672512054443\n",
      "Batch 8746/14851, Loss: 0.027110770344734192\n",
      "Batch 8747/14851, Loss: 0.003704416798427701\n",
      "Batch 8748/14851, Loss: 0.00019107137632090598\n",
      "Batch 8749/14851, Loss: 0.002036041347309947\n",
      "Batch 8750/14851, Loss: 0.02070687897503376\n",
      "Batch 8751/14851, Loss: 0.0032760214526206255\n",
      "Batch 8752/14851, Loss: 0.01931297406554222\n",
      "Batch 8753/14851, Loss: 0.013672013767063618\n",
      "Batch 8754/14851, Loss: 0.008480777032673359\n",
      "Batch 8755/14851, Loss: 0.03558097407221794\n",
      "Batch 8756/14851, Loss: 0.008813031017780304\n",
      "Batch 8757/14851, Loss: 0.00871000625193119\n",
      "Batch 8758/14851, Loss: 0.003348899306729436\n",
      "Batch 8759/14851, Loss: 0.0030535857658833265\n",
      "Batch 8760/14851, Loss: 0.0006374676595441997\n",
      "Batch 8761/14851, Loss: 0.0018980632303282619\n",
      "Batch 8762/14851, Loss: 0.0015416344394907355\n",
      "Batch 8763/14851, Loss: 0.002017022343352437\n",
      "Batch 8764/14851, Loss: 0.0017517220694571733\n",
      "Batch 8765/14851, Loss: 0.0018967166543006897\n",
      "Batch 8766/14851, Loss: 0.003788571571931243\n",
      "Batch 8767/14851, Loss: 0.037249982357025146\n",
      "Batch 8768/14851, Loss: 0.011324740014970303\n",
      "Batch 8769/14851, Loss: 0.0005472165648825467\n",
      "Batch 8770/14851, Loss: 0.00013044849038124084\n",
      "Batch 8771/14851, Loss: 0.0012100955937057734\n",
      "Batch 8772/14851, Loss: 0.03453327342867851\n",
      "Batch 8773/14851, Loss: 0.03239325061440468\n",
      "Batch 8774/14851, Loss: 7.642433047294617e-05\n",
      "Batch 8775/14851, Loss: 0.013365145772695541\n",
      "Batch 8776/14851, Loss: 0.0014117248356342316\n",
      "Batch 8777/14851, Loss: 0.0004863850772380829\n",
      "Batch 8778/14851, Loss: 0.002513307146728039\n",
      "Batch 8779/14851, Loss: 0.03473594784736633\n",
      "Batch 8780/14851, Loss: 0.021263325586915016\n",
      "Batch 8781/14851, Loss: 0.00030584060004912317\n",
      "Batch 8782/14851, Loss: 0.003907543141394854\n",
      "Batch 8783/14851, Loss: 0.008430049754679203\n",
      "Batch 8784/14851, Loss: 0.000796523701865226\n",
      "Batch 8785/14851, Loss: 0.0070754773914813995\n",
      "Batch 8786/14851, Loss: 0.0012673852033913136\n",
      "Batch 8787/14851, Loss: 0.006224455777555704\n",
      "Batch 8788/14851, Loss: 0.002957731019705534\n",
      "Batch 8789/14851, Loss: 0.008625157177448273\n",
      "Batch 8790/14851, Loss: 0.001881886157207191\n",
      "Batch 8791/14851, Loss: 0.0001754450349835679\n",
      "Batch 8792/14851, Loss: 0.0003355101216584444\n",
      "Batch 8793/14851, Loss: 0.005469538737088442\n",
      "Batch 8794/14851, Loss: 0.038248058408498764\n",
      "Batch 8795/14851, Loss: 0.0008027131552807987\n",
      "Batch 8796/14851, Loss: 0.018091920763254166\n",
      "Batch 8797/14851, Loss: 0.0050368239171803\n",
      "Batch 8798/14851, Loss: 0.005342043936252594\n",
      "Batch 8799/14851, Loss: 0.04707407206296921\n",
      "Batch 8800/14851, Loss: 0.002263207919895649\n",
      "Batch 8801/14851, Loss: 0.012297076173126698\n",
      "Batch 8802/14851, Loss: 0.025461610406637192\n",
      "Batch 8803/14851, Loss: 0.001100560068152845\n",
      "Batch 8804/14851, Loss: 0.00295412284322083\n",
      "Batch 8805/14851, Loss: 0.0016843117773532867\n",
      "Batch 8806/14851, Loss: 0.008377300575375557\n",
      "Batch 8807/14851, Loss: 0.006262135226279497\n",
      "Batch 8808/14851, Loss: 0.005000587552785873\n",
      "Batch 8809/14851, Loss: 0.021614111959934235\n",
      "Batch 8810/14851, Loss: 0.009769639931619167\n",
      "Batch 8811/14851, Loss: 0.0011002296814695\n",
      "Batch 8812/14851, Loss: 0.02212558127939701\n",
      "Batch 8813/14851, Loss: 0.0075207860209047794\n",
      "Batch 8814/14851, Loss: 0.0007644093711860478\n",
      "Batch 8815/14851, Loss: 0.006531116086989641\n",
      "Batch 8816/14851, Loss: 0.01331897359341383\n",
      "Batch 8817/14851, Loss: 0.007003700826317072\n",
      "Batch 8818/14851, Loss: 0.00040578594780527055\n",
      "Batch 8819/14851, Loss: 0.003129978897050023\n",
      "Batch 8820/14851, Loss: 0.0007500430219806731\n",
      "Batch 8821/14851, Loss: 0.005136480089277029\n",
      "Batch 8822/14851, Loss: 0.0016696485690772533\n",
      "Batch 8823/14851, Loss: 0.007489559706300497\n",
      "Batch 8824/14851, Loss: 0.0012273144675418735\n",
      "Batch 8825/14851, Loss: 0.02639663778245449\n",
      "Batch 8826/14851, Loss: 0.005428370088338852\n",
      "Batch 8827/14851, Loss: 0.023834357038140297\n",
      "Batch 8828/14851, Loss: 0.0429760217666626\n",
      "Batch 8829/14851, Loss: 0.006864103022962809\n",
      "Batch 8830/14851, Loss: 0.0006775173242203891\n",
      "Batch 8831/14851, Loss: 0.0008134196395985782\n",
      "Batch 8832/14851, Loss: 0.004628419876098633\n",
      "Batch 8833/14851, Loss: 0.01403720211237669\n",
      "Batch 8834/14851, Loss: 0.0005998015403747559\n",
      "Batch 8835/14851, Loss: 0.0037473496049642563\n",
      "Batch 8836/14851, Loss: 0.001961899222806096\n",
      "Batch 8837/14851, Loss: 0.00012210880231577903\n",
      "Batch 8838/14851, Loss: 0.0028156638145446777\n",
      "Batch 8839/14851, Loss: 0.002608676441013813\n",
      "Batch 8840/14851, Loss: 0.004286209587007761\n",
      "Batch 8841/14851, Loss: 0.02442988194525242\n",
      "Batch 8842/14851, Loss: 0.0036815155763179064\n",
      "Batch 8843/14851, Loss: 0.0024559996090829372\n",
      "Batch 8844/14851, Loss: 0.0004177847586106509\n",
      "Batch 8845/14851, Loss: 0.00241701933555305\n",
      "Batch 8846/14851, Loss: 0.0016909092664718628\n",
      "Batch 8847/14851, Loss: 0.007739919703453779\n",
      "Batch 8848/14851, Loss: 0.02955101989209652\n",
      "Batch 8849/14851, Loss: 0.0027672818396240473\n",
      "Batch 8850/14851, Loss: 0.024942414835095406\n",
      "Batch 8851/14851, Loss: 0.0016901144990697503\n",
      "Batch 8852/14851, Loss: 0.002059136750176549\n",
      "Batch 8853/14851, Loss: 0.000817418098449707\n",
      "Batch 8854/14851, Loss: 0.00020214542746543884\n",
      "Batch 8855/14851, Loss: 0.041542086750268936\n",
      "Batch 8856/14851, Loss: 0.004294726997613907\n",
      "Batch 8857/14851, Loss: 0.0005318485200405121\n",
      "Batch 8858/14851, Loss: 0.0019832339603453875\n",
      "Batch 8859/14851, Loss: 0.0013222906272858381\n",
      "Batch 8860/14851, Loss: 0.0005862551624886692\n",
      "Batch 8861/14851, Loss: 0.0007347700302489102\n",
      "Batch 8862/14851, Loss: 0.03165655955672264\n",
      "Batch 8863/14851, Loss: 0.00041076415800489485\n",
      "Batch 8864/14851, Loss: 0.0020928741432726383\n",
      "Batch 8865/14851, Loss: 0.00301546324044466\n",
      "Batch 8866/14851, Loss: 0.00038538873195648193\n",
      "Batch 8867/14851, Loss: 0.030673237517476082\n",
      "Batch 8868/14851, Loss: 0.005201918538659811\n",
      "Batch 8869/14851, Loss: 0.005689406301826239\n",
      "Batch 8870/14851, Loss: 0.025447865948081017\n",
      "Batch 8871/14851, Loss: 0.0004082392842974514\n",
      "Batch 8872/14851, Loss: 0.0010402662446722388\n",
      "Batch 8873/14851, Loss: 0.011099672876298428\n",
      "Batch 8874/14851, Loss: 0.00017877384379971772\n",
      "Batch 8875/14851, Loss: 0.0018088245997205377\n",
      "Batch 8876/14851, Loss: 0.0015765317948535085\n",
      "Batch 8877/14851, Loss: 0.02852407470345497\n",
      "Batch 8878/14851, Loss: 0.0006310840253718197\n",
      "Batch 8879/14851, Loss: 0.00014774873852729797\n",
      "Batch 8880/14851, Loss: 0.029834719374775887\n",
      "Batch 8881/14851, Loss: 0.0004953021998517215\n",
      "Batch 8882/14851, Loss: 0.06155648082494736\n",
      "Batch 8883/14851, Loss: 0.00511733815073967\n",
      "Batch 8884/14851, Loss: 0.004910413641482592\n",
      "Batch 8885/14851, Loss: 0.0004813415580429137\n",
      "Batch 8886/14851, Loss: 0.0003940453752875328\n",
      "Batch 8887/14851, Loss: 0.00418836809694767\n",
      "Batch 8888/14851, Loss: 0.008284538052976131\n",
      "Batch 8889/14851, Loss: 0.004307436756789684\n",
      "Batch 8890/14851, Loss: 0.0008435696363449097\n",
      "Batch 8891/14851, Loss: 0.015673412010073662\n",
      "Batch 8892/14851, Loss: 0.0007449363474734128\n",
      "Batch 8893/14851, Loss: 0.00022515903401654214\n",
      "Batch 8894/14851, Loss: 0.011589533649384975\n",
      "Batch 8895/14851, Loss: 0.033271826803684235\n",
      "Batch 8896/14851, Loss: 0.002728822408244014\n",
      "Batch 8897/14851, Loss: 0.020775776356458664\n",
      "Batch 8898/14851, Loss: 0.03567974641919136\n",
      "Batch 8899/14851, Loss: 0.0018945103511214256\n",
      "Batch 8900/14851, Loss: 0.03245530650019646\n",
      "Batch 8901/14851, Loss: 0.0051782261580228806\n",
      "Batch 8902/14851, Loss: 0.054336439818143845\n",
      "Batch 8903/14851, Loss: 0.0014087733579799533\n",
      "Batch 8904/14851, Loss: 0.0013704242883250117\n",
      "Batch 8905/14851, Loss: 0.0006800529663451016\n",
      "Batch 8906/14851, Loss: 0.0034082469064742327\n",
      "Batch 8907/14851, Loss: 0.004471421241760254\n",
      "Batch 8908/14851, Loss: 0.01934000849723816\n",
      "Batch 8909/14851, Loss: 0.006018154323101044\n",
      "Batch 8910/14851, Loss: 0.01426353957504034\n",
      "Batch 8911/14851, Loss: 0.01019627507776022\n",
      "Batch 8912/14851, Loss: 0.06398217380046844\n",
      "Batch 8913/14851, Loss: 0.0029635813552886248\n",
      "Batch 8914/14851, Loss: 0.027635522186756134\n",
      "Batch 8915/14851, Loss: 0.002397960750386119\n",
      "Batch 8916/14851, Loss: 0.019943226128816605\n",
      "Batch 8917/14851, Loss: 0.0011824175016954541\n",
      "Batch 8918/14851, Loss: 0.0004389653622638434\n",
      "Batch 8919/14851, Loss: 0.025803299620747566\n",
      "Batch 8920/14851, Loss: 0.0035260915756225586\n",
      "Batch 8921/14851, Loss: 0.011626370251178741\n",
      "Batch 8922/14851, Loss: 0.0010909214615821838\n",
      "Batch 8923/14851, Loss: 0.0009288614382967353\n",
      "Batch 8924/14851, Loss: 0.0072724828496575356\n",
      "Batch 8925/14851, Loss: 0.004975033458322287\n",
      "Batch 8926/14851, Loss: 0.008726486936211586\n",
      "Batch 8927/14851, Loss: 0.0013903012732043862\n",
      "Batch 8928/14851, Loss: 0.00885605625808239\n",
      "Batch 8929/14851, Loss: 0.0004939631908200681\n",
      "Batch 8930/14851, Loss: 0.013137317262589931\n",
      "Batch 8931/14851, Loss: 0.0010871626436710358\n",
      "Batch 8932/14851, Loss: 0.004926139954477549\n",
      "Batch 8933/14851, Loss: 0.006815938279032707\n",
      "Batch 8934/14851, Loss: 0.0012115412391722202\n",
      "Batch 8935/14851, Loss: 0.005862572230398655\n",
      "Batch 8936/14851, Loss: 0.001234447001479566\n",
      "Batch 8937/14851, Loss: 0.015672994777560234\n",
      "Batch 8938/14851, Loss: 0.00467334920540452\n",
      "Batch 8939/14851, Loss: 0.030354876071214676\n",
      "Batch 8940/14851, Loss: 0.023967629298567772\n",
      "Batch 8941/14851, Loss: 0.015343450009822845\n",
      "Batch 8942/14851, Loss: 0.008435389026999474\n",
      "Batch 8943/14851, Loss: 0.030409513041377068\n",
      "Batch 8944/14851, Loss: 0.062368012964725494\n",
      "Batch 8945/14851, Loss: 0.03571527451276779\n",
      "Batch 8946/14851, Loss: 0.006025326903909445\n",
      "Batch 8947/14851, Loss: 0.006848277058452368\n",
      "Batch 8948/14851, Loss: 0.02270759455859661\n",
      "Batch 8949/14851, Loss: 0.001571228145621717\n",
      "Batch 8950/14851, Loss: 0.004568547010421753\n",
      "Batch 8951/14851, Loss: 0.04574095085263252\n",
      "Batch 8952/14851, Loss: 0.0011880187084898353\n",
      "Batch 8953/14851, Loss: 0.0025265298318117857\n",
      "Batch 8954/14851, Loss: 0.01098247617483139\n",
      "Batch 8955/14851, Loss: 0.03633205592632294\n",
      "Batch 8956/14851, Loss: 0.007109226193279028\n",
      "Batch 8957/14851, Loss: 0.0074752746149897575\n",
      "Batch 8958/14851, Loss: 0.008380608633160591\n",
      "Batch 8959/14851, Loss: 0.0006972369737923145\n",
      "Batch 8960/14851, Loss: 0.0021458070259541273\n",
      "Batch 8961/14851, Loss: 0.0021407913882285357\n",
      "Batch 8962/14851, Loss: 0.018519695848226547\n",
      "Batch 8963/14851, Loss: 0.00763903371989727\n",
      "Batch 8964/14851, Loss: 0.019855128601193428\n",
      "Batch 8965/14851, Loss: 0.0010018280008807778\n",
      "Batch 8966/14851, Loss: 0.010793905705213547\n",
      "Batch 8967/14851, Loss: 0.013769962824881077\n",
      "Batch 8968/14851, Loss: 0.004957711789757013\n",
      "Batch 8969/14851, Loss: 0.01783771440386772\n",
      "Batch 8970/14851, Loss: 0.0023354466538876295\n",
      "Batch 8971/14851, Loss: 0.0008727274835109711\n",
      "Batch 8972/14851, Loss: 0.01851959340274334\n",
      "Batch 8973/14851, Loss: 0.0035628709010779858\n",
      "Batch 8974/14851, Loss: 0.0006349931354634464\n",
      "Batch 8975/14851, Loss: 0.00024746358394622803\n",
      "Batch 8976/14851, Loss: 0.00400799885392189\n",
      "Batch 8977/14851, Loss: 0.0015444904565811157\n",
      "Batch 8978/14851, Loss: 0.0003015659749507904\n",
      "Batch 8979/14851, Loss: 0.0005210128729231656\n",
      "Batch 8980/14851, Loss: 0.028277652338147163\n",
      "Batch 8981/14851, Loss: 0.0024240873754024506\n",
      "Batch 8982/14851, Loss: 0.011283346451818943\n",
      "Batch 8983/14851, Loss: 0.005901881493628025\n",
      "Batch 8984/14851, Loss: 0.021182741969823837\n",
      "Batch 8985/14851, Loss: 0.0012073218822479248\n",
      "Batch 8986/14851, Loss: 0.002715056063607335\n",
      "Batch 8987/14851, Loss: 0.008831672370433807\n",
      "Batch 8988/14851, Loss: 0.047569822520017624\n",
      "Batch 8989/14851, Loss: 0.0012206273386254907\n",
      "Batch 8990/14851, Loss: 0.015296357683837414\n",
      "Batch 8991/14851, Loss: 0.011424212716519833\n",
      "Batch 8992/14851, Loss: 0.0002534066734369844\n",
      "Batch 8993/14851, Loss: 0.008458894677460194\n",
      "Batch 8994/14851, Loss: 0.0006969869136810303\n",
      "Batch 8995/14851, Loss: 0.00011876970529556274\n",
      "Batch 8996/14851, Loss: 0.02381913922727108\n",
      "Batch 8997/14851, Loss: 0.002498397370800376\n",
      "Batch 8998/14851, Loss: 0.00027899941778741777\n",
      "Batch 8999/14851, Loss: 0.017744483426213264\n",
      "Batch 9000/14851, Loss: 6.649395072599873e-05\n",
      "Batch 9001/14851, Loss: 0.0011955747613683343\n",
      "Batch 9002/14851, Loss: 0.0013826830545440316\n",
      "Batch 9003/14851, Loss: 0.011835513636469841\n",
      "Batch 9004/14851, Loss: 0.06515272706747055\n",
      "Batch 9005/14851, Loss: 0.004273307044059038\n",
      "Batch 9006/14851, Loss: 0.008460205048322678\n",
      "Batch 9007/14851, Loss: 0.0021438002586364746\n",
      "Batch 9008/14851, Loss: 0.0001971783785847947\n",
      "Batch 9009/14851, Loss: 0.0008380432846024632\n",
      "Batch 9010/14851, Loss: 0.001013124012388289\n",
      "Batch 9011/14851, Loss: 0.0011535933008417487\n",
      "Batch 9012/14851, Loss: 0.001766858040355146\n",
      "Batch 9013/14851, Loss: 0.005447001662105322\n",
      "Batch 9014/14851, Loss: 0.015509740449488163\n",
      "Batch 9015/14851, Loss: 0.005929235368967056\n",
      "Batch 9016/14851, Loss: 0.001826997846364975\n",
      "Batch 9017/14851, Loss: 0.004701992031186819\n",
      "Batch 9018/14851, Loss: 0.011573679745197296\n",
      "Batch 9019/14851, Loss: 0.008454065769910812\n",
      "Batch 9020/14851, Loss: 0.0044190348125994205\n",
      "Batch 9021/14851, Loss: 0.011934835463762283\n",
      "Batch 9022/14851, Loss: 0.0013435253640636802\n",
      "Batch 9023/14851, Loss: 0.015298070386052132\n",
      "Batch 9024/14851, Loss: 0.007089887745678425\n",
      "Batch 9025/14851, Loss: 0.0002938956022262573\n",
      "Batch 9026/14851, Loss: 0.0001808678061934188\n",
      "Batch 9027/14851, Loss: 7.21116884960793e-05\n",
      "Batch 9028/14851, Loss: 0.006030794233083725\n",
      "Batch 9029/14851, Loss: 0.0015065406914800406\n",
      "Batch 9030/14851, Loss: 0.013522262685000896\n",
      "Batch 9031/14851, Loss: 0.019186517223715782\n",
      "Batch 9032/14851, Loss: 3.1147152185440063e-05\n",
      "Batch 9033/14851, Loss: 0.00031611943268217146\n",
      "Batch 9034/14851, Loss: 0.0005114649538882077\n",
      "Batch 9035/14851, Loss: 0.002770946593955159\n",
      "Batch 9036/14851, Loss: 0.00024208053946495056\n",
      "Batch 9037/14851, Loss: 0.0007204115390777588\n",
      "Batch 9038/14851, Loss: 0.0001566794962855056\n",
      "Batch 9039/14851, Loss: 0.01064336858689785\n",
      "Batch 9040/14851, Loss: 0.0011233389377593994\n",
      "Batch 9041/14851, Loss: 0.003460606560111046\n",
      "Batch 9042/14851, Loss: 0.005376217421144247\n",
      "Batch 9043/14851, Loss: 0.022576581686735153\n",
      "Batch 9044/14851, Loss: 0.04040359705686569\n",
      "Batch 9045/14851, Loss: 0.03857799619436264\n",
      "Batch 9046/14851, Loss: 0.032558687031269073\n",
      "Batch 9047/14851, Loss: 0.005124737974256277\n",
      "Batch 9048/14851, Loss: 0.007714425679296255\n",
      "Batch 9049/14851, Loss: 0.019515618681907654\n",
      "Batch 9050/14851, Loss: 0.0006210530991666019\n",
      "Batch 9051/14851, Loss: 0.0005950735649093986\n",
      "Batch 9052/14851, Loss: 0.0017995958914980292\n",
      "Batch 9053/14851, Loss: 0.015135090798139572\n",
      "Batch 9054/14851, Loss: 0.0029483723919838667\n",
      "Batch 9055/14851, Loss: 0.012446227483451366\n",
      "Batch 9056/14851, Loss: 0.021174713969230652\n",
      "Batch 9057/14851, Loss: 0.012789182364940643\n",
      "Batch 9058/14851, Loss: 0.0010379701852798462\n",
      "Batch 9059/14851, Loss: 0.00029036900377832353\n",
      "Batch 9060/14851, Loss: 0.004879073705524206\n",
      "Batch 9061/14851, Loss: 0.008090346120297909\n",
      "Batch 9062/14851, Loss: 0.0062717776745557785\n",
      "Batch 9063/14851, Loss: 0.028973504900932312\n",
      "Batch 9064/14851, Loss: 0.005269641522318125\n",
      "Batch 9065/14851, Loss: 0.0005003459518775344\n",
      "Batch 9066/14851, Loss: 0.07451309263706207\n",
      "Batch 9067/14851, Loss: 0.0067166429944336414\n",
      "Batch 9068/14851, Loss: 0.02003847062587738\n",
      "Batch 9069/14851, Loss: 0.002109558554366231\n",
      "Batch 9070/14851, Loss: 0.003262445330619812\n",
      "Batch 9071/14851, Loss: 0.002071100054308772\n",
      "Batch 9072/14851, Loss: 0.014462537132203579\n",
      "Batch 9073/14851, Loss: 0.0007282458245754242\n",
      "Batch 9074/14851, Loss: 0.0012326637515798211\n",
      "Batch 9075/14851, Loss: 0.00029820576310157776\n",
      "Batch 9076/14851, Loss: 0.016589684411883354\n",
      "Batch 9077/14851, Loss: 0.0007669560727663338\n",
      "Batch 9078/14851, Loss: 0.0017316477606073022\n",
      "Batch 9079/14851, Loss: 0.0013810619711875916\n",
      "Batch 9080/14851, Loss: 3.280739110778086e-05\n",
      "Batch 9081/14851, Loss: 0.002764105796813965\n",
      "Batch 9082/14851, Loss: 0.0011716593289747834\n",
      "Batch 9083/14851, Loss: 0.0008400598890148103\n",
      "Batch 9084/14851, Loss: 0.001108746975660324\n",
      "Batch 9085/14851, Loss: 0.015279039740562439\n",
      "Batch 9086/14851, Loss: 0.0041217925027012825\n",
      "Batch 9087/14851, Loss: 0.020148033276200294\n",
      "Batch 9088/14851, Loss: 0.0010007207747548819\n",
      "Batch 9089/14851, Loss: 0.0002168715000152588\n",
      "Batch 9090/14851, Loss: 0.0002977574768010527\n",
      "Batch 9091/14851, Loss: 0.012763711623847485\n",
      "Batch 9092/14851, Loss: 0.0039300150237977505\n",
      "Batch 9093/14851, Loss: 0.02150597982108593\n",
      "Batch 9094/14851, Loss: 0.0006655168836005032\n",
      "Batch 9095/14851, Loss: 0.00333433598279953\n",
      "Batch 9096/14851, Loss: 0.0456264354288578\n",
      "Batch 9097/14851, Loss: 0.011075946502387524\n",
      "Batch 9098/14851, Loss: 6.183484947541729e-05\n",
      "Batch 9099/14851, Loss: 0.009644825011491776\n",
      "Batch 9100/14851, Loss: 0.0069634090177714825\n",
      "Batch 9101/14851, Loss: 0.0017069224268198013\n",
      "Batch 9102/14851, Loss: 0.016085287556052208\n",
      "Batch 9103/14851, Loss: 0.0007681511342525482\n",
      "Batch 9104/14851, Loss: 0.0035986502189189196\n",
      "Batch 9105/14851, Loss: 0.0031672802288085222\n",
      "Batch 9106/14851, Loss: 0.026810018345713615\n",
      "Batch 9107/14851, Loss: 0.0066886707209050655\n",
      "Batch 9108/14851, Loss: 0.0008652562974020839\n",
      "Batch 9109/14851, Loss: 0.00012717768549919128\n",
      "Batch 9110/14851, Loss: 0.02161146141588688\n",
      "Batch 9111/14851, Loss: 0.00887003168463707\n",
      "Batch 9112/14851, Loss: 0.007003669161349535\n",
      "Batch 9113/14851, Loss: 0.0034167503472417593\n",
      "Batch 9114/14851, Loss: 0.0008457675576210022\n",
      "Batch 9115/14851, Loss: 0.002107995329424739\n",
      "Batch 9116/14851, Loss: 0.001669410616159439\n",
      "Batch 9117/14851, Loss: 0.06357107311487198\n",
      "Batch 9118/14851, Loss: 0.10291800647974014\n",
      "Batch 9119/14851, Loss: 0.007761318236589432\n",
      "Batch 9120/14851, Loss: 0.0031485643703490496\n",
      "Batch 9121/14851, Loss: 0.001841486431658268\n",
      "Batch 9122/14851, Loss: 0.0016986988484859467\n",
      "Batch 9123/14851, Loss: 0.0027357947546988726\n",
      "Batch 9124/14851, Loss: 0.023195991292595863\n",
      "Batch 9125/14851, Loss: 0.00886814296245575\n",
      "Batch 9126/14851, Loss: 0.0009049810469150543\n",
      "Batch 9127/14851, Loss: 0.028974439948797226\n",
      "Batch 9128/14851, Loss: 0.00409713014960289\n",
      "Batch 9129/14851, Loss: 0.00016001488256733865\n",
      "Batch 9130/14851, Loss: 0.0017949737375602126\n",
      "Batch 9131/14851, Loss: 0.0003903483448084444\n",
      "Batch 9132/14851, Loss: 0.0006589193944819272\n",
      "Batch 9133/14851, Loss: 0.001635634689591825\n",
      "Batch 9134/14851, Loss: 0.0062447539530694485\n",
      "Batch 9135/14851, Loss: 0.011383424513041973\n",
      "Batch 9136/14851, Loss: 0.010710400529205799\n",
      "Batch 9137/14851, Loss: 0.023220153525471687\n",
      "Batch 9138/14851, Loss: 0.003389527555555105\n",
      "Batch 9139/14851, Loss: 0.008969313465058804\n",
      "Batch 9140/14851, Loss: 0.0014201700687408447\n",
      "Batch 9141/14851, Loss: 0.06468886882066727\n",
      "Batch 9142/14851, Loss: 0.00101538747549057\n",
      "Batch 9143/14851, Loss: 0.008619209751486778\n",
      "Batch 9144/14851, Loss: 0.0021468065679073334\n",
      "Batch 9145/14851, Loss: 0.0058219353668391705\n",
      "Batch 9146/14851, Loss: 0.0010406901128590107\n",
      "Batch 9147/14851, Loss: 0.006513380911201239\n",
      "Batch 9148/14851, Loss: 0.004276122897863388\n",
      "Batch 9149/14851, Loss: 0.028136655688285828\n",
      "Batch 9150/14851, Loss: 0.019548315554857254\n",
      "Batch 9151/14851, Loss: 0.0007382917101494968\n",
      "Batch 9152/14851, Loss: 0.04583034664392471\n",
      "Batch 9153/14851, Loss: 0.04231046885251999\n",
      "Batch 9154/14851, Loss: 0.006939861923456192\n",
      "Batch 9155/14851, Loss: 0.022928675636649132\n",
      "Batch 9156/14851, Loss: 0.019913414493203163\n",
      "Batch 9157/14851, Loss: 0.0018823530990630388\n",
      "Batch 9158/14851, Loss: 0.0017984993755817413\n",
      "Batch 9159/14851, Loss: 0.0082827378064394\n",
      "Batch 9160/14851, Loss: 0.03525407984852791\n",
      "Batch 9161/14851, Loss: 0.019549377262592316\n",
      "Batch 9162/14851, Loss: 0.0015331953763961792\n",
      "Batch 9163/14851, Loss: 0.00607261061668396\n",
      "Batch 9164/14851, Loss: 0.051205094903707504\n",
      "Batch 9165/14851, Loss: 0.0038604214787483215\n",
      "Batch 9166/14851, Loss: 0.0013892253628000617\n",
      "Batch 9167/14851, Loss: 0.0031516116578131914\n",
      "Batch 9168/14851, Loss: 0.0023786600213497877\n",
      "Batch 9169/14851, Loss: 0.003407453652471304\n",
      "Batch 9170/14851, Loss: 0.0012580528855323792\n",
      "Batch 9171/14851, Loss: 0.004652417730540037\n",
      "Batch 9172/14851, Loss: 0.005965198390185833\n",
      "Batch 9173/14851, Loss: 0.0008299599285237491\n",
      "Batch 9174/14851, Loss: 0.013837636448442936\n",
      "Batch 9175/14851, Loss: 0.007096295244991779\n",
      "Batch 9176/14851, Loss: 0.09233342856168747\n",
      "Batch 9177/14851, Loss: 0.004525072872638702\n",
      "Batch 9178/14851, Loss: 0.0005696788430213928\n",
      "Batch 9179/14851, Loss: 0.000618980557192117\n",
      "Batch 9180/14851, Loss: 0.0281545240432024\n",
      "Batch 9181/14851, Loss: 0.005772846285253763\n",
      "Batch 9182/14851, Loss: 0.0009746942087076604\n",
      "Batch 9183/14851, Loss: 0.08973667025566101\n",
      "Batch 9184/14851, Loss: 0.03204092010855675\n",
      "Batch 9185/14851, Loss: 0.07222704589366913\n",
      "Batch 9186/14851, Loss: 0.01893341727554798\n",
      "Batch 9187/14851, Loss: 0.00679204473271966\n",
      "Batch 9188/14851, Loss: 0.0034799266140908003\n",
      "Batch 9189/14851, Loss: 0.005811964627355337\n",
      "Batch 9190/14851, Loss: 0.0014149656053632498\n",
      "Batch 9191/14851, Loss: 0.0029996857047080994\n",
      "Batch 9192/14851, Loss: 0.0029981061816215515\n",
      "Batch 9193/14851, Loss: 0.024371357634663582\n",
      "Batch 9194/14851, Loss: 0.00018982093024533242\n",
      "Batch 9195/14851, Loss: 0.04748208075761795\n",
      "Batch 9196/14851, Loss: 0.09745830297470093\n",
      "Batch 9197/14851, Loss: 0.007037989795207977\n",
      "Batch 9198/14851, Loss: 3.3915042877197266e-05\n",
      "Batch 9199/14851, Loss: 0.0017145747551694512\n",
      "Batch 9200/14851, Loss: 0.0015323630068451166\n",
      "Batch 9201/14851, Loss: 0.04236530512571335\n",
      "Batch 9202/14851, Loss: 0.006428632885217667\n",
      "Batch 9203/14851, Loss: 0.0010446136584505439\n",
      "Batch 9204/14851, Loss: 0.01562752015888691\n",
      "Batch 9205/14851, Loss: 0.06574220955371857\n",
      "Batch 9206/14851, Loss: 0.0192093588411808\n",
      "Batch 9207/14851, Loss: 0.021368006244301796\n",
      "Batch 9208/14851, Loss: 0.007957719266414642\n",
      "Batch 9209/14851, Loss: 0.007048826664686203\n",
      "Batch 9210/14851, Loss: 0.009225348010659218\n",
      "Batch 9211/14851, Loss: 0.017758037894964218\n",
      "Batch 9212/14851, Loss: 0.003784789936617017\n",
      "Batch 9213/14851, Loss: 0.01213031355291605\n",
      "Batch 9214/14851, Loss: 0.0029903959948569536\n",
      "Batch 9215/14851, Loss: 0.0006431813235394657\n",
      "Batch 9216/14851, Loss: 6.815046072006226e-05\n",
      "Batch 9217/14851, Loss: 0.0020984013099223375\n",
      "Batch 9218/14851, Loss: 0.002431899309158325\n",
      "Batch 9219/14851, Loss: 0.025873618200421333\n",
      "Batch 9220/14851, Loss: 0.02041366696357727\n",
      "Batch 9221/14851, Loss: 0.006480131298303604\n",
      "Batch 9222/14851, Loss: 0.0018558427691459656\n",
      "Batch 9223/14851, Loss: 0.0006943407352082431\n",
      "Batch 9224/14851, Loss: 0.004023062065243721\n",
      "Batch 9225/14851, Loss: 0.026981230825185776\n",
      "Batch 9226/14851, Loss: 6.027519702911377e-05\n",
      "Batch 9227/14851, Loss: 0.0015113267581909895\n",
      "Batch 9228/14851, Loss: 0.0004540147783700377\n",
      "Batch 9229/14851, Loss: 0.009504730813205242\n",
      "Batch 9230/14851, Loss: 0.00901191309094429\n",
      "Batch 9231/14851, Loss: 0.017760632559657097\n",
      "Batch 9232/14851, Loss: 0.00234077125787735\n",
      "Batch 9233/14851, Loss: 0.03181833773851395\n",
      "Batch 9234/14851, Loss: 0.000798246415797621\n",
      "Batch 9235/14851, Loss: 0.001340234070084989\n",
      "Batch 9236/14851, Loss: 0.0051598576828837395\n",
      "Batch 9237/14851, Loss: 0.003658115863800049\n",
      "Batch 9238/14851, Loss: 0.0021098863799124956\n",
      "Batch 9239/14851, Loss: 0.00018895541143137962\n",
      "Batch 9240/14851, Loss: 0.11725533753633499\n",
      "Batch 9241/14851, Loss: 0.013273059390485287\n",
      "Batch 9242/14851, Loss: 0.003631607862189412\n",
      "Batch 9243/14851, Loss: 0.001543243764899671\n",
      "Batch 9244/14851, Loss: 0.00039759621722623706\n",
      "Batch 9245/14851, Loss: 0.004025869537144899\n",
      "Batch 9246/14851, Loss: 0.0011152997612953186\n",
      "Batch 9247/14851, Loss: 0.004881818313151598\n",
      "Batch 9248/14851, Loss: 0.0014371424913406372\n",
      "Batch 9249/14851, Loss: 0.010576137341558933\n",
      "Batch 9250/14851, Loss: 0.004296505823731422\n",
      "Batch 9251/14851, Loss: 0.0033819947857409716\n",
      "Batch 9252/14851, Loss: 0.005907629616558552\n",
      "Batch 9253/14851, Loss: 4.559755325317383e-05\n",
      "Batch 9254/14851, Loss: 0.0031622073147445917\n",
      "Batch 9255/14851, Loss: 0.009186421521008015\n",
      "Batch 9256/14851, Loss: 0.05456622689962387\n",
      "Batch 9257/14851, Loss: 0.010472312569618225\n",
      "Batch 9258/14851, Loss: 0.015651604160666466\n",
      "Batch 9259/14851, Loss: 0.02417965792119503\n",
      "Batch 9260/14851, Loss: 0.0029251512605696917\n",
      "Batch 9261/14851, Loss: 0.005956590175628662\n",
      "Batch 9262/14851, Loss: 0.004345654044300318\n",
      "Batch 9263/14851, Loss: 0.08355212211608887\n",
      "Batch 9264/14851, Loss: 0.0007892561261542141\n",
      "Batch 9265/14851, Loss: 0.011239413172006607\n",
      "Batch 9266/14851, Loss: 0.012204189784824848\n",
      "Skipping batch 9267 due to NaN loss\n",
      "Batch 9268/14851, Loss: nan\n",
      "Batch 9269/14851, Loss: 0.0006987477536313236\n",
      "Batch 9270/14851, Loss: 0.002146568149328232\n",
      "Batch 9271/14851, Loss: 0.014196646399796009\n",
      "Batch 9272/14851, Loss: 0.0016993669560179114\n",
      "Batch 9273/14851, Loss: 0.007163701578974724\n",
      "Batch 9274/14851, Loss: 0.004124374128878117\n",
      "Batch 9275/14851, Loss: 0.010415265336632729\n",
      "Batch 9276/14851, Loss: 0.0006213324959389865\n",
      "Batch 9277/14851, Loss: 0.020077276974916458\n",
      "Batch 9278/14851, Loss: 0.0002267652889713645\n",
      "Batch 9279/14851, Loss: 0.0020342546049505472\n",
      "Batch 9280/14851, Loss: 0.0001182618216262199\n",
      "Batch 9281/14851, Loss: 0.005619303788989782\n",
      "Batch 9282/14851, Loss: 0.0006293294136412442\n",
      "Batch 9283/14851, Loss: 0.0056021614000201225\n",
      "Batch 9284/14851, Loss: 0.024485457688570023\n",
      "Batch 9285/14851, Loss: 0.013525335118174553\n",
      "Batch 9286/14851, Loss: 0.0003370183112565428\n",
      "Batch 9287/14851, Loss: 0.0027438809629529715\n",
      "Batch 9288/14851, Loss: 0.0020030061714351177\n",
      "Batch 9289/14851, Loss: 0.0008759101037867367\n",
      "Batch 9290/14851, Loss: 0.022907624021172523\n",
      "Batch 9291/14851, Loss: 0.0022716911043971777\n",
      "Batch 9292/14851, Loss: 0.017683308571577072\n",
      "Batch 9293/14851, Loss: 0.001907884026877582\n",
      "Batch 9294/14851, Loss: 0.03182951360940933\n",
      "Batch 9295/14851, Loss: 0.0062023065984249115\n",
      "Batch 9296/14851, Loss: 0.005996767431497574\n",
      "Batch 9297/14851, Loss: 0.0008226546342484653\n",
      "Batch 9298/14851, Loss: 0.04445911943912506\n",
      "Batch 9299/14851, Loss: 0.005979449488222599\n",
      "Batch 9300/14851, Loss: 0.026589302346110344\n",
      "Batch 9301/14851, Loss: 0.0011157616972923279\n",
      "Batch 9302/14851, Loss: 0.006162284407764673\n",
      "Batch 9303/14851, Loss: 0.003135867416858673\n",
      "Batch 9304/14851, Loss: 4.8588961362838745e-05\n",
      "Batch 9305/14851, Loss: 0.002206691773608327\n",
      "Batch 9306/14851, Loss: 0.024297531694173813\n",
      "Batch 9307/14851, Loss: 0.00690260948613286\n",
      "Batch 9308/14851, Loss: 0.00273112952709198\n",
      "Batch 9309/14851, Loss: 9.842713916441426e-05\n",
      "Batch 9310/14851, Loss: 0.00471762428060174\n",
      "Batch 9311/14851, Loss: 0.032997287809848785\n",
      "Batch 9312/14851, Loss: 0.0026612204965204\n",
      "Batch 9313/14851, Loss: 0.016424302011728287\n",
      "Batch 9314/14851, Loss: 0.07847397774457932\n",
      "Batch 9315/14851, Loss: 0.007235213182866573\n",
      "Batch 9316/14851, Loss: 0.00022760406136512756\n",
      "Batch 9317/14851, Loss: 0.028324250131845474\n",
      "Batch 9318/14851, Loss: 0.006985042709857225\n",
      "Batch 9319/14851, Loss: 0.007270533125847578\n",
      "Batch 9320/14851, Loss: 0.021646646782755852\n",
      "Batch 9321/14851, Loss: 0.0023261632304638624\n",
      "Batch 9322/14851, Loss: 0.004851846490055323\n",
      "Batch 9323/14851, Loss: 0.032822586596012115\n",
      "Batch 9324/14851, Loss: 0.0032677799463272095\n",
      "Batch 9325/14851, Loss: 0.06879442185163498\n",
      "Batch 9326/14851, Loss: 0.003162490203976631\n",
      "Batch 9327/14851, Loss: 0.03860346972942352\n",
      "Batch 9328/14851, Loss: 0.002148084342479706\n",
      "Batch 9329/14851, Loss: 0.011148755438625813\n",
      "Batch 9330/14851, Loss: 0.0005912569467909634\n",
      "Batch 9331/14851, Loss: 0.003908614162355661\n",
      "Batch 9332/14851, Loss: 0.012815105728805065\n",
      "Batch 9333/14851, Loss: 0.007370375096797943\n",
      "Batch 9334/14851, Loss: 0.00762782571837306\n",
      "Batch 9335/14851, Loss: 0.0037940044421702623\n",
      "Batch 9336/14851, Loss: 0.0016413778066635132\n",
      "Batch 9337/14851, Loss: 0.0034065013751387596\n",
      "Batch 9338/14851, Loss: 0.0015660474309697747\n",
      "Batch 9339/14851, Loss: 0.010637679137289524\n",
      "Batch 9340/14851, Loss: 0.012011554092168808\n",
      "Batch 9341/14851, Loss: 0.0056321644224226475\n",
      "Batch 9342/14851, Loss: 0.013867776840925217\n",
      "Batch 9343/14851, Loss: 0.0004324639739934355\n",
      "Batch 9344/14851, Loss: 0.0013929655542597175\n",
      "Batch 9345/14851, Loss: 0.0022774687968194485\n",
      "Batch 9346/14851, Loss: 0.00011541446292540058\n",
      "Batch 9347/14851, Loss: 0.01506340317428112\n",
      "Batch 9348/14851, Loss: 0.004351746290922165\n",
      "Batch 9349/14851, Loss: 0.0020222750026732683\n",
      "Batch 9350/14851, Loss: 0.0009401937131769955\n",
      "Batch 9351/14851, Loss: 0.0002519500849302858\n",
      "Batch 9352/14851, Loss: 0.001981875626370311\n",
      "Batch 9353/14851, Loss: 0.0012998258462175727\n",
      "Batch 9354/14851, Loss: 0.013962429016828537\n",
      "Batch 9355/14851, Loss: 0.0019167736172676086\n",
      "Batch 9356/14851, Loss: 0.0001824808568926528\n",
      "Batch 9357/14851, Loss: 0.0019156684866175056\n",
      "Batch 9358/14851, Loss: 0.0008254622225649655\n",
      "Batch 9359/14851, Loss: 0.002594819525256753\n",
      "Batch 9360/14851, Loss: 0.00048287585377693176\n",
      "Batch 9361/14851, Loss: 0.005604324396699667\n",
      "Batch 9362/14851, Loss: 0.005046723410487175\n",
      "Batch 9363/14851, Loss: 0.00015742331743240356\n",
      "Batch 9364/14851, Loss: 0.003618028713390231\n",
      "Batch 9365/14851, Loss: 0.03309042751789093\n",
      "Batch 9366/14851, Loss: 0.01633559912443161\n",
      "Batch 9367/14851, Loss: 0.02775333821773529\n",
      "Batch 9368/14851, Loss: 0.005339778494089842\n",
      "Batch 9369/14851, Loss: 0.006435010582208633\n",
      "Batch 9370/14851, Loss: 0.018566550686955452\n",
      "Batch 9371/14851, Loss: 0.011990537866950035\n",
      "Batch 9372/14851, Loss: 0.000902599364053458\n",
      "Batch 9373/14851, Loss: 0.006460969336330891\n",
      "Batch 9374/14851, Loss: 0.005904447287321091\n",
      "Batch 9375/14851, Loss: 0.0006407921318896115\n",
      "Batch 9376/14851, Loss: 0.0005551464855670929\n",
      "Batch 9377/14851, Loss: 0.01256603840738535\n",
      "Batch 9378/14851, Loss: 7.850676774978638e-05\n",
      "Batch 9379/14851, Loss: 0.005062434822320938\n",
      "Batch 9380/14851, Loss: 0.0033768077846616507\n",
      "Batch 9381/14851, Loss: 0.007546946406364441\n",
      "Batch 9382/14851, Loss: 0.0009609804255887866\n",
      "Batch 9383/14851, Loss: 0.002503753872588277\n",
      "Batch 9384/14851, Loss: 0.00012248381972312927\n",
      "Batch 9385/14851, Loss: 0.0036471248604357243\n",
      "Batch 9386/14851, Loss: 0.011310497298836708\n",
      "Batch 9387/14851, Loss: 0.00022050738334655762\n",
      "Batch 9388/14851, Loss: 0.07024865597486496\n",
      "Batch 9389/14851, Loss: 0.0355408638715744\n",
      "Batch 9390/14851, Loss: 0.0027582496404647827\n",
      "Batch 9391/14851, Loss: 0.0021608818788081408\n",
      "Batch 9392/14851, Loss: 0.03004484437406063\n",
      "Batch 9393/14851, Loss: 7.497643673559651e-05\n",
      "Batch 9394/14851, Loss: 0.00551913445815444\n",
      "Batch 9395/14851, Loss: 0.014427048154175282\n",
      "Batch 9396/14851, Loss: 0.00016109769057948142\n",
      "Batch 9397/14851, Loss: 0.0009540095925331116\n",
      "Batch 9398/14851, Loss: 0.002325086621567607\n",
      "Batch 9399/14851, Loss: 0.0015386377926915884\n",
      "Batch 9400/14851, Loss: 0.0005648347432725132\n",
      "Batch 9401/14851, Loss: 0.04702332615852356\n",
      "Batch 9402/14851, Loss: 0.02739686518907547\n",
      "Batch 9403/14851, Loss: 0.013603782281279564\n",
      "Batch 9404/14851, Loss: 0.006629975978285074\n",
      "Batch 9405/14851, Loss: 0.0007172487676143646\n",
      "Batch 9406/14851, Loss: 0.008498112671077251\n",
      "Batch 9407/14851, Loss: 0.005512291565537453\n",
      "Batch 9408/14851, Loss: 0.003185805631801486\n",
      "Batch 9409/14851, Loss: 0.0074800895527005196\n",
      "Batch 9410/14851, Loss: 0.001541130244731903\n",
      "Batch 9411/14851, Loss: 0.009090369567275047\n",
      "Batch 9412/14851, Loss: 0.002696018200367689\n",
      "Batch 9413/14851, Loss: 0.000807213073130697\n",
      "Batch 9414/14851, Loss: 0.00659250607714057\n",
      "Batch 9415/14851, Loss: 0.004413662012666464\n",
      "Batch 9416/14851, Loss: 0.013620818965137005\n",
      "Batch 9417/14851, Loss: 0.003728034207597375\n",
      "Batch 9418/14851, Loss: 0.0271599180996418\n",
      "Batch 9419/14851, Loss: 0.0012022443115711212\n",
      "Batch 9420/14851, Loss: 0.04260311648249626\n",
      "Batch 9421/14851, Loss: 5.4181862651603296e-05\n",
      "Batch 9422/14851, Loss: 6.367266178131104e-05\n",
      "Batch 9423/14851, Loss: 0.0017751677660271525\n",
      "Batch 9424/14851, Loss: 0.000253140926361084\n",
      "Batch 9425/14851, Loss: 0.00048666694783605635\n",
      "Batch 9426/14851, Loss: 0.00025113546871580184\n",
      "Batch 9427/14851, Loss: 0.001269348431378603\n",
      "Batch 9428/14851, Loss: 0.004640117287635803\n",
      "Batch 9429/14851, Loss: 0.0038726255297660828\n",
      "Batch 9430/14851, Loss: 0.04077273607254028\n",
      "Batch 9431/14851, Loss: 0.06898913532495499\n",
      "Batch 9432/14851, Loss: 0.03154822811484337\n",
      "Batch 9433/14851, Loss: 0.04074598848819733\n",
      "Batch 9434/14851, Loss: 0.009441797621548176\n",
      "Batch 9435/14851, Loss: 0.004707949701696634\n",
      "Batch 9436/14851, Loss: 0.007550206035375595\n",
      "Batch 9437/14851, Loss: 0.06861630827188492\n",
      "Batch 9438/14851, Loss: 0.007290943060070276\n",
      "Batch 9439/14851, Loss: 0.02735062874853611\n",
      "Batch 9440/14851, Loss: 0.005739094689488411\n",
      "Batch 9441/14851, Loss: 0.012915062718093395\n",
      "Batch 9442/14851, Loss: 0.0048821368254721165\n",
      "Batch 9443/14851, Loss: 0.0007954115862958133\n",
      "Batch 9444/14851, Loss: 0.012923664413392544\n",
      "Batch 9445/14851, Loss: 0.0016978437779471278\n",
      "Batch 9446/14851, Loss: 0.0072171688079833984\n",
      "Batch 9447/14851, Loss: 0.009162596426904202\n",
      "Batch 9448/14851, Loss: 8.82983222254552e-05\n",
      "Batch 9449/14851, Loss: 0.0023766218218952417\n",
      "Batch 9450/14851, Loss: 0.007984738796949387\n",
      "Batch 9451/14851, Loss: 0.001372460275888443\n",
      "Batch 9452/14851, Loss: 0.002337119309231639\n",
      "Batch 9453/14851, Loss: 0.018418235704302788\n",
      "Batch 9454/14851, Loss: 0.002475174842402339\n",
      "Batch 9455/14851, Loss: 0.004470006097108126\n",
      "Batch 9456/14851, Loss: 0.000965707004070282\n",
      "Batch 9457/14851, Loss: 0.010156386531889439\n",
      "Batch 9458/14851, Loss: 0.0012606718810275197\n",
      "Batch 9459/14851, Loss: 2.1573156118392944e-05\n",
      "Batch 9460/14851, Loss: 0.0021503427997231483\n",
      "Batch 9461/14851, Loss: 0.0009135693544521928\n",
      "Batch 9462/14851, Loss: 0.005105073098093271\n",
      "Batch 9463/14851, Loss: 0.004705477971583605\n",
      "Batch 9464/14851, Loss: 0.0027547532226890326\n",
      "Batch 9465/14851, Loss: 0.0019936447497457266\n",
      "Batch 9466/14851, Loss: 0.0007583734695799649\n",
      "Batch 9467/14851, Loss: 0.00111992412712425\n",
      "Batch 9468/14851, Loss: 0.010468597523868084\n",
      "Batch 9469/14851, Loss: 0.03897206485271454\n",
      "Batch 9470/14851, Loss: 0.00039959573769010603\n",
      "Batch 9471/14851, Loss: 0.0029889224097132683\n",
      "Batch 9472/14851, Loss: 0.0001893195294542238\n",
      "Batch 9473/14851, Loss: 0.011569105088710785\n",
      "Batch 9474/14851, Loss: 0.032472122460603714\n",
      "Batch 9475/14851, Loss: 0.0011030063033103943\n",
      "Batch 9476/14851, Loss: 0.049868207424879074\n",
      "Batch 9477/14851, Loss: 0.00010394553100923076\n",
      "Batch 9478/14851, Loss: 5.7894736528396606e-05\n",
      "Batch 9479/14851, Loss: 0.006916627287864685\n",
      "Batch 9480/14851, Loss: 0.010793921537697315\n",
      "Batch 9481/14851, Loss: 0.002783180447295308\n",
      "Batch 9482/14851, Loss: 0.018585802987217903\n",
      "Batch 9483/14851, Loss: 0.06809582561254501\n",
      "Batch 9484/14851, Loss: 0.00019534181046765298\n",
      "Batch 9485/14851, Loss: 0.001819594413973391\n",
      "Batch 9486/14851, Loss: 0.004541946575045586\n",
      "Batch 9487/14851, Loss: 0.0010511259315535426\n",
      "Batch 9488/14851, Loss: 0.00019926205277442932\n",
      "Batch 9489/14851, Loss: 0.0006713090115226805\n",
      "Batch 9490/14851, Loss: 0.0002593534591142088\n",
      "Batch 9491/14851, Loss: 0.01390285138040781\n",
      "Batch 9492/14851, Loss: 0.00225270539522171\n",
      "Batch 9493/14851, Loss: 0.0007540372316725552\n",
      "Batch 9494/14851, Loss: 0.002176778856664896\n",
      "Batch 9495/14851, Loss: 0.008633744902908802\n",
      "Batch 9496/14851, Loss: 0.007755134254693985\n",
      "Batch 9497/14851, Loss: 0.00010685498273232952\n",
      "Batch 9498/14851, Loss: 0.0459035225212574\n",
      "Batch 9499/14851, Loss: 0.03560829907655716\n",
      "Batch 9500/14851, Loss: 0.0034520511981099844\n",
      "Batch 9501/14851, Loss: 0.002131521701812744\n",
      "Batch 9502/14851, Loss: 0.001745935995131731\n",
      "Batch 9503/14851, Loss: 0.004917194601148367\n",
      "Batch 9504/14851, Loss: 0.0002092731447191909\n",
      "Batch 9505/14851, Loss: 4.354740303824656e-05\n",
      "Batch 9506/14851, Loss: 0.0359325036406517\n",
      "Batch 9507/14851, Loss: 0.025897255167365074\n",
      "Batch 9508/14851, Loss: 0.041841983795166016\n",
      "Batch 9509/14851, Loss: 0.0010820900788530707\n",
      "Batch 9510/14851, Loss: 0.0011807344853878021\n",
      "Batch 9511/14851, Loss: 0.03244578838348389\n",
      "Batch 9512/14851, Loss: 0.0035978120286017656\n",
      "Batch 9513/14851, Loss: 3.6089371860725805e-05\n",
      "Batch 9514/14851, Loss: 0.0006033467943780124\n",
      "Batch 9515/14851, Loss: 0.0007137867505662143\n",
      "Batch 9516/14851, Loss: 0.007319299504160881\n",
      "Batch 9517/14851, Loss: 0.0026233575772494078\n",
      "Batch 9518/14851, Loss: 0.0009149883990176022\n",
      "Batch 9519/14851, Loss: 0.014894404448568821\n",
      "Batch 9520/14851, Loss: 0.0018743500113487244\n",
      "Batch 9521/14851, Loss: 0.027433395385742188\n",
      "Batch 9522/14851, Loss: 0.0015049949288368225\n",
      "Batch 9523/14851, Loss: 0.0029255806002765894\n",
      "Batch 9524/14851, Loss: 0.0011797560146078467\n",
      "Batch 9525/14851, Loss: 0.0012964146444573998\n",
      "Batch 9526/14851, Loss: 0.038395512849092484\n",
      "Batch 9527/14851, Loss: 7.137408101698384e-05\n",
      "Batch 9528/14851, Loss: 0.00843331590294838\n",
      "Batch 9529/14851, Loss: 0.015265616588294506\n",
      "Batch 9530/14851, Loss: 0.0006371406489051878\n",
      "Batch 9531/14851, Loss: 0.0008760293130762875\n",
      "Batch 9532/14851, Loss: 0.021238379180431366\n",
      "Batch 9533/14851, Loss: 0.01842162199318409\n",
      "Batch 9534/14851, Loss: 0.008218832314014435\n",
      "Batch 9535/14851, Loss: 0.00880009401589632\n",
      "Batch 9536/14851, Loss: 0.0002696216106414795\n",
      "Batch 9537/14851, Loss: 0.004483259283006191\n",
      "Batch 9538/14851, Loss: 0.016078105196356773\n",
      "Batch 9539/14851, Loss: 0.000641144928522408\n",
      "Batch 9540/14851, Loss: 4.690637069870718e-05\n",
      "Batch 9541/14851, Loss: 0.02783479169011116\n",
      "Batch 9542/14851, Loss: 0.006963047664612532\n",
      "Batch 9543/14851, Loss: 0.04738258942961693\n",
      "Batch 9544/14851, Loss: 0.0007476272876374424\n",
      "Batch 9545/14851, Loss: 0.003062628209590912\n",
      "Batch 9546/14851, Loss: 0.03376149386167526\n",
      "Batch 9547/14851, Loss: 0.003159677144140005\n",
      "Batch 9548/14851, Loss: 0.053669944405555725\n",
      "Batch 9549/14851, Loss: 0.005350257735699415\n",
      "Batch 9550/14851, Loss: 0.03563835844397545\n",
      "Batch 9551/14851, Loss: 0.005094640888273716\n",
      "Batch 9552/14851, Loss: 0.0005101387505419552\n",
      "Batch 9553/14851, Loss: 0.004523735027760267\n",
      "Batch 9554/14851, Loss: 0.0026434974279254675\n",
      "Batch 9555/14851, Loss: 0.02591104246675968\n",
      "Batch 9556/14851, Loss: 0.0012323660776019096\n",
      "Batch 9557/14851, Loss: 0.006339406128972769\n",
      "Batch 9558/14851, Loss: 0.045622263103723526\n",
      "Batch 9559/14851, Loss: 0.0002751698193605989\n",
      "Batch 9560/14851, Loss: 0.015176829881966114\n",
      "Batch 9561/14851, Loss: 0.0005770139396190643\n",
      "Batch 9562/14851, Loss: 0.006258869543671608\n",
      "Batch 9563/14851, Loss: 0.0057304371148347855\n",
      "Batch 9564/14851, Loss: 0.007024772930890322\n",
      "Batch 9565/14851, Loss: 0.011428449302911758\n",
      "Batch 9566/14851, Loss: 0.003636331530287862\n",
      "Batch 9567/14851, Loss: 0.02522091008722782\n",
      "Batch 9568/14851, Loss: 0.0013320803409442306\n",
      "Batch 9569/14851, Loss: 0.07427594810724258\n",
      "Batch 9570/14851, Loss: 0.003996551036834717\n",
      "Batch 9571/14851, Loss: 0.004575915634632111\n",
      "Batch 9572/14851, Loss: 0.01715591363608837\n",
      "Batch 9573/14851, Loss: 0.03942755237221718\n",
      "Batch 9574/14851, Loss: 0.008391330018639565\n",
      "Batch 9575/14851, Loss: 0.013362271711230278\n",
      "Batch 9576/14851, Loss: 0.003242104547098279\n",
      "Batch 9577/14851, Loss: 0.11993337422609329\n",
      "Batch 9578/14851, Loss: 0.0003418196283746511\n",
      "Batch 9579/14851, Loss: 0.0039697326719760895\n",
      "Batch 9580/14851, Loss: 0.005697531159967184\n",
      "Batch 9581/14851, Loss: 0.026646515354514122\n",
      "Batch 9582/14851, Loss: 0.0077062444761395454\n",
      "Batch 9583/14851, Loss: 0.00939956959336996\n",
      "Batch 9584/14851, Loss: 0.007899202406406403\n",
      "Batch 9585/14851, Loss: 0.006871702615171671\n",
      "Batch 9586/14851, Loss: 0.000715977163054049\n",
      "Batch 9587/14851, Loss: 0.03480697050690651\n",
      "Batch 9588/14851, Loss: 0.01169546227902174\n",
      "Batch 9589/14851, Loss: 0.01124990452080965\n",
      "Batch 9590/14851, Loss: 0.002230715937912464\n",
      "Batch 9591/14851, Loss: 0.020118551328778267\n",
      "Batch 9592/14851, Loss: 0.02208651602268219\n",
      "Batch 9593/14851, Loss: 0.028164956718683243\n",
      "Batch 9594/14851, Loss: 0.0003147259121760726\n",
      "Batch 9595/14851, Loss: 0.007331551983952522\n",
      "Batch 9596/14851, Loss: 0.00034222877002321184\n",
      "Batch 9597/14851, Loss: 0.0015193410217761993\n",
      "Batch 9598/14851, Loss: 0.01705201342701912\n",
      "Batch 9599/14851, Loss: 0.0018001304706558585\n",
      "Batch 9600/14851, Loss: 0.009745721705257893\n",
      "Batch 9601/14851, Loss: 0.01619761250913143\n",
      "Batch 9602/14851, Loss: 0.0012309513986110687\n",
      "Batch 9603/14851, Loss: 0.0006910574738867581\n",
      "Batch 9604/14851, Loss: 0.0021193597931414843\n",
      "Batch 9605/14851, Loss: 0.002780481008812785\n",
      "Batch 9606/14851, Loss: 0.0007723718881607056\n",
      "Batch 9607/14851, Loss: 0.0007620789110660553\n",
      "Batch 9608/14851, Loss: 0.0025946074165403843\n",
      "Batch 9609/14851, Loss: 0.0009809694020077586\n",
      "Batch 9610/14851, Loss: 0.0012772580375894904\n",
      "Batch 9611/14851, Loss: 0.002793174237012863\n",
      "Batch 9612/14851, Loss: 0.056891705840826035\n",
      "Batch 9613/14851, Loss: 0.0003682656679302454\n",
      "Batch 9614/14851, Loss: 0.034180957823991776\n",
      "Batch 9615/14851, Loss: 0.024999672546982765\n",
      "Batch 9616/14851, Loss: 0.004512492101639509\n",
      "Batch 9617/14851, Loss: 0.014586329460144043\n",
      "Batch 9618/14851, Loss: 0.0012725144624710083\n",
      "Batch 9619/14851, Loss: 0.007170951925218105\n",
      "Batch 9620/14851, Loss: 0.0017986545572057366\n",
      "Batch 9621/14851, Loss: 0.00235343468375504\n",
      "Batch 9622/14851, Loss: 0.02386917918920517\n",
      "Batch 9623/14851, Loss: 0.0003217868506908417\n",
      "Batch 9624/14851, Loss: 0.002661078004166484\n",
      "Batch 9625/14851, Loss: 0.04139874503016472\n",
      "Batch 9626/14851, Loss: 0.026846418157219887\n",
      "Batch 9627/14851, Loss: 0.004197918344289064\n",
      "Batch 9628/14851, Loss: 0.0006310579483397305\n",
      "Batch 9629/14851, Loss: 0.019877564162015915\n",
      "Batch 9630/14851, Loss: 0.0008686070214025676\n",
      "Batch 9631/14851, Loss: 0.0017225096235051751\n",
      "Batch 9632/14851, Loss: 0.008455566130578518\n",
      "Batch 9633/14851, Loss: 0.006846901029348373\n",
      "Batch 9634/14851, Loss: 0.02308395691215992\n",
      "Batch 9635/14851, Loss: 0.009189046919345856\n",
      "Batch 9636/14851, Loss: 0.001988951349630952\n",
      "Batch 9637/14851, Loss: 0.0021157092414796352\n",
      "Batch 9638/14851, Loss: 0.00968734547495842\n",
      "Batch 9639/14851, Loss: 0.028387019410729408\n",
      "Batch 9640/14851, Loss: 0.009648562408983707\n",
      "Batch 9641/14851, Loss: 0.0022500057239085436\n",
      "Batch 9642/14851, Loss: 0.001442969893105328\n",
      "Batch 9643/14851, Loss: 0.05194951221346855\n",
      "Batch 9644/14851, Loss: 0.0013984529068693519\n",
      "Batch 9645/14851, Loss: 0.0006901468150317669\n",
      "Batch 9646/14851, Loss: 0.03450816497206688\n",
      "Batch 9647/14851, Loss: 0.03503507003188133\n",
      "Batch 9648/14851, Loss: 0.0002700239419937134\n",
      "Batch 9649/14851, Loss: 0.0003983825445175171\n",
      "Batch 9650/14851, Loss: 0.00031012421823106706\n",
      "Batch 9651/14851, Loss: 0.0013230964541435242\n",
      "Batch 9652/14851, Loss: 0.0002324500965187326\n",
      "Batch 9653/14851, Loss: 0.00350756268016994\n",
      "Batch 9654/14851, Loss: 0.02702409401535988\n",
      "Batch 9655/14851, Loss: 0.003147898940369487\n",
      "Batch 9656/14851, Loss: 0.012567778117954731\n",
      "Batch 9657/14851, Loss: 0.013519519940018654\n",
      "Batch 9658/14851, Loss: 0.012265760451555252\n",
      "Batch 9659/14851, Loss: 0.023014476522803307\n",
      "Batch 9660/14851, Loss: 0.02169734612107277\n",
      "Batch 9661/14851, Loss: 0.0007376273279078305\n",
      "Batch 9662/14851, Loss: 0.00383444270119071\n",
      "Batch 9663/14851, Loss: 0.027177168056368828\n",
      "Batch 9664/14851, Loss: 0.0012045574840158224\n",
      "Batch 9665/14851, Loss: 0.001319578499533236\n",
      "Batch 9666/14851, Loss: 0.006791809573769569\n",
      "Batch 9667/14851, Loss: 0.0008805915713310242\n",
      "Batch 9668/14851, Loss: 0.024380896240472794\n",
      "Batch 9669/14851, Loss: 0.00015057499695103616\n",
      "Batch 9670/14851, Loss: 0.0005762936780229211\n",
      "Batch 9671/14851, Loss: 1.6201287508010864e-05\n",
      "Batch 9672/14851, Loss: 0.0012294029584154487\n",
      "Batch 9673/14851, Loss: 1.4431774616241455e-05\n",
      "Batch 9674/14851, Loss: 0.011580432765185833\n",
      "Batch 9675/14851, Loss: 0.006530021782964468\n",
      "Batch 9676/14851, Loss: 0.00010250136256217957\n",
      "Batch 9677/14851, Loss: 0.045483823865652084\n",
      "Batch 9678/14851, Loss: 0.00024518618010915816\n",
      "Batch 9679/14851, Loss: 0.0024547118227928877\n",
      "Batch 9680/14851, Loss: 0.007302930112928152\n",
      "Batch 9681/14851, Loss: 0.0009075154666788876\n",
      "Batch 9682/14851, Loss: 0.0013744309544563293\n",
      "Batch 9683/14851, Loss: 0.0011611678637564182\n",
      "Batch 9684/14851, Loss: 0.0013797482242807746\n",
      "Batch 9685/14851, Loss: 0.0001649099140195176\n",
      "Batch 9686/14851, Loss: 0.03017704002559185\n",
      "Batch 9687/14851, Loss: 0.00019153207540512085\n",
      "Batch 9688/14851, Loss: 0.002820738824084401\n",
      "Batch 9689/14851, Loss: 0.0017336085438728333\n",
      "Batch 9690/14851, Loss: 0.000109843909740448\n",
      "Batch 9691/14851, Loss: 0.0005622990429401398\n",
      "Batch 9692/14851, Loss: 0.002609777031466365\n",
      "Batch 9693/14851, Loss: 0.008906520903110504\n",
      "Batch 9694/14851, Loss: 0.003260372905060649\n",
      "Batch 9695/14851, Loss: 0.008485906757414341\n",
      "Batch 9696/14851, Loss: 0.00026128938770852983\n",
      "Batch 9697/14851, Loss: 0.03203975409269333\n",
      "Batch 9698/14851, Loss: 0.00015370920300483704\n",
      "Batch 9699/14851, Loss: 0.002554522594437003\n",
      "Batch 9700/14851, Loss: 0.00040676569915376604\n",
      "Batch 9701/14851, Loss: 0.011191233061254025\n",
      "Batch 9702/14851, Loss: 0.0008430692250840366\n",
      "Batch 9703/14851, Loss: 0.0007024332880973816\n",
      "Batch 9704/14851, Loss: 6.716822827002034e-05\n",
      "Batch 9705/14851, Loss: 6.974240386625752e-05\n",
      "Batch 9706/14851, Loss: 0.0005537581746466458\n",
      "Batch 9707/14851, Loss: 0.0002263498754473403\n",
      "Batch 9708/14851, Loss: 0.0025418910663574934\n",
      "Batch 9709/14851, Loss: 0.0023104981519281864\n",
      "Batch 9710/14851, Loss: 0.0507749505341053\n",
      "Batch 9711/14851, Loss: 0.0007479029591195285\n",
      "Batch 9712/14851, Loss: 1.3864289030607324e-05\n",
      "Batch 9713/14851, Loss: 0.00044226893805898726\n",
      "Batch 9714/14851, Loss: 0.01743147149682045\n",
      "Batch 9715/14851, Loss: 0.0008169685606844723\n",
      "Batch 9716/14851, Loss: 0.0002852472534868866\n",
      "Batch 9717/14851, Loss: 0.0014205314218997955\n",
      "Batch 9718/14851, Loss: 5.1597755373222753e-05\n",
      "Batch 9719/14851, Loss: 0.002110312459990382\n",
      "Batch 9720/14851, Loss: 0.001194539712741971\n",
      "Batch 9721/14851, Loss: 0.007515349891036749\n",
      "Batch 9722/14851, Loss: 0.0042472900822758675\n",
      "Batch 9723/14851, Loss: 0.00129849708173424\n",
      "Batch 9724/14851, Loss: 0.0014081243425607681\n",
      "Batch 9725/14851, Loss: 0.009642858058214188\n",
      "Batch 9726/14851, Loss: 0.0011554619995877147\n",
      "Batch 9727/14851, Loss: 0.00145451829303056\n",
      "Batch 9728/14851, Loss: 0.0025461725890636444\n",
      "Batch 9729/14851, Loss: 0.007560981437563896\n",
      "Batch 9730/14851, Loss: 0.02881474606692791\n",
      "Batch 9731/14851, Loss: 0.0005816306220367551\n",
      "Batch 9732/14851, Loss: 0.023935260251164436\n",
      "Batch 9733/14851, Loss: 0.00049615278840065\n",
      "Batch 9734/14851, Loss: 7.115304470062256e-06\n",
      "Batch 9735/14851, Loss: 0.02480967715382576\n",
      "Batch 9736/14851, Loss: 0.0007075655157677829\n",
      "Batch 9737/14851, Loss: 0.013216793537139893\n",
      "Batch 9738/14851, Loss: 0.0018230527639389038\n",
      "Batch 9739/14851, Loss: 0.019977444782853127\n",
      "Batch 9740/14851, Loss: 0.005003686062991619\n",
      "Batch 9741/14851, Loss: 0.0015026782639324665\n",
      "Batch 9742/14851, Loss: 0.00887264683842659\n",
      "Batch 9743/14851, Loss: 0.000405273080104962\n",
      "Batch 9744/14851, Loss: 0.027991868555545807\n",
      "Batch 9745/14851, Loss: 0.0003194237651769072\n",
      "Batch 9746/14851, Loss: 0.004423822741955519\n",
      "Batch 9747/14851, Loss: 0.0002026173024205491\n",
      "Batch 9748/14851, Loss: 6.476206908700988e-05\n",
      "Batch 9749/14851, Loss: 0.002530044876039028\n",
      "Batch 9750/14851, Loss: 0.005840009544044733\n",
      "Batch 9751/14851, Loss: 0.011465783230960369\n",
      "Batch 9752/14851, Loss: 0.005477439612150192\n",
      "Batch 9753/14851, Loss: 0.0012503558536991477\n",
      "Batch 9754/14851, Loss: 0.0003350861370563507\n",
      "Batch 9755/14851, Loss: 0.002932955278083682\n",
      "Batch 9756/14851, Loss: 9.233753189619165e-06\n",
      "Batch 9757/14851, Loss: 0.0017704649362713099\n",
      "Batch 9758/14851, Loss: 0.0032035401090979576\n",
      "Batch 9759/14851, Loss: 0.005088768899440765\n",
      "Batch 9760/14851, Loss: 0.0006915905396454036\n",
      "Batch 9761/14851, Loss: 0.005549921654164791\n",
      "Batch 9762/14851, Loss: 0.0039117224514484406\n",
      "Batch 9763/14851, Loss: 0.06670176982879639\n",
      "Batch 9764/14851, Loss: 0.001916930079460144\n",
      "Batch 9765/14851, Loss: 0.012614977546036243\n",
      "Batch 9766/14851, Loss: 0.004552074242383242\n",
      "Batch 9767/14851, Loss: 0.0007647636230103672\n",
      "Batch 9768/14851, Loss: 0.0002660490572452545\n",
      "Batch 9769/14851, Loss: 0.006856089923530817\n",
      "Batch 9770/14851, Loss: 0.04273448511958122\n",
      "Batch 9771/14851, Loss: 0.031166264787316322\n",
      "Batch 9772/14851, Loss: 0.02333982288837433\n",
      "Batch 9773/14851, Loss: 0.0004326440393924713\n",
      "Batch 9774/14851, Loss: 0.02732446789741516\n",
      "Batch 9775/14851, Loss: 0.024448325857520103\n",
      "Batch 9776/14851, Loss: 0.01550342421978712\n",
      "Batch 9777/14851, Loss: 0.06123609468340874\n",
      "Batch 9778/14851, Loss: 0.008290798403322697\n",
      "Batch 9779/14851, Loss: 0.0007967402343638241\n",
      "Batch 9780/14851, Loss: 0.01834547147154808\n",
      "Batch 9781/14851, Loss: 0.017299018800258636\n",
      "Batch 9782/14851, Loss: 0.0012795155635103583\n",
      "Batch 9783/14851, Loss: 0.011171857826411724\n",
      "Batch 9784/14851, Loss: 0.038628630340099335\n",
      "Batch 9785/14851, Loss: 0.006154812406748533\n",
      "Batch 9786/14851, Loss: 0.0003730928001459688\n",
      "Batch 9787/14851, Loss: 0.009921201504766941\n",
      "Batch 9788/14851, Loss: 0.003570355474948883\n",
      "Batch 9789/14851, Loss: 0.005475926212966442\n",
      "Batch 9790/14851, Loss: 0.002559090731665492\n",
      "Batch 9791/14851, Loss: 0.0009322253172285855\n",
      "Batch 9792/14851, Loss: 0.0011773601872846484\n",
      "Batch 9793/14851, Loss: 0.005992468912154436\n",
      "Batch 9794/14851, Loss: 0.0012124665081501007\n",
      "Batch 9795/14851, Loss: 0.0013948355335742235\n",
      "Batch 9796/14851, Loss: 0.00015593071293551475\n",
      "Batch 9797/14851, Loss: 0.003981180489063263\n",
      "Batch 9798/14851, Loss: 0.004089238587766886\n",
      "Batch 9799/14851, Loss: 0.04515165090560913\n",
      "Batch 9800/14851, Loss: 0.003249544184654951\n",
      "Batch 9801/14851, Loss: 0.00013458107423502952\n",
      "Batch 9802/14851, Loss: 0.007634854409843683\n",
      "Batch 9803/14851, Loss: 0.00016612310719210654\n",
      "Batch 9804/14851, Loss: 0.01013925764709711\n",
      "Batch 9805/14851, Loss: 0.0078241266310215\n",
      "Batch 9806/14851, Loss: 0.0017316205194219947\n",
      "Batch 9807/14851, Loss: 0.0003029195067938417\n",
      "Batch 9808/14851, Loss: 0.0001004400328383781\n",
      "Batch 9809/14851, Loss: 0.006863937713205814\n",
      "Batch 9810/14851, Loss: 0.0010896213352680206\n",
      "Batch 9811/14851, Loss: 0.00016935914754867554\n",
      "Batch 9812/14851, Loss: 0.09736482799053192\n",
      "Batch 9813/14851, Loss: 0.047510579228401184\n",
      "Batch 9814/14851, Loss: 0.001001120894216001\n",
      "Batch 9815/14851, Loss: 0.0011286487570032477\n",
      "Batch 9816/14851, Loss: 0.007205646019428968\n",
      "Batch 9817/14851, Loss: 0.009210683405399323\n",
      "Batch 9818/14851, Loss: 0.046661023050546646\n",
      "Batch 9819/14851, Loss: 0.000505475967656821\n",
      "Batch 9820/14851, Loss: 0.01087921392172575\n",
      "Batch 9821/14851, Loss: 0.017472120001912117\n",
      "Batch 9822/14851, Loss: 0.007230708841234446\n",
      "Batch 9823/14851, Loss: 0.004840712528675795\n",
      "Batch 9824/14851, Loss: 0.004318322986364365\n",
      "Batch 9825/14851, Loss: 0.00019585837435442954\n",
      "Batch 9826/14851, Loss: 0.0009047338389791548\n",
      "Batch 9827/14851, Loss: 0.013160851784050465\n",
      "Batch 9828/14851, Loss: 0.004773871507495642\n",
      "Batch 9829/14851, Loss: 0.0006548829842358828\n",
      "Batch 9830/14851, Loss: 0.005932273808866739\n",
      "Batch 9831/14851, Loss: 0.013894171454012394\n",
      "Batch 9832/14851, Loss: 0.024359213188290596\n",
      "Batch 9833/14851, Loss: 0.004772272892296314\n",
      "Batch 9834/14851, Loss: 0.0009004436433315277\n",
      "Batch 9835/14851, Loss: 0.006263252347707748\n",
      "Batch 9836/14851, Loss: 0.00302513362839818\n",
      "Batch 9837/14851, Loss: 0.0008017942309379578\n",
      "Batch 9838/14851, Loss: 0.04824250563979149\n",
      "Batch 9839/14851, Loss: 0.03404539078474045\n",
      "Batch 9840/14851, Loss: 0.03101501800119877\n",
      "Batch 9841/14851, Loss: 0.004439546260982752\n",
      "Batch 9842/14851, Loss: 0.0004913806915283203\n",
      "Batch 9843/14851, Loss: 0.008335968479514122\n",
      "Batch 9844/14851, Loss: 0.0016088344855234027\n",
      "Batch 9845/14851, Loss: 0.024572769179940224\n",
      "Batch 9846/14851, Loss: 0.0004979099030606449\n",
      "Batch 9847/14851, Loss: 0.011398812755942345\n",
      "Batch 9848/14851, Loss: 0.00023847694683354348\n",
      "Batch 9849/14851, Loss: 0.0037427693605422974\n",
      "Batch 9850/14851, Loss: 0.002292051911354065\n",
      "Batch 9851/14851, Loss: 0.023271042853593826\n",
      "Batch 9852/14851, Loss: 0.012271042913198471\n",
      "Batch 9853/14851, Loss: 0.005578485783189535\n",
      "Batch 9854/14851, Loss: 0.0007346806232817471\n",
      "Batch 9855/14851, Loss: 0.006431325804442167\n",
      "Batch 9856/14851, Loss: 0.002502214629203081\n",
      "Batch 9857/14851, Loss: 0.03076624870300293\n",
      "Batch 9858/14851, Loss: 0.0021919209975749254\n",
      "Batch 9859/14851, Loss: 0.00014657899737358093\n",
      "Batch 9860/14851, Loss: 1.8036613255389966e-05\n",
      "Batch 9861/14851, Loss: 0.00022455181169789284\n",
      "Batch 9862/14851, Loss: 0.005243717227131128\n",
      "Batch 9863/14851, Loss: 0.0245524812489748\n",
      "Batch 9864/14851, Loss: 0.03255094960331917\n",
      "Batch 9865/14851, Loss: 0.005861381534487009\n",
      "Batch 9866/14851, Loss: 0.0002677204611245543\n",
      "Batch 9867/14851, Loss: 0.0035533022601157427\n",
      "Batch 9868/14851, Loss: 0.028130454942584038\n",
      "Batch 9869/14851, Loss: 0.012846900150179863\n",
      "Batch 9870/14851, Loss: 0.00040382688166573644\n",
      "Batch 9871/14851, Loss: 0.009293961338698864\n",
      "Batch 9872/14851, Loss: 0.04011853411793709\n",
      "Batch 9873/14851, Loss: 0.04946935176849365\n",
      "Batch 9874/14851, Loss: 0.007493229582905769\n",
      "Batch 9875/14851, Loss: 0.001792892231605947\n",
      "Batch 9876/14851, Loss: 0.00038028383278287947\n",
      "Batch 9877/14851, Loss: 0.030536849051713943\n",
      "Batch 9878/14851, Loss: 0.0031829671934247017\n",
      "Batch 9879/14851, Loss: 0.0010822837939485908\n",
      "Batch 9880/14851, Loss: 0.033482931554317474\n",
      "Batch 9881/14851, Loss: 0.000563744455575943\n",
      "Batch 9882/14851, Loss: 0.0015977671137079597\n",
      "Batch 9883/14851, Loss: 0.0005431572790257633\n",
      "Batch 9884/14851, Loss: 0.002581780543550849\n",
      "Batch 9885/14851, Loss: 0.003998792730271816\n",
      "Batch 9886/14851, Loss: 0.0074790664948523045\n",
      "Batch 9887/14851, Loss: 0.09898059815168381\n",
      "Batch 9888/14851, Loss: 0.00020890434097964317\n",
      "Batch 9889/14851, Loss: 0.001238414435647428\n",
      "Batch 9890/14851, Loss: 0.00028548637055791914\n",
      "Batch 9891/14851, Loss: 0.0008578374981880188\n",
      "Batch 9892/14851, Loss: 0.0009510628879070282\n",
      "Batch 9893/14851, Loss: 0.020944729447364807\n",
      "Batch 9894/14851, Loss: 0.045844703912734985\n",
      "Batch 9895/14851, Loss: 0.003651595441624522\n",
      "Batch 9896/14851, Loss: 0.005655944347381592\n",
      "Batch 9897/14851, Loss: 0.0425758995115757\n",
      "Batch 9898/14851, Loss: 0.002573883393779397\n",
      "Batch 9899/14851, Loss: 0.05731258913874626\n",
      "Batch 9900/14851, Loss: 0.030989302322268486\n",
      "Batch 9901/14851, Loss: 0.012422416359186172\n",
      "Batch 9902/14851, Loss: 0.01595568284392357\n",
      "Batch 9903/14851, Loss: 0.01843610778450966\n",
      "Batch 9904/14851, Loss: 0.0016481553902849555\n",
      "Batch 9905/14851, Loss: 0.008367245085537434\n",
      "Batch 9906/14851, Loss: 0.00015666957187931985\n",
      "Batch 9907/14851, Loss: 0.051783487200737\n",
      "Batch 9908/14851, Loss: 0.00015410657215397805\n",
      "Batch 9909/14851, Loss: 0.025999682024121284\n",
      "Batch 9910/14851, Loss: 0.011727114208042622\n",
      "Batch 9911/14851, Loss: 0.038343001157045364\n",
      "Batch 9912/14851, Loss: 0.007649503648281097\n",
      "Batch 9913/14851, Loss: 0.00777796283364296\n",
      "Batch 9914/14851, Loss: 0.0017011314630508423\n",
      "Batch 9915/14851, Loss: 0.02266842871904373\n",
      "Batch 9916/14851, Loss: 0.0057888515293598175\n",
      "Batch 9917/14851, Loss: 0.013629362918436527\n",
      "Batch 9918/14851, Loss: 0.002268758835271001\n",
      "Batch 9919/14851, Loss: 0.004846825264394283\n",
      "Batch 9920/14851, Loss: 0.005512824282050133\n",
      "Batch 9921/14851, Loss: 0.03216652199625969\n",
      "Batch 9922/14851, Loss: 0.005953628569841385\n",
      "Batch 9923/14851, Loss: 0.008954597637057304\n",
      "Batch 9924/14851, Loss: 0.002541234949603677\n",
      "Batch 9925/14851, Loss: 0.0034229643642902374\n",
      "Batch 9926/14851, Loss: 0.0007860809564590454\n",
      "Batch 9927/14851, Loss: 0.0075662522576749325\n",
      "Batch 9928/14851, Loss: 0.0004184780118521303\n",
      "Batch 9929/14851, Loss: 0.01371669489890337\n",
      "Batch 9930/14851, Loss: 0.05262082442641258\n",
      "Batch 9931/14851, Loss: 0.005280023906379938\n",
      "Batch 9932/14851, Loss: 0.012477267533540726\n",
      "Batch 9933/14851, Loss: 0.004978320095688105\n",
      "Batch 9934/14851, Loss: 0.013931057415902615\n",
      "Batch 9935/14851, Loss: 0.02302481234073639\n",
      "Batch 9936/14851, Loss: 0.014867069199681282\n",
      "Batch 9937/14851, Loss: 0.0015988916857168078\n",
      "Batch 9938/14851, Loss: 0.007761639077216387\n",
      "Batch 9939/14851, Loss: 0.00039460757398046553\n",
      "Batch 9940/14851, Loss: 0.03297078236937523\n",
      "Batch 9941/14851, Loss: 0.026021089404821396\n",
      "Batch 9942/14851, Loss: 0.001499558100476861\n",
      "Batch 9943/14851, Loss: 0.000105584658740554\n",
      "Batch 9944/14851, Loss: 0.010980106890201569\n",
      "Batch 9945/14851, Loss: 0.034186478704214096\n",
      "Batch 9946/14851, Loss: 0.009244517423212528\n",
      "Batch 9947/14851, Loss: 0.0005625599878840148\n",
      "Batch 9948/14851, Loss: 2.8780350476154126e-05\n",
      "Batch 9949/14851, Loss: 0.021570613607764244\n",
      "Batch 9950/14851, Loss: 0.010725508444011211\n",
      "Batch 9951/14851, Loss: 0.009680382907390594\n",
      "Batch 9952/14851, Loss: 0.00016720511484891176\n",
      "Batch 9953/14851, Loss: 0.009900473058223724\n",
      "Batch 9954/14851, Loss: 0.0001219920814037323\n",
      "Batch 9955/14851, Loss: 0.0005877763032913208\n",
      "Batch 9956/14851, Loss: 0.0003118788299616426\n",
      "Batch 9957/14851, Loss: 0.0005217927391640842\n",
      "Batch 9958/14851, Loss: 0.03606828674674034\n",
      "Batch 9959/14851, Loss: 0.0008247909718193114\n",
      "Batch 9960/14851, Loss: 1.4472752809524536e-05\n",
      "Batch 9961/14851, Loss: 0.0007700519054196775\n",
      "Batch 9962/14851, Loss: 0.00010805153578985482\n",
      "Batch 9963/14851, Loss: 0.0067020440474152565\n",
      "Batch 9964/14851, Loss: 0.0006517873262055218\n",
      "Batch 9965/14851, Loss: 0.005860671866685152\n",
      "Batch 9966/14851, Loss: 0.0001837797462940216\n",
      "Batch 9967/14851, Loss: 0.0018353213090449572\n",
      "Batch 9968/14851, Loss: 0.02620389312505722\n",
      "Batch 9969/14851, Loss: 0.000134451940539293\n",
      "Batch 9970/14851, Loss: 0.01389737892895937\n",
      "Batch 9971/14851, Loss: 0.0023545611184090376\n",
      "Batch 9972/14851, Loss: 0.07191092520952225\n",
      "Batch 9973/14851, Loss: 0.005831626243889332\n",
      "Batch 9974/14851, Loss: 0.0008251505787484348\n",
      "Batch 9975/14851, Loss: 0.0004312092496547848\n",
      "Batch 9976/14851, Loss: 0.035767246037721634\n",
      "Batch 9977/14851, Loss: 7.592389738420025e-05\n",
      "Batch 9978/14851, Loss: 0.0007932372391223907\n",
      "Batch 9979/14851, Loss: 0.021044939756393433\n",
      "Batch 9980/14851, Loss: 6.861115252831951e-05\n",
      "Batch 9981/14851, Loss: 0.0012620947090908885\n",
      "Batch 9982/14851, Loss: 0.00041027614497579634\n",
      "Batch 9983/14851, Loss: 4.574904960463755e-05\n",
      "Batch 9984/14851, Loss: 0.0025886856019496918\n",
      "Batch 9985/14851, Loss: 0.002219123998656869\n",
      "Batch 9986/14851, Loss: 5.566328763961792e-05\n",
      "Batch 9987/14851, Loss: 0.00015917916607577354\n",
      "Batch 9988/14851, Loss: 0.042508382350206375\n",
      "Batch 9989/14851, Loss: 0.015907511115074158\n",
      "Batch 9990/14851, Loss: 0.028621193021535873\n",
      "Batch 9991/14851, Loss: 0.05110994726419449\n",
      "Batch 9992/14851, Loss: 0.07841619849205017\n",
      "Batch 9993/14851, Loss: 0.010174105875194073\n",
      "Batch 9994/14851, Loss: 0.0019721004646271467\n",
      "Batch 9995/14851, Loss: 0.0035103443078696728\n",
      "Batch 9996/14851, Loss: 0.0005959148402325809\n",
      "Batch 9997/14851, Loss: 0.002252067206427455\n",
      "Batch 9998/14851, Loss: 0.0017536206869408488\n",
      "Batch 9999/14851, Loss: 0.0017713680863380432\n",
      "Batch 10000/14851, Loss: 0.004566316958516836\n",
      "Batch 10001/14851, Loss: 0.03902292996644974\n",
      "Batch 10002/14851, Loss: 0.005955336149781942\n",
      "Batch 10003/14851, Loss: 0.009016443975269794\n",
      "Batch 10004/14851, Loss: 0.0009882007725536823\n",
      "Batch 10005/14851, Loss: 0.0007385356002487242\n",
      "Batch 10006/14851, Loss: 0.00016496206808369607\n",
      "Batch 10007/14851, Loss: 0.0030906624160706997\n",
      "Batch 10008/14851, Loss: 0.006797370966523886\n",
      "Batch 10009/14851, Loss: 0.0008903021807782352\n",
      "Batch 10010/14851, Loss: 0.000494200736284256\n",
      "Batch 10011/14851, Loss: 0.004105667117983103\n",
      "Batch 10012/14851, Loss: 0.012942294590175152\n",
      "Batch 10013/14851, Loss: 0.003966411110013723\n",
      "Batch 10014/14851, Loss: 0.004056775011122227\n",
      "Batch 10015/14851, Loss: 0.008624122478067875\n",
      "Batch 10016/14851, Loss: 0.002777439309284091\n",
      "Batch 10017/14851, Loss: 0.039317402988672256\n",
      "Batch 10018/14851, Loss: 0.00017452861357014626\n",
      "Batch 10019/14851, Loss: 0.013442092575132847\n",
      "Batch 10020/14851, Loss: 0.015544584020972252\n",
      "Batch 10021/14851, Loss: 0.0017531185876578093\n",
      "Batch 10022/14851, Loss: 0.013227823190391064\n",
      "Batch 10023/14851, Loss: 0.00012221187353134155\n",
      "Batch 10024/14851, Loss: 0.015090699307620525\n",
      "Batch 10025/14851, Loss: 0.006212212145328522\n",
      "Batch 10026/14851, Loss: 0.007873362861573696\n",
      "Batch 10027/14851, Loss: 0.04721347987651825\n",
      "Batch 10028/14851, Loss: 0.001963270828127861\n",
      "Batch 10029/14851, Loss: 0.001332167536020279\n",
      "Batch 10030/14851, Loss: 0.005460791289806366\n",
      "Batch 10031/14851, Loss: 0.00561642087996006\n",
      "Batch 10032/14851, Loss: 0.004909222479909658\n",
      "Batch 10033/14851, Loss: 0.003058298956602812\n",
      "Batch 10034/14851, Loss: 0.08427972346544266\n",
      "Batch 10035/14851, Loss: 0.002825133502483368\n",
      "Batch 10036/14851, Loss: 0.00029729059315286577\n",
      "Batch 10037/14851, Loss: 0.00716053694486618\n",
      "Batch 10038/14851, Loss: 0.0018211578717455268\n",
      "Batch 10039/14851, Loss: 0.04491584002971649\n",
      "Batch 10040/14851, Loss: 0.0005403051618486643\n",
      "Batch 10041/14851, Loss: 0.016329530626535416\n",
      "Batch 10042/14851, Loss: 0.004728840198367834\n",
      "Batch 10043/14851, Loss: 3.513445335556753e-05\n",
      "Batch 10044/14851, Loss: 0.00017504766583442688\n",
      "Batch 10045/14851, Loss: 0.013719928450882435\n",
      "Batch 10046/14851, Loss: 0.0018395744264125824\n",
      "Batch 10047/14851, Loss: 0.004260390065610409\n",
      "Batch 10048/14851, Loss: 0.006560841575264931\n",
      "Batch 10049/14851, Loss: 0.0005470079486258328\n",
      "Batch 10050/14851, Loss: 0.0004012932477053255\n",
      "Batch 10051/14851, Loss: 0.011551330797374249\n",
      "Batch 10052/14851, Loss: 0.02602478861808777\n",
      "Batch 10053/14851, Loss: 0.00336388498544693\n",
      "Batch 10054/14851, Loss: 0.000692073255777359\n",
      "Batch 10055/14851, Loss: 0.0043121762573719025\n",
      "Batch 10056/14851, Loss: 0.002857471816241741\n",
      "Batch 10057/14851, Loss: 0.011972122825682163\n",
      "Batch 10058/14851, Loss: 0.0067539517767727375\n",
      "Batch 10059/14851, Loss: 0.01495042908936739\n",
      "Batch 10060/14851, Loss: 0.026203205808997154\n",
      "Batch 10061/14851, Loss: 0.0003100177855230868\n",
      "Batch 10062/14851, Loss: 0.0014317830791696906\n",
      "Batch 10063/14851, Loss: 0.0015032751252874732\n",
      "Batch 10064/14851, Loss: 2.783164381980896e-05\n",
      "Batch 10065/14851, Loss: 7.462874054908752e-05\n",
      "Batch 10066/14851, Loss: 7.976964116096497e-05\n",
      "Batch 10067/14851, Loss: 0.001234356313943863\n",
      "Batch 10068/14851, Loss: 0.016238901764154434\n",
      "Batch 10069/14851, Loss: 0.009857511147856712\n",
      "Batch 10070/14851, Loss: 0.0007083788514137268\n",
      "Batch 10071/14851, Loss: 0.0002982020378112793\n",
      "Batch 10072/14851, Loss: 0.0008760492200963199\n",
      "Batch 10073/14851, Loss: 0.000182167932507582\n",
      "Batch 10074/14851, Loss: 0.014182294718921185\n",
      "Batch 10075/14851, Loss: 0.00027650719857774675\n",
      "Batch 10076/14851, Loss: 0.004877667874097824\n",
      "Batch 10077/14851, Loss: 0.00014468282461166382\n",
      "Batch 10078/14851, Loss: 0.008541177026927471\n",
      "Batch 10079/14851, Loss: 0.00144852080848068\n",
      "Batch 10080/14851, Loss: 0.03265929967164993\n",
      "Batch 10081/14851, Loss: 0.0021521151065826416\n",
      "Batch 10082/14851, Loss: 0.003321841824799776\n",
      "Batch 10083/14851, Loss: 0.0017638342687860131\n",
      "Batch 10084/14851, Loss: 0.02922389656305313\n",
      "Batch 10085/14851, Loss: 0.038104575127363205\n",
      "Batch 10086/14851, Loss: 0.0010643129935488105\n",
      "Batch 10087/14851, Loss: 0.026757700368762016\n",
      "Batch 10088/14851, Loss: 0.00043179094791412354\n",
      "Batch 10089/14851, Loss: 0.0016858914168551564\n",
      "Batch 10090/14851, Loss: 0.003907730337232351\n",
      "Batch 10091/14851, Loss: 0.001782543957233429\n",
      "Batch 10092/14851, Loss: 0.1264202892780304\n",
      "Batch 10093/14851, Loss: 0.002162449061870575\n",
      "Batch 10094/14851, Loss: 0.0060784220695495605\n",
      "Batch 10095/14851, Loss: 0.015301575884222984\n",
      "Batch 10096/14851, Loss: 0.030356934294104576\n",
      "Batch 10097/14851, Loss: 0.009042450226843357\n",
      "Batch 10098/14851, Loss: 0.03407689929008484\n",
      "Batch 10099/14851, Loss: 0.0004703619342762977\n",
      "Batch 10100/14851, Loss: 0.0014900272944942117\n",
      "Batch 10101/14851, Loss: 0.0042192391119897366\n",
      "Batch 10102/14851, Loss: 0.006038563791662455\n",
      "Batch 10103/14851, Loss: 0.0039380700327456\n",
      "Batch 10104/14851, Loss: 0.014728138223290443\n",
      "Batch 10105/14851, Loss: 0.0034349756315350533\n",
      "Batch 10106/14851, Loss: 0.013049946166574955\n",
      "Batch 10107/14851, Loss: 0.00022120028734207153\n",
      "Batch 10108/14851, Loss: 0.013276818208396435\n",
      "Batch 10109/14851, Loss: 0.0027774160262197256\n",
      "Batch 10110/14851, Loss: 0.025070954114198685\n",
      "Batch 10111/14851, Loss: 0.01786833070218563\n",
      "Batch 10112/14851, Loss: 0.008769218809902668\n",
      "Batch 10113/14851, Loss: 0.0002479627728462219\n",
      "Batch 10114/14851, Loss: 0.0046735708601772785\n",
      "Batch 10115/14851, Loss: 0.007925445213913918\n",
      "Batch 10116/14851, Loss: 0.06831338256597519\n",
      "Batch 10117/14851, Loss: 0.04281546175479889\n",
      "Batch 10118/14851, Loss: 0.018622754141688347\n",
      "Batch 10119/14851, Loss: 0.00046252956963144243\n",
      "Batch 10120/14851, Loss: 0.0009693315951153636\n",
      "Batch 10121/14851, Loss: 0.00616169860586524\n",
      "Batch 10122/14851, Loss: 0.0009817605605348945\n",
      "Batch 10123/14851, Loss: 0.001618706970475614\n",
      "Batch 10124/14851, Loss: 0.001079406589269638\n",
      "Batch 10125/14851, Loss: 0.0026137398090213537\n",
      "Batch 10126/14851, Loss: 0.00293436530046165\n",
      "Batch 10127/14851, Loss: 0.0008086959715001285\n",
      "Batch 10128/14851, Loss: 0.006851451937109232\n",
      "Batch 10129/14851, Loss: 0.007507022004574537\n",
      "Batch 10130/14851, Loss: 0.0006744849379174411\n",
      "Batch 10131/14851, Loss: 0.0009724063565954566\n",
      "Batch 10132/14851, Loss: 0.003486383706331253\n",
      "Batch 10133/14851, Loss: 0.001993336947634816\n",
      "Batch 10134/14851, Loss: 0.04011000320315361\n",
      "Batch 10135/14851, Loss: 0.014778590761125088\n",
      "Batch 10136/14851, Loss: 0.0010753124952316284\n",
      "Batch 10137/14851, Loss: 0.0029164531733840704\n",
      "Batch 10138/14851, Loss: 0.007172546815127134\n",
      "Batch 10139/14851, Loss: 0.0032913710456341505\n",
      "Batch 10140/14851, Loss: 0.0002901218831539154\n",
      "Batch 10141/14851, Loss: 0.013736953027546406\n",
      "Batch 10142/14851, Loss: 0.0039686947129666805\n",
      "Batch 10143/14851, Loss: 0.01987939514219761\n",
      "Batch 10144/14851, Loss: 0.009958866983652115\n",
      "Batch 10145/14851, Loss: 0.02954692766070366\n",
      "Batch 10146/14851, Loss: 0.0004701899888459593\n",
      "Batch 10147/14851, Loss: 3.858904165099375e-05\n",
      "Batch 10148/14851, Loss: 0.08629261702299118\n",
      "Batch 10149/14851, Loss: 0.0493425689637661\n",
      "Batch 10150/14851, Loss: 0.007299397140741348\n",
      "Batch 10151/14851, Loss: 0.000921493221540004\n",
      "Batch 10152/14851, Loss: 0.00535338930785656\n",
      "Batch 10153/14851, Loss: 0.0017543608555570245\n",
      "Batch 10154/14851, Loss: 0.0024295945186167955\n",
      "Batch 10155/14851, Loss: 0.00017772491264622658\n",
      "Batch 10156/14851, Loss: 0.21512576937675476\n",
      "Batch 10157/14851, Loss: 0.001957251923158765\n",
      "Batch 10158/14851, Loss: 0.07770685851573944\n",
      "Batch 10159/14851, Loss: 0.005112605635076761\n",
      "Batch 10160/14851, Loss: 0.010546826757490635\n",
      "Batch 10161/14851, Loss: 0.0045138015411794186\n",
      "Batch 10162/14851, Loss: 0.04383595287799835\n",
      "Batch 10163/14851, Loss: 0.020060617476701736\n",
      "Batch 10164/14851, Loss: 0.0005166095215827227\n",
      "Batch 10165/14851, Loss: 0.000289001822238788\n",
      "Batch 10166/14851, Loss: 8.348251139977947e-05\n",
      "Batch 10167/14851, Loss: 9.953603148460388e-05\n",
      "Batch 10168/14851, Loss: 0.00261065480299294\n",
      "Batch 10169/14851, Loss: 0.04509258642792702\n",
      "Batch 10170/14851, Loss: 0.000369158893590793\n",
      "Batch 10171/14851, Loss: 0.001029761740937829\n",
      "Batch 10172/14851, Loss: 0.010901353321969509\n",
      "Batch 10173/14851, Loss: 0.0009121275506913662\n",
      "Batch 10174/14851, Loss: 0.00716034322977066\n",
      "Batch 10175/14851, Loss: 0.0159798264503479\n",
      "Batch 10176/14851, Loss: 0.0009703785181045532\n",
      "Batch 10177/14851, Loss: 0.11390028893947601\n",
      "Batch 10178/14851, Loss: 0.00978423748165369\n",
      "Batch 10179/14851, Loss: 0.0006395826931111515\n",
      "Batch 10180/14851, Loss: 0.10573258996009827\n",
      "Batch 10181/14851, Loss: 0.0002620182931423187\n",
      "Batch 10182/14851, Loss: 0.0034622661769390106\n",
      "Batch 10183/14851, Loss: 0.15401670336723328\n",
      "Batch 10184/14851, Loss: 0.0012514168629422784\n",
      "Batch 10185/14851, Loss: 0.0009469923679716885\n",
      "Batch 10186/14851, Loss: 0.0003505684435367584\n",
      "Batch 10187/14851, Loss: 0.008416865952312946\n",
      "Batch 10188/14851, Loss: 0.00024092942476272583\n",
      "Batch 10189/14851, Loss: 0.0602741539478302\n",
      "Batch 10190/14851, Loss: 0.008033442310988903\n",
      "Batch 10191/14851, Loss: 0.011759302578866482\n",
      "Batch 10192/14851, Loss: 0.024527303874492645\n",
      "Batch 10193/14851, Loss: 0.010560129769146442\n",
      "Batch 10194/14851, Loss: 0.0011622036108747125\n",
      "Batch 10195/14851, Loss: 0.01598629541695118\n",
      "Batch 10196/14851, Loss: 0.00019847805378958583\n",
      "Batch 10197/14851, Loss: 0.1877051144838333\n",
      "Batch 10198/14851, Loss: 0.0013971725711598992\n",
      "Batch 10199/14851, Loss: 0.003407253883779049\n",
      "Batch 10200/14851, Loss: 0.183489590883255\n",
      "Batch 10201/14851, Loss: 0.0019197402289137244\n",
      "Batch 10202/14851, Loss: 0.0013888408429920673\n",
      "Batch 10203/14851, Loss: 6.623193621635437e-05\n",
      "Batch 10204/14851, Loss: 0.0008704140782356262\n",
      "Batch 10205/14851, Loss: 0.0025599945802241564\n",
      "Batch 10206/14851, Loss: 0.0015948936343193054\n",
      "Batch 10207/14851, Loss: 0.009518973529338837\n",
      "Batch 10208/14851, Loss: 0.003068006131798029\n",
      "Batch 10209/14851, Loss: 0.009986860677599907\n",
      "Batch 10210/14851, Loss: 0.04268454387784004\n",
      "Batch 10211/14851, Loss: 0.0034128304105252028\n",
      "Batch 10212/14851, Loss: 0.01928149349987507\n",
      "Batch 10213/14851, Loss: 0.000441250711446628\n",
      "Batch 10214/14851, Loss: 7.573582843178883e-05\n",
      "Batch 10215/14851, Loss: 0.0002311679272679612\n",
      "Batch 10216/14851, Loss: 0.0009188975091092288\n",
      "Batch 10217/14851, Loss: 0.016487764194607735\n",
      "Batch 10218/14851, Loss: 0.031731583178043365\n",
      "Batch 10219/14851, Loss: 0.0005541034042835236\n",
      "Batch 10220/14851, Loss: 0.013478620909154415\n",
      "Batch 10221/14851, Loss: 0.014853505417704582\n",
      "Batch 10222/14851, Loss: 0.013972016982734203\n",
      "Batch 10223/14851, Loss: 0.002972857328131795\n",
      "Batch 10224/14851, Loss: 0.012997965328395367\n",
      "Batch 10225/14851, Loss: 0.0008229625527746975\n",
      "Batch 10226/14851, Loss: 0.006523475516587496\n",
      "Batch 10227/14851, Loss: 6.651381409028545e-05\n",
      "Batch 10228/14851, Loss: 0.002992391586303711\n",
      "Batch 10229/14851, Loss: 0.012980073690414429\n",
      "Batch 10230/14851, Loss: 0.019835514947772026\n",
      "Batch 10231/14851, Loss: 0.008993313647806644\n",
      "Batch 10232/14851, Loss: 0.04053056985139847\n",
      "Batch 10233/14851, Loss: 0.002612101612612605\n",
      "Batch 10234/14851, Loss: 0.0005947801400907338\n",
      "Batch 10235/14851, Loss: 0.005863750819116831\n",
      "Batch 10236/14851, Loss: 0.07221931964159012\n",
      "Batch 10237/14851, Loss: 0.001367120654322207\n",
      "Batch 10238/14851, Loss: 0.009209473617374897\n",
      "Batch 10239/14851, Loss: 0.00023924063134472817\n",
      "Batch 10240/14851, Loss: 0.0006957203149795532\n",
      "Batch 10241/14851, Loss: 0.013748835772275925\n",
      "Batch 10242/14851, Loss: 0.007104028481990099\n",
      "Batch 10243/14851, Loss: 0.048805851489305496\n",
      "Batch 10244/14851, Loss: 0.018901947885751724\n",
      "Batch 10245/14851, Loss: 0.039414118975400925\n",
      "Batch 10246/14851, Loss: 0.020635884255170822\n",
      "Batch 10247/14851, Loss: 9.448701894143596e-05\n",
      "Batch 10248/14851, Loss: 0.0024051654618233442\n",
      "Batch 10249/14851, Loss: 0.02149791456758976\n",
      "Batch 10250/14851, Loss: 0.017878718674182892\n",
      "Batch 10251/14851, Loss: 0.00025950372219085693\n",
      "Batch 10252/14851, Loss: 0.0014929294120520353\n",
      "Batch 10253/14851, Loss: 0.0003031832748092711\n",
      "Batch 10254/14851, Loss: 0.031865380704402924\n",
      "Batch 10255/14851, Loss: 0.002684658858925104\n",
      "Batch 10256/14851, Loss: 0.07287218421697617\n",
      "Batch 10257/14851, Loss: 0.0005890292231924832\n",
      "Batch 10258/14851, Loss: 0.0006083722109906375\n",
      "Batch 10259/14851, Loss: 0.003030650317668915\n",
      "Batch 10260/14851, Loss: 0.003588509513065219\n",
      "Batch 10261/14851, Loss: 0.001462640822865069\n",
      "Batch 10262/14851, Loss: 0.0009721728856675327\n",
      "Batch 10263/14851, Loss: 0.0015981681644916534\n",
      "Batch 10264/14851, Loss: 0.0018365507712587714\n",
      "Batch 10265/14851, Loss: 0.009478735737502575\n",
      "Batch 10266/14851, Loss: 0.01800505444407463\n",
      "Batch 10267/14851, Loss: 0.003050427883863449\n",
      "Batch 10268/14851, Loss: 0.00045744949602521956\n",
      "Batch 10269/14851, Loss: 0.0014318509493023157\n",
      "Batch 10270/14851, Loss: 0.004535669460892677\n",
      "Batch 10271/14851, Loss: 0.004875677637755871\n",
      "Batch 10272/14851, Loss: 0.020841093733906746\n",
      "Batch 10273/14851, Loss: 0.030634915456175804\n",
      "Batch 10274/14851, Loss: 0.002343373838812113\n",
      "Batch 10275/14851, Loss: 0.01644275337457657\n",
      "Batch 10276/14851, Loss: 0.04625455662608147\n",
      "Batch 10277/14851, Loss: 0.0020592573564499617\n",
      "Batch 10278/14851, Loss: 0.004772980231791735\n",
      "Batch 10279/14851, Loss: 0.0048348926939070225\n",
      "Batch 10280/14851, Loss: 0.0014848560094833374\n",
      "Batch 10281/14851, Loss: 0.0038532440084964037\n",
      "Batch 10282/14851, Loss: 0.014352694153785706\n",
      "Batch 10283/14851, Loss: 0.01143569964915514\n",
      "Batch 10284/14851, Loss: 0.0134451724588871\n",
      "Batch 10285/14851, Loss: 0.015075930394232273\n",
      "Batch 10286/14851, Loss: 0.002970537869259715\n",
      "Batch 10287/14851, Loss: 0.0006364182918332517\n",
      "Batch 10288/14851, Loss: 0.016502464190125465\n",
      "Batch 10289/14851, Loss: 0.03972020372748375\n",
      "Batch 10290/14851, Loss: 0.004600924905389547\n",
      "Batch 10291/14851, Loss: 0.015460657887160778\n",
      "Batch 10292/14851, Loss: 0.004952834453433752\n",
      "Batch 10293/14851, Loss: 0.018956243991851807\n",
      "Batch 10294/14851, Loss: 0.002723495941609144\n",
      "Batch 10295/14851, Loss: 0.0017750771949067712\n",
      "Batch 10296/14851, Loss: 0.0020446740090847015\n",
      "Batch 10297/14851, Loss: 0.009310463443398476\n",
      "Batch 10298/14851, Loss: 0.007279737386852503\n",
      "Batch 10299/14851, Loss: 0.020791983231902122\n",
      "Batch 10300/14851, Loss: 0.024055249989032745\n",
      "Batch 10301/14851, Loss: 0.01734776981174946\n",
      "Batch 10302/14851, Loss: 0.0016790044028311968\n",
      "Batch 10303/14851, Loss: 0.008909294381737709\n",
      "Batch 10304/14851, Loss: 0.004036183003336191\n",
      "Batch 10305/14851, Loss: 0.026894332841038704\n",
      "Batch 10306/14851, Loss: 0.042563434690237045\n",
      "Batch 10307/14851, Loss: 0.009666326455771923\n",
      "Batch 10308/14851, Loss: 0.0004730267100967467\n",
      "Batch 10309/14851, Loss: 0.003364185569807887\n",
      "Batch 10310/14851, Loss: 0.0003370208141859621\n",
      "Batch 10311/14851, Loss: 0.0042993114329874516\n",
      "Batch 10312/14851, Loss: 0.03204800561070442\n",
      "Batch 10313/14851, Loss: 0.0027082848828285933\n",
      "Batch 10314/14851, Loss: 0.01770734414458275\n",
      "Batch 10315/14851, Loss: 0.00033689194242469966\n",
      "Batch 10316/14851, Loss: 0.046814728528261185\n",
      "Batch 10317/14851, Loss: 0.017204532399773598\n",
      "Batch 10318/14851, Loss: 0.0006718796794302762\n",
      "Batch 10319/14851, Loss: 0.0003500195743981749\n",
      "Batch 10320/14851, Loss: 0.0006171865970827639\n",
      "Batch 10321/14851, Loss: 0.00012184679508209229\n",
      "Batch 10322/14851, Loss: 0.0017469098092988133\n",
      "Batch 10323/14851, Loss: 0.009447149001061916\n",
      "Batch 10324/14851, Loss: 0.037521880120038986\n",
      "Batch 10325/14851, Loss: 0.004042965359985828\n",
      "Batch 10326/14851, Loss: 0.0010280278511345387\n",
      "Batch 10327/14851, Loss: 0.0008378110942430794\n",
      "Batch 10328/14851, Loss: 0.09679169207811356\n",
      "Batch 10329/14851, Loss: 0.008078941144049168\n",
      "Batch 10330/14851, Loss: 0.03414856642484665\n",
      "Batch 10331/14851, Loss: 0.0002034014614764601\n",
      "Batch 10332/14851, Loss: 0.001892324537038803\n",
      "Batch 10333/14851, Loss: 0.02312091365456581\n",
      "Batch 10334/14851, Loss: 0.00905593391507864\n",
      "Batch 10335/14851, Loss: 0.010307545773684978\n",
      "Batch 10336/14851, Loss: 0.04360249266028404\n",
      "Batch 10337/14851, Loss: 0.002605101093649864\n",
      "Batch 10338/14851, Loss: 0.0018518567085266113\n",
      "Batch 10339/14851, Loss: 0.004872288554906845\n",
      "Batch 10340/14851, Loss: 0.001957077533006668\n",
      "Batch 10341/14851, Loss: 0.008048349060118198\n",
      "Batch 10342/14851, Loss: 0.010074661113321781\n",
      "Batch 10343/14851, Loss: 0.06654596328735352\n",
      "Batch 10344/14851, Loss: 0.043143562972545624\n",
      "Batch 10345/14851, Loss: 0.001432793796993792\n",
      "Batch 10346/14851, Loss: 0.05065518990159035\n",
      "Batch 10347/14851, Loss: 0.009621357545256615\n",
      "Batch 10348/14851, Loss: 0.00529112946242094\n",
      "Batch 10349/14851, Loss: 0.008774194866418839\n",
      "Batch 10350/14851, Loss: 0.0015083795879036188\n",
      "Batch 10351/14851, Loss: 0.0030251801945269108\n",
      "Batch 10352/14851, Loss: 0.018839970231056213\n",
      "Batch 10353/14851, Loss: 0.014882160350680351\n",
      "Batch 10354/14851, Loss: 0.01851942017674446\n",
      "Batch 10355/14851, Loss: 0.040533024817705154\n",
      "Batch 10356/14851, Loss: 0.01779491826891899\n",
      "Batch 10357/14851, Loss: 0.005646304693073034\n",
      "Batch 10358/14851, Loss: 0.033743347972631454\n",
      "Batch 10359/14851, Loss: 0.002531773177906871\n",
      "Batch 10360/14851, Loss: 0.0037632137537002563\n",
      "Batch 10361/14851, Loss: 0.00667176628485322\n",
      "Batch 10362/14851, Loss: 0.004573607351630926\n",
      "Batch 10363/14851, Loss: 0.0017972985515370965\n",
      "Batch 10364/14851, Loss: 0.016395585611462593\n",
      "Batch 10365/14851, Loss: 0.006331900600343943\n",
      "Batch 10366/14851, Loss: 0.00019901232735719532\n",
      "Batch 10367/14851, Loss: 0.0003465121553745121\n",
      "Batch 10368/14851, Loss: 0.03447965160012245\n",
      "Batch 10369/14851, Loss: 0.006055599544197321\n",
      "Batch 10370/14851, Loss: 0.005520131438970566\n",
      "Batch 10371/14851, Loss: 0.052817896008491516\n",
      "Batch 10372/14851, Loss: 0.0035807688254863024\n",
      "Batch 10373/14851, Loss: 0.010415290482342243\n",
      "Batch 10374/14851, Loss: 0.0011378166964277625\n",
      "Batch 10375/14851, Loss: 4.328911381890066e-05\n",
      "Batch 10376/14851, Loss: 0.007210677023977041\n",
      "Batch 10377/14851, Loss: 0.0004015602171421051\n",
      "Batch 10378/14851, Loss: 0.010042821057140827\n",
      "Batch 10379/14851, Loss: 0.02506049908697605\n",
      "Batch 10380/14851, Loss: 0.006118191871792078\n",
      "Batch 10381/14851, Loss: 0.009809684008359909\n",
      "Batch 10382/14851, Loss: 0.002328168600797653\n",
      "Batch 10383/14851, Loss: 0.025803126394748688\n",
      "Batch 10384/14851, Loss: 0.007736223749816418\n",
      "Batch 10385/14851, Loss: 0.008400922641158104\n",
      "Batch 10386/14851, Loss: 0.00890689343214035\n",
      "Batch 10387/14851, Loss: 0.03026866540312767\n",
      "Batch 10388/14851, Loss: 0.04441234841942787\n",
      "Batch 10389/14851, Loss: 0.0007242336869239807\n",
      "Batch 10390/14851, Loss: 0.0018189674010500312\n",
      "Batch 10391/14851, Loss: 0.0011316625168547034\n",
      "Batch 10392/14851, Loss: 0.0004485746321734041\n",
      "Batch 10393/14851, Loss: 0.0006710737943649292\n",
      "Batch 10394/14851, Loss: 0.0012333504855632782\n",
      "Batch 10395/14851, Loss: 0.008708097971975803\n",
      "Batch 10396/14851, Loss: 0.0016893470892682672\n",
      "Batch 10397/14851, Loss: 0.0006267900462262332\n",
      "Batch 10398/14851, Loss: 0.017704136669635773\n",
      "Batch 10399/14851, Loss: 0.012005466967821121\n",
      "Batch 10400/14851, Loss: 0.0001333379332209006\n",
      "Batch 10401/14851, Loss: 0.0192837156355381\n",
      "Batch 10402/14851, Loss: 0.0008681743638589978\n",
      "Batch 10403/14851, Loss: 0.027017271146178246\n",
      "Batch 10404/14851, Loss: 0.0004478009941522032\n",
      "Batch 10405/14851, Loss: 0.0034172707237303257\n",
      "Batch 10406/14851, Loss: 7.713958621025085e-05\n",
      "Batch 10407/14851, Loss: 0.0011475421488285065\n",
      "Batch 10408/14851, Loss: 0.02971085160970688\n",
      "Batch 10409/14851, Loss: 0.030894646421074867\n",
      "Batch 10410/14851, Loss: 0.03286769986152649\n",
      "Batch 10411/14851, Loss: 0.0055689685977995396\n",
      "Batch 10412/14851, Loss: 0.011031507514417171\n",
      "Batch 10413/14851, Loss: 0.0002210363745689392\n",
      "Batch 10414/14851, Loss: 0.019734947010874748\n",
      "Batch 10415/14851, Loss: 0.00011380637442925945\n",
      "Batch 10416/14851, Loss: 0.0033507782500237226\n",
      "Batch 10417/14851, Loss: 0.03795476257801056\n",
      "Batch 10418/14851, Loss: 0.00019893795251846313\n",
      "Batch 10419/14851, Loss: 0.0010024694493040442\n",
      "Batch 10420/14851, Loss: 0.007790269795805216\n",
      "Batch 10421/14851, Loss: 0.0003878753923345357\n",
      "Batch 10422/14851, Loss: 0.016185929998755455\n",
      "Batch 10423/14851, Loss: 0.01849181018769741\n",
      "Batch 10424/14851, Loss: 0.03234156221151352\n",
      "Batch 10425/14851, Loss: 0.030542632564902306\n",
      "Batch 10426/14851, Loss: 0.020478587597608566\n",
      "Batch 10427/14851, Loss: 0.008094032295048237\n",
      "Batch 10428/14851, Loss: 0.0014572391519322991\n",
      "Batch 10429/14851, Loss: 0.0001862222998170182\n",
      "Batch 10430/14851, Loss: 0.002952519804239273\n",
      "Batch 10431/14851, Loss: 4.3977051973342896e-05\n",
      "Batch 10432/14851, Loss: 0.0007126281852833927\n",
      "Batch 10433/14851, Loss: 0.10155066102743149\n",
      "Batch 10434/14851, Loss: 0.049019504338502884\n",
      "Batch 10435/14851, Loss: 0.019996095448732376\n",
      "Batch 10436/14851, Loss: 0.0013623759150505066\n",
      "Batch 10437/14851, Loss: 0.001946455566212535\n",
      "Batch 10438/14851, Loss: 0.020196354016661644\n",
      "Batch 10439/14851, Loss: 0.0013634314527735114\n",
      "Batch 10440/14851, Loss: 0.023523151874542236\n",
      "Batch 10441/14851, Loss: 0.035149555653333664\n",
      "Batch 10442/14851, Loss: 0.006767897866666317\n",
      "Batch 10443/14851, Loss: 0.020947115495800972\n",
      "Batch 10444/14851, Loss: 0.005371944513171911\n",
      "Batch 10445/14851, Loss: 0.002122975653037429\n",
      "Batch 10446/14851, Loss: 0.0007777797873131931\n",
      "Batch 10447/14851, Loss: 0.002887990325689316\n",
      "Batch 10448/14851, Loss: 0.0007200827240012586\n",
      "Batch 10449/14851, Loss: 0.0027521129231899977\n",
      "Batch 10450/14851, Loss: 0.007114312145859003\n",
      "Batch 10451/14851, Loss: 0.01929725520312786\n",
      "Batch 10452/14851, Loss: 0.002931444440037012\n",
      "Batch 10453/14851, Loss: 0.006034145597368479\n",
      "Batch 10454/14851, Loss: 0.00026515996432863176\n",
      "Batch 10455/14851, Loss: 0.05714936926960945\n",
      "Batch 10456/14851, Loss: 0.003299565287306905\n",
      "Batch 10457/14851, Loss: 0.0714428573846817\n",
      "Batch 10458/14851, Loss: 0.0292037520557642\n",
      "Batch 10459/14851, Loss: 0.07783246785402298\n",
      "Batch 10460/14851, Loss: 0.002896539866924286\n",
      "Batch 10461/14851, Loss: 0.0006727961008436978\n",
      "Batch 10462/14851, Loss: 0.020662961527705193\n",
      "Batch 10463/14851, Loss: 0.0141873424872756\n",
      "Batch 10464/14851, Loss: 0.005589407403022051\n",
      "Batch 10465/14851, Loss: 0.0008164197206497192\n",
      "Batch 10466/14851, Loss: 0.003216774435713887\n",
      "Batch 10467/14851, Loss: 0.061085376888513565\n",
      "Batch 10468/14851, Loss: 0.01883607730269432\n",
      "Batch 10469/14851, Loss: 0.03378409519791603\n",
      "Batch 10470/14851, Loss: 0.001946382224559784\n",
      "Batch 10471/14851, Loss: 0.023986220359802246\n",
      "Batch 10472/14851, Loss: 0.022440308704972267\n",
      "Batch 10473/14851, Loss: 0.0008014005725272\n",
      "Batch 10474/14851, Loss: 0.00023726497602183372\n",
      "Batch 10475/14851, Loss: 0.007922605611383915\n",
      "Batch 10476/14851, Loss: 0.0009063047473318875\n",
      "Batch 10477/14851, Loss: 0.012453577481210232\n",
      "Batch 10478/14851, Loss: 0.01566135138273239\n",
      "Batch 10479/14851, Loss: 0.015963392332196236\n",
      "Batch 10480/14851, Loss: 0.0003661202790681273\n",
      "Batch 10481/14851, Loss: 0.0003718547523021698\n",
      "Batch 10482/14851, Loss: 0.0009069989318959415\n",
      "Batch 10483/14851, Loss: 0.0009404569864273071\n",
      "Batch 10484/14851, Loss: 0.025503821671009064\n",
      "Batch 10485/14851, Loss: 0.0002495745720807463\n",
      "Batch 10486/14851, Loss: 0.04156602546572685\n",
      "Batch 10487/14851, Loss: 0.0001041466966853477\n",
      "Batch 10488/14851, Loss: 0.029319126158952713\n",
      "Batch 10489/14851, Loss: 0.0003805669548455626\n",
      "Batch 10490/14851, Loss: 0.007946696132421494\n",
      "Batch 10491/14851, Loss: 0.002057031961157918\n",
      "Batch 10492/14851, Loss: 0.03370599448680878\n",
      "Batch 10493/14851, Loss: 0.0013805135386064649\n",
      "Batch 10494/14851, Loss: 0.0022529680281877518\n",
      "Batch 10495/14851, Loss: 0.016606034711003304\n",
      "Batch 10496/14851, Loss: 0.02028581313788891\n",
      "Batch 10497/14851, Loss: 0.0016734969103708863\n",
      "Batch 10498/14851, Loss: 0.006649474147707224\n",
      "Batch 10499/14851, Loss: 0.009936140850186348\n",
      "Batch 10500/14851, Loss: 0.004632840398699045\n",
      "Batch 10501/14851, Loss: 0.0007974095642566681\n",
      "Batch 10502/14851, Loss: 0.004174510017037392\n",
      "Batch 10503/14851, Loss: 0.029018955305218697\n",
      "Batch 10504/14851, Loss: 0.0008740909397602081\n",
      "Batch 10505/14851, Loss: 0.018757274374365807\n",
      "Batch 10506/14851, Loss: 0.02356756664812565\n",
      "Batch 10507/14851, Loss: 0.030257705599069595\n",
      "Batch 10508/14851, Loss: 0.014195727184414864\n",
      "Batch 10509/14851, Loss: 0.00014565388846676797\n",
      "Batch 10510/14851, Loss: 0.00036957344855181873\n",
      "Batch 10511/14851, Loss: 0.006195753812789917\n",
      "Batch 10512/14851, Loss: 0.00031116834725253284\n",
      "Batch 10513/14851, Loss: 0.0028767448384314775\n",
      "Batch 10514/14851, Loss: 0.008587180636823177\n",
      "Batch 10515/14851, Loss: 0.0020990706980228424\n",
      "Batch 10516/14851, Loss: 0.001607891172170639\n",
      "Batch 10517/14851, Loss: 0.006821341346949339\n",
      "Batch 10518/14851, Loss: 0.019356392323970795\n",
      "Batch 10519/14851, Loss: 0.01455844845622778\n",
      "Batch 10520/14851, Loss: 0.002771350322291255\n",
      "Batch 10521/14851, Loss: 0.020626187324523926\n",
      "Batch 10522/14851, Loss: 0.039247751235961914\n",
      "Batch 10523/14851, Loss: 8.111819624900818e-05\n",
      "Batch 10524/14851, Loss: 0.044406432658433914\n",
      "Batch 10525/14851, Loss: 0.006613756064325571\n",
      "Batch 10526/14851, Loss: 0.0011534044751897454\n",
      "Batch 10527/14851, Loss: 0.0002049890608759597\n",
      "Batch 10528/14851, Loss: 0.07255859673023224\n",
      "Batch 10529/14851, Loss: 0.007292854133993387\n",
      "Batch 10530/14851, Loss: 0.0075989640317857265\n",
      "Batch 10531/14851, Loss: 0.030277634039521217\n",
      "Batch 10532/14851, Loss: 0.023442501202225685\n",
      "Batch 10533/14851, Loss: 0.012635729275643826\n",
      "Batch 10534/14851, Loss: 0.0004614864883478731\n",
      "Batch 10535/14851, Loss: 0.0007119377260096371\n",
      "Batch 10536/14851, Loss: 0.0006425033207051456\n",
      "Batch 10537/14851, Loss: 0.00023448359570465982\n",
      "Batch 10538/14851, Loss: 0.00039957588887773454\n",
      "Batch 10539/14851, Loss: 0.0007222449057735503\n",
      "Batch 10540/14851, Loss: 0.0005552638904191554\n",
      "Batch 10541/14851, Loss: 0.055829405784606934\n",
      "Batch 10542/14851, Loss: 0.0008858442306518555\n",
      "Batch 10543/14851, Loss: 0.0017641339218243957\n",
      "Batch 10544/14851, Loss: 0.007021199446171522\n",
      "Batch 10545/14851, Loss: 0.020047781988978386\n",
      "Batch 10546/14851, Loss: 0.0006611440912820399\n",
      "Batch 10547/14851, Loss: 0.015905601903796196\n",
      "Batch 10548/14851, Loss: 0.027349207550287247\n",
      "Batch 10549/14851, Loss: 0.007841351442039013\n",
      "Batch 10550/14851, Loss: 0.0005875209462828934\n",
      "Batch 10551/14851, Loss: 0.0013698647962883115\n",
      "Batch 10552/14851, Loss: 0.01296235155314207\n",
      "Batch 10553/14851, Loss: 0.0007553609902970493\n",
      "Batch 10554/14851, Loss: 8.987912588054314e-05\n",
      "Batch 10555/14851, Loss: 2.080574631690979e-05\n",
      "Batch 10556/14851, Loss: 0.0009809146868065\n",
      "Batch 10557/14851, Loss: 0.012698007747530937\n",
      "Batch 10558/14851, Loss: 2.590566873550415e-05\n",
      "Batch 10559/14851, Loss: 0.007356076966971159\n",
      "Batch 10560/14851, Loss: 0.00017272308468818665\n",
      "Batch 10561/14851, Loss: 0.09149760007858276\n",
      "Batch 10562/14851, Loss: 0.0001383274793624878\n",
      "Batch 10563/14851, Loss: 0.00048783421516418457\n",
      "Batch 10564/14851, Loss: 0.0010257564717903733\n",
      "Batch 10565/14851, Loss: 0.006543194409459829\n",
      "Batch 10566/14851, Loss: 0.026350265368819237\n",
      "Batch 10567/14851, Loss: 0.014522300101816654\n",
      "Batch 10568/14851, Loss: 0.040408749133348465\n",
      "Batch 10569/14851, Loss: 0.0012437142431735992\n",
      "Batch 10570/14851, Loss: 0.000944641709793359\n",
      "Batch 10571/14851, Loss: 2.8329590350040235e-05\n",
      "Batch 10572/14851, Loss: 0.027528518810868263\n",
      "Batch 10573/14851, Loss: 2.9089549570926465e-05\n",
      "Batch 10574/14851, Loss: 0.041200727224349976\n",
      "Batch 10575/14851, Loss: 0.007970146834850311\n",
      "Batch 10576/14851, Loss: 0.01810261979699135\n",
      "Batch 10577/14851, Loss: 0.0967145711183548\n",
      "Batch 10578/14851, Loss: 0.0002442064287606627\n",
      "Batch 10579/14851, Loss: 0.0037365444004535675\n",
      "Batch 10580/14851, Loss: 0.000689855485688895\n",
      "Batch 10581/14851, Loss: 0.005112632177770138\n",
      "Batch 10582/14851, Loss: 0.00025295838713645935\n",
      "Batch 10583/14851, Loss: 0.0036718391347676516\n",
      "Batch 10584/14851, Loss: 7.194901263574138e-05\n",
      "Batch 10585/14851, Loss: 0.05845402181148529\n",
      "Batch 10586/14851, Loss: 0.0019422376062721014\n",
      "Batch 10587/14851, Loss: 0.0745420828461647\n",
      "Batch 10588/14851, Loss: 0.00018751838069874793\n",
      "Batch 10589/14851, Loss: 0.06385305523872375\n",
      "Batch 10590/14851, Loss: 1.3452023267745972e-05\n",
      "Batch 10591/14851, Loss: 0.000715727626811713\n",
      "Batch 10592/14851, Loss: 0.03658987954258919\n",
      "Batch 10593/14851, Loss: 0.00926772877573967\n",
      "Batch 10594/14851, Loss: 0.003600967349484563\n",
      "Batch 10595/14851, Loss: 0.003976849373430014\n",
      "Batch 10596/14851, Loss: 0.0015412146458402276\n",
      "Batch 10597/14851, Loss: 1.3885398402635474e-05\n",
      "Batch 10598/14851, Loss: 0.00026546791195869446\n",
      "Batch 10599/14851, Loss: 0.04508848860859871\n",
      "Batch 10600/14851, Loss: 0.00012186045205453411\n",
      "Batch 10601/14851, Loss: 0.011947883293032646\n",
      "Batch 10602/14851, Loss: 0.001960084540769458\n",
      "Batch 10603/14851, Loss: 0.029667772352695465\n",
      "Batch 10604/14851, Loss: 4.233221261529252e-05\n",
      "Batch 10605/14851, Loss: 4.9875427066581324e-05\n",
      "Batch 10606/14851, Loss: 0.040245551615953445\n",
      "Batch 10607/14851, Loss: 0.0005120978457853198\n",
      "Batch 10608/14851, Loss: 0.0005158595740795135\n",
      "Batch 10609/14851, Loss: 0.0002227810473414138\n",
      "Batch 10610/14851, Loss: 0.00034505376243032515\n",
      "Batch 10611/14851, Loss: 0.015046198852360249\n",
      "Batch 10612/14851, Loss: 0.006750927772372961\n",
      "Batch 10613/14851, Loss: 0.003939035814255476\n",
      "Batch 10614/14851, Loss: 0.025147732347249985\n",
      "Batch 10615/14851, Loss: 0.004565555602312088\n",
      "Batch 10616/14851, Loss: 0.00016679614782333374\n",
      "Batch 10617/14851, Loss: 5.625684934784658e-05\n",
      "Batch 10618/14851, Loss: 0.02335272915661335\n",
      "Batch 10619/14851, Loss: 0.0009410323109477758\n",
      "Batch 10620/14851, Loss: 0.00022073835134506226\n",
      "Batch 10621/14851, Loss: 0.01297042053192854\n",
      "Batch 10622/14851, Loss: 0.005659082438796759\n",
      "Batch 10623/14851, Loss: 0.0003863871097564697\n",
      "Batch 10624/14851, Loss: 0.00982715841382742\n",
      "Batch 10625/14851, Loss: 0.00645097903907299\n",
      "Batch 10626/14851, Loss: 0.0014705732464790344\n",
      "Batch 10627/14851, Loss: 0.004566065967082977\n",
      "Batch 10628/14851, Loss: 0.005499791819602251\n",
      "Batch 10629/14851, Loss: 0.004852247890084982\n",
      "Batch 10630/14851, Loss: 0.00013251087511889637\n",
      "Batch 10631/14851, Loss: 0.0179746150970459\n",
      "Batch 10632/14851, Loss: 0.00032438462949357927\n",
      "Batch 10633/14851, Loss: 0.00376877817325294\n",
      "Batch 10634/14851, Loss: 0.002208119025453925\n",
      "Batch 10635/14851, Loss: 0.0011035463539883494\n",
      "Batch 10636/14851, Loss: 0.0025373585522174835\n",
      "Batch 10637/14851, Loss: 0.0004324366746004671\n",
      "Batch 10638/14851, Loss: 0.005590548273175955\n",
      "Batch 10639/14851, Loss: 0.014231099747121334\n",
      "Batch 10640/14851, Loss: 0.04166385531425476\n",
      "Batch 10641/14851, Loss: 0.0032334651332348585\n",
      "Batch 10642/14851, Loss: 0.00027871131896972656\n",
      "Batch 10643/14851, Loss: 0.002144085941836238\n",
      "Batch 10644/14851, Loss: 0.0008547879406251013\n",
      "Batch 10645/14851, Loss: 0.05927474796772003\n",
      "Batch 10646/14851, Loss: 0.02258698269724846\n",
      "Batch 10647/14851, Loss: 0.018660951405763626\n",
      "Batch 10648/14851, Loss: 0.01768592931330204\n",
      "Batch 10649/14851, Loss: 0.0076832897029817104\n",
      "Batch 10650/14851, Loss: 0.0006669117137789726\n",
      "Batch 10651/14851, Loss: 0.0026997055392712355\n",
      "Batch 10652/14851, Loss: 0.0032492130994796753\n",
      "Batch 10653/14851, Loss: 0.02448294870555401\n",
      "Batch 10654/14851, Loss: 0.02549240179359913\n",
      "Batch 10655/14851, Loss: 0.000958658754825592\n",
      "Batch 10656/14851, Loss: 0.00111311674118042\n",
      "Batch 10657/14851, Loss: 0.005451388191431761\n",
      "Batch 10658/14851, Loss: 0.00973618496209383\n",
      "Batch 10659/14851, Loss: 0.0036693301517516375\n",
      "Batch 10660/14851, Loss: 0.004391147755086422\n",
      "Batch 10661/14851, Loss: 0.03231911361217499\n",
      "Batch 10662/14851, Loss: 0.0003782324492931366\n",
      "Batch 10663/14851, Loss: 0.0007943089003674686\n",
      "Batch 10664/14851, Loss: 0.0037010733503848314\n",
      "Batch 10665/14851, Loss: 0.0017457297071814537\n",
      "Batch 10666/14851, Loss: 0.022452589124441147\n",
      "Batch 10667/14851, Loss: 0.003685947507619858\n",
      "Batch 10668/14851, Loss: 0.0679205060005188\n",
      "Batch 10669/14851, Loss: 0.0013421697076410055\n",
      "Batch 10670/14851, Loss: 0.024664724245667458\n",
      "Batch 10671/14851, Loss: 0.0008725772495381534\n",
      "Batch 10672/14851, Loss: 0.003231313079595566\n",
      "Batch 10673/14851, Loss: 0.005090564489364624\n",
      "Batch 10674/14851, Loss: 0.006513409316539764\n",
      "Batch 10675/14851, Loss: 0.0037575443275272846\n",
      "Batch 10676/14851, Loss: 0.007122080773115158\n",
      "Batch 10677/14851, Loss: 0.01006042119115591\n",
      "Batch 10678/14851, Loss: 0.0005092595820315182\n",
      "Batch 10679/14851, Loss: 0.0007327339262701571\n",
      "Batch 10680/14851, Loss: 0.00030550360679626465\n",
      "Batch 10681/14851, Loss: 0.00224548252299428\n",
      "Batch 10682/14851, Loss: 0.015558909624814987\n",
      "Batch 10683/14851, Loss: 0.00024651861167512834\n",
      "Batch 10684/14851, Loss: 0.004384386818856001\n",
      "Batch 10685/14851, Loss: 0.001800571451894939\n",
      "Batch 10686/14851, Loss: 0.011814805679023266\n",
      "Batch 10687/14851, Loss: 0.02240619994699955\n",
      "Batch 10688/14851, Loss: 0.005352368578314781\n",
      "Batch 10689/14851, Loss: 0.02707686461508274\n",
      "Batch 10690/14851, Loss: 0.0004913645097985864\n",
      "Batch 10691/14851, Loss: 0.0008285939693450928\n",
      "Batch 10692/14851, Loss: 0.0010367383947595954\n",
      "Batch 10693/14851, Loss: 0.0004407241940498352\n",
      "Batch 10694/14851, Loss: 0.00048649063683114946\n",
      "Batch 10695/14851, Loss: 0.000908767688088119\n",
      "Batch 10696/14851, Loss: 0.02474445290863514\n",
      "Batch 10697/14851, Loss: 0.008443482220172882\n",
      "Batch 10698/14851, Loss: 0.04257873073220253\n",
      "Batch 10699/14851, Loss: 0.00042064240551553667\n",
      "Batch 10700/14851, Loss: 0.0005176879349164665\n",
      "Batch 10701/14851, Loss: 0.0003236395714338869\n",
      "Batch 10702/14851, Loss: 0.029577629640698433\n",
      "Batch 10703/14851, Loss: 0.0034503124188631773\n",
      "Batch 10704/14851, Loss: 0.0006010482902638614\n",
      "Batch 10705/14851, Loss: 0.00021963815379422158\n",
      "Batch 10706/14851, Loss: 0.0027513401582837105\n",
      "Batch 10707/14851, Loss: 0.0020334525033831596\n",
      "Batch 10708/14851, Loss: 0.0011452793842181563\n",
      "Batch 10709/14851, Loss: 0.020442666485905647\n",
      "Batch 10710/14851, Loss: 0.003537168260663748\n",
      "Batch 10711/14851, Loss: 0.012675433419644833\n",
      "Batch 10712/14851, Loss: 0.0018845292506739497\n",
      "Batch 10713/14851, Loss: 0.0304697435349226\n",
      "Batch 10714/14851, Loss: 0.004084925167262554\n",
      "Batch 10715/14851, Loss: 0.016950389370322227\n",
      "Batch 10716/14851, Loss: 0.009668937884271145\n",
      "Batch 10717/14851, Loss: 0.006474107038229704\n",
      "Batch 10718/14851, Loss: 0.0008864489500410855\n",
      "Batch 10719/14851, Loss: 0.001286433427594602\n",
      "Batch 10720/14851, Loss: 0.012123633176088333\n",
      "Batch 10721/14851, Loss: 0.001991931116208434\n",
      "Batch 10722/14851, Loss: 0.016428157687187195\n",
      "Batch 10723/14851, Loss: 0.0007694835658185184\n",
      "Batch 10724/14851, Loss: 0.017942631617188454\n",
      "Batch 10725/14851, Loss: 0.0014934142818674445\n",
      "Batch 10726/14851, Loss: 0.0015522254398092628\n",
      "Batch 10727/14851, Loss: 0.001334457308985293\n",
      "Batch 10728/14851, Loss: 0.005647567566484213\n",
      "Batch 10729/14851, Loss: 0.007705854717642069\n",
      "Batch 10730/14851, Loss: 0.00019778807472903281\n",
      "Batch 10731/14851, Loss: 0.0013037555618211627\n",
      "Batch 10732/14851, Loss: 0.002395175164565444\n",
      "Batch 10733/14851, Loss: 0.026353660970926285\n",
      "Batch 10734/14851, Loss: 0.0010842880001291633\n",
      "Batch 10735/14851, Loss: 0.07479614019393921\n",
      "Batch 10736/14851, Loss: 0.013602734543383121\n",
      "Batch 10737/14851, Loss: 0.006791340187191963\n",
      "Batch 10738/14851, Loss: 0.0023200660943984985\n",
      "Batch 10739/14851, Loss: 0.006905379239469767\n",
      "Batch 10740/14851, Loss: 0.003136733779683709\n",
      "Batch 10741/14851, Loss: 0.0038643558509647846\n",
      "Batch 10742/14851, Loss: 0.020458504557609558\n",
      "Batch 10743/14851, Loss: 0.0014403872191905975\n",
      "Batch 10744/14851, Loss: 0.0004968233406543732\n",
      "Batch 10745/14851, Loss: 0.00036283457302488387\n",
      "Batch 10746/14851, Loss: 0.0012065706541761756\n",
      "Batch 10747/14851, Loss: 0.008330849930644035\n",
      "Batch 10748/14851, Loss: 0.008673892356455326\n",
      "Batch 10749/14851, Loss: 0.0020248647779226303\n",
      "Batch 10750/14851, Loss: 0.0016736127436161041\n",
      "Batch 10751/14851, Loss: 0.0004920872743241489\n",
      "Batch 10752/14851, Loss: 0.002681192010641098\n",
      "Batch 10753/14851, Loss: 0.053126897662878036\n",
      "Batch 10754/14851, Loss: 0.044156257063150406\n",
      "Batch 10755/14851, Loss: 0.009119421243667603\n",
      "Batch 10756/14851, Loss: 0.024000026285648346\n",
      "Batch 10757/14851, Loss: 0.0046759434044361115\n",
      "Batch 10758/14851, Loss: 0.0014828029088675976\n",
      "Batch 10759/14851, Loss: 0.0015975335845723748\n",
      "Batch 10760/14851, Loss: 0.00964228343218565\n",
      "Batch 10761/14851, Loss: 0.009631212800741196\n",
      "Batch 10762/14851, Loss: 0.010068927891552448\n",
      "Batch 10763/14851, Loss: 0.0005734190344810486\n",
      "Batch 10764/14851, Loss: 0.041946981102228165\n",
      "Batch 10765/14851, Loss: 0.023378845304250717\n",
      "Batch 10766/14851, Loss: 0.009352095425128937\n",
      "Batch 10767/14851, Loss: 0.009612832218408585\n",
      "Batch 10768/14851, Loss: 0.013414702378213406\n",
      "Batch 10769/14851, Loss: 0.00026650354266166687\n",
      "Batch 10770/14851, Loss: 0.0032616357784718275\n",
      "Batch 10771/14851, Loss: 0.0001201865597977303\n",
      "Batch 10772/14851, Loss: 0.0002501271665096283\n",
      "Batch 10773/14851, Loss: 0.008416981436312199\n",
      "Batch 10774/14851, Loss: 0.026856916025280952\n",
      "Batch 10775/14851, Loss: 0.00733885308727622\n",
      "Batch 10776/14851, Loss: 0.017690513283014297\n",
      "Batch 10777/14851, Loss: 2.7940919608226977e-05\n",
      "Batch 10778/14851, Loss: 0.0018320009112358093\n",
      "Batch 10779/14851, Loss: 0.008020134642720222\n",
      "Batch 10780/14851, Loss: 0.003112131729722023\n",
      "Batch 10781/14851, Loss: 0.014799139462411404\n",
      "Batch 10782/14851, Loss: 0.01666763611137867\n",
      "Batch 10783/14851, Loss: 0.0034713149070739746\n",
      "Batch 10784/14851, Loss: 0.0013703309232369065\n",
      "Batch 10785/14851, Loss: 0.006263484247028828\n",
      "Batch 10786/14851, Loss: 8.559592242818326e-05\n",
      "Batch 10787/14851, Loss: 0.003390437690541148\n",
      "Batch 10788/14851, Loss: 0.0036988346837460995\n",
      "Batch 10789/14851, Loss: 0.0013176240026950836\n",
      "Batch 10790/14851, Loss: 0.003176788566634059\n",
      "Batch 10791/14851, Loss: 0.0007616666262038052\n",
      "Batch 10792/14851, Loss: 0.004652862902730703\n",
      "Batch 10793/14851, Loss: 0.0006532482802867889\n",
      "Batch 10794/14851, Loss: 0.0033206455409526825\n",
      "Batch 10795/14851, Loss: 0.003155809361487627\n",
      "Batch 10796/14851, Loss: 0.00444146990776062\n",
      "Batch 10797/14851, Loss: 0.05526794493198395\n",
      "Batch 10798/14851, Loss: 0.00837189145386219\n",
      "Batch 10799/14851, Loss: 0.00011063103011110798\n",
      "Batch 10800/14851, Loss: 0.0005499472026713192\n",
      "Batch 10801/14851, Loss: 0.0020700159948319197\n",
      "Batch 10802/14851, Loss: 0.06439677625894547\n",
      "Batch 10803/14851, Loss: 0.0001930457801790908\n",
      "Batch 10804/14851, Loss: 0.0005885777063667774\n",
      "Batch 10805/14851, Loss: 0.0042023551650345325\n",
      "Batch 10806/14851, Loss: 0.002428510459139943\n",
      "Batch 10807/14851, Loss: 0.00011973207438131794\n",
      "Batch 10808/14851, Loss: 0.0005815096665173769\n",
      "Batch 10809/14851, Loss: 1.701215842331294e-05\n",
      "Batch 10810/14851, Loss: 0.006817431189119816\n",
      "Batch 10811/14851, Loss: 0.00036499276757240295\n",
      "Batch 10812/14851, Loss: 0.018380803987383842\n",
      "Batch 10813/14851, Loss: 0.003603520570322871\n",
      "Batch 10814/14851, Loss: 0.0022152625024318695\n",
      "Batch 10815/14851, Loss: 0.0001572805194882676\n",
      "Batch 10816/14851, Loss: 0.0036150997038930655\n",
      "Batch 10817/14851, Loss: 0.002573768375441432\n",
      "Batch 10818/14851, Loss: 0.001284528523683548\n",
      "Batch 10819/14851, Loss: 0.0039117359556257725\n",
      "Batch 10820/14851, Loss: 0.0038211336359381676\n",
      "Batch 10821/14851, Loss: 0.008294875733554363\n",
      "Batch 10822/14851, Loss: 0.005734623875468969\n",
      "Batch 10823/14851, Loss: 0.013520434498786926\n",
      "Batch 10824/14851, Loss: 0.005420689936727285\n",
      "Batch 10825/14851, Loss: 0.0030091963708400726\n",
      "Batch 10826/14851, Loss: 3.346179801155813e-05\n",
      "Batch 10827/14851, Loss: 0.029118699952960014\n",
      "Batch 10828/14851, Loss: 0.02801702171564102\n",
      "Batch 10829/14851, Loss: 0.012400366365909576\n",
      "Batch 10830/14851, Loss: 0.0005984501331113279\n",
      "Batch 10831/14851, Loss: 0.01247921958565712\n",
      "Batch 10832/14851, Loss: 0.0010523226810619235\n",
      "Batch 10833/14851, Loss: 0.029306087642908096\n",
      "Batch 10834/14851, Loss: 0.00017419208597857505\n",
      "Batch 10835/14851, Loss: 0.04601138457655907\n",
      "Batch 10836/14851, Loss: 0.035745713859796524\n",
      "Batch 10837/14851, Loss: 0.007936771027743816\n",
      "Batch 10838/14851, Loss: 0.008980254642665386\n",
      "Batch 10839/14851, Loss: 4.434709626366384e-05\n",
      "Batch 10840/14851, Loss: 0.051119644194841385\n",
      "Batch 10841/14851, Loss: 7.5511634349823e-06\n",
      "Batch 10842/14851, Loss: 0.001167507260106504\n",
      "Batch 10843/14851, Loss: 0.08176316320896149\n",
      "Batch 10844/14851, Loss: 0.0034541424829512835\n",
      "Batch 10845/14851, Loss: 0.0036032828502357006\n",
      "Batch 10846/14851, Loss: 0.008068524301052094\n",
      "Batch 10847/14851, Loss: 0.02713545598089695\n",
      "Batch 10848/14851, Loss: 0.0003933509287890047\n",
      "Batch 10849/14851, Loss: 0.0011036557843908668\n",
      "Batch 10850/14851, Loss: 0.005227378103882074\n",
      "Batch 10851/14851, Loss: 0.0008251211256720126\n",
      "Batch 10852/14851, Loss: 0.013789106160402298\n",
      "Batch 10853/14851, Loss: 0.00026902928948402405\n",
      "Batch 10854/14851, Loss: 0.0004632700583897531\n",
      "Batch 10855/14851, Loss: 0.0246906615793705\n",
      "Batch 10856/14851, Loss: 0.00020433713507372886\n",
      "Batch 10857/14851, Loss: 0.0005466814036481082\n",
      "Batch 10858/14851, Loss: 7.737427949905396e-06\n",
      "Batch 10859/14851, Loss: 0.0002859868109226227\n",
      "Batch 10860/14851, Loss: 0.06750159710645676\n",
      "Batch 10861/14851, Loss: 0.000825542607344687\n",
      "Batch 10862/14851, Loss: 3.644203025032766e-05\n",
      "Batch 10863/14851, Loss: 0.07423637807369232\n",
      "Batch 10864/14851, Loss: 0.07670046389102936\n",
      "Batch 10865/14851, Loss: 1.557047107780818e-05\n",
      "Batch 10866/14851, Loss: 0.010784384794533253\n",
      "Batch 10867/14851, Loss: 0.0279180146753788\n",
      "Batch 10868/14851, Loss: 0.00026422119117341936\n",
      "Batch 10869/14851, Loss: 0.09304505586624146\n",
      "Batch 10870/14851, Loss: 0.04405462369322777\n",
      "Batch 10871/14851, Loss: 0.0006155068404041231\n",
      "Batch 10872/14851, Loss: 0.000757269561290741\n",
      "Batch 10873/14851, Loss: 0.05076403543353081\n",
      "Batch 10874/14851, Loss: 0.0004957188502885401\n",
      "Batch 10875/14851, Loss: 0.00021205346274655312\n",
      "Batch 10876/14851, Loss: 0.008588570170104504\n",
      "Batch 10877/14851, Loss: 9.278953075408936e-05\n",
      "Batch 10878/14851, Loss: 5.5632244766457006e-05\n",
      "Batch 10879/14851, Loss: 0.0015353579074144363\n",
      "Batch 10880/14851, Loss: 0.019052142277359962\n",
      "Batch 10881/14851, Loss: 5.649402737617493e-05\n",
      "Batch 10882/14851, Loss: 0.004009527619928122\n",
      "Batch 10883/14851, Loss: 0.0002695485600270331\n",
      "Batch 10884/14851, Loss: 0.017034074291586876\n",
      "Batch 10885/14851, Loss: 0.0025798629503697157\n",
      "Batch 10886/14851, Loss: 0.00043673068284988403\n",
      "Batch 10887/14851, Loss: 0.04351317137479782\n",
      "Batch 10888/14851, Loss: 0.020710136741399765\n",
      "Batch 10889/14851, Loss: 0.00019645257270894945\n",
      "Batch 10890/14851, Loss: 0.03923562169075012\n",
      "Batch 10891/14851, Loss: 8.478885138174519e-05\n",
      "Batch 10892/14851, Loss: 0.0032161250710487366\n",
      "Batch 10893/14851, Loss: 0.03747502341866493\n",
      "Batch 10894/14851, Loss: 0.00012781347322743386\n",
      "Batch 10895/14851, Loss: 2.240637877548579e-05\n",
      "Batch 10896/14851, Loss: 0.0005121218855492771\n",
      "Batch 10897/14851, Loss: 0.0014529339969158173\n",
      "Batch 10898/14851, Loss: 0.0027949195355176926\n",
      "Batch 10899/14851, Loss: 0.06752706319093704\n",
      "Batch 10900/14851, Loss: 0.0006867838674224913\n",
      "Batch 10901/14851, Loss: 0.014297048561275005\n",
      "Batch 10902/14851, Loss: 0.06757955253124237\n",
      "Batch 10903/14851, Loss: 0.013979621231555939\n",
      "Batch 10904/14851, Loss: 0.00019647304725367576\n",
      "Batch 10905/14851, Loss: 0.001704701571725309\n",
      "Batch 10906/14851, Loss: 0.0005626914207823575\n",
      "Batch 10907/14851, Loss: 0.0008765404927544296\n",
      "Batch 10908/14851, Loss: 0.0019127167761325836\n",
      "Batch 10909/14851, Loss: 0.03334750235080719\n",
      "Batch 10910/14851, Loss: 0.04180016741156578\n",
      "Batch 10911/14851, Loss: 0.006066333968192339\n",
      "Batch 10912/14851, Loss: 0.001486141816712916\n",
      "Batch 10913/14851, Loss: 0.02726505696773529\n",
      "Batch 10914/14851, Loss: 0.019780755043029785\n",
      "Batch 10915/14851, Loss: 0.0008201237069442868\n",
      "Batch 10916/14851, Loss: 0.0026758757885545492\n",
      "Batch 10917/14851, Loss: 0.11437923461198807\n",
      "Batch 10918/14851, Loss: 0.03649250790476799\n",
      "Batch 10919/14851, Loss: 0.009503515437245369\n",
      "Batch 10920/14851, Loss: 0.00036268666735850275\n",
      "Batch 10921/14851, Loss: 0.006852456834167242\n",
      "Batch 10922/14851, Loss: 0.0037405132316052914\n",
      "Batch 10923/14851, Loss: 0.0006310406024567783\n",
      "Batch 10924/14851, Loss: 0.0023173480294644833\n",
      "Batch 10925/14851, Loss: 0.020765695720911026\n",
      "Batch 10926/14851, Loss: 0.014060689136385918\n",
      "Batch 10927/14851, Loss: 0.002358613768592477\n",
      "Batch 10928/14851, Loss: 0.01806606911122799\n",
      "Batch 10929/14851, Loss: 0.01941387727856636\n",
      "Batch 10930/14851, Loss: 0.0015471391379833221\n",
      "Batch 10931/14851, Loss: 0.03217272460460663\n",
      "Batch 10932/14851, Loss: 0.00019529089331626892\n",
      "Batch 10933/14851, Loss: 0.0003285171987954527\n",
      "Batch 10934/14851, Loss: 0.004947682376950979\n",
      "Batch 10935/14851, Loss: 0.0011869838926941156\n",
      "Batch 10936/14851, Loss: 0.006287644151598215\n",
      "Batch 10937/14851, Loss: 0.002982786390930414\n",
      "Batch 10938/14851, Loss: 0.002642186591401696\n",
      "Batch 10939/14851, Loss: 0.00013161450624465942\n",
      "Batch 10940/14851, Loss: 0.038735777139663696\n",
      "Batch 10941/14851, Loss: 0.003428080352023244\n",
      "Batch 10942/14851, Loss: 0.0007742457091808319\n",
      "Batch 10943/14851, Loss: 0.017286507412791252\n",
      "Batch 10944/14851, Loss: 0.0010087676346302032\n",
      "Batch 10945/14851, Loss: 0.0023459915537387133\n",
      "Batch 10946/14851, Loss: 0.0005412779282778502\n",
      "Batch 10947/14851, Loss: 0.0007784987101331353\n",
      "Batch 10948/14851, Loss: 0.0009904924081638455\n",
      "Batch 10949/14851, Loss: 0.03851859271526337\n",
      "Batch 10950/14851, Loss: 0.005189910996705294\n",
      "Batch 10951/14851, Loss: 0.0006151783163659275\n",
      "Batch 10952/14851, Loss: 0.00010308623313903809\n",
      "Batch 10953/14851, Loss: 0.005495544523000717\n",
      "Batch 10954/14851, Loss: 0.027480633929371834\n",
      "Batch 10955/14851, Loss: 0.015573745593428612\n",
      "Batch 10956/14851, Loss: 0.045918434858322144\n",
      "Batch 10957/14851, Loss: 0.015570029616355896\n",
      "Batch 10958/14851, Loss: 0.009439471177756786\n",
      "Batch 10959/14851, Loss: 0.001634849002584815\n",
      "Batch 10960/14851, Loss: 0.005011135246604681\n",
      "Batch 10961/14851, Loss: 0.0009021821315400302\n",
      "Batch 10962/14851, Loss: 0.015865923836827278\n",
      "Batch 10963/14851, Loss: 0.00011441359674790874\n",
      "Batch 10964/14851, Loss: 0.008269436657428741\n",
      "Batch 10965/14851, Loss: 0.03672613203525543\n",
      "Batch 10966/14851, Loss: 0.0004505527613218874\n",
      "Batch 10967/14851, Loss: 0.0013063164660707116\n",
      "Batch 10968/14851, Loss: 0.04412497952580452\n",
      "Batch 10969/14851, Loss: 0.017089614644646645\n",
      "Batch 10970/14851, Loss: 0.0006466321647167206\n",
      "Batch 10971/14851, Loss: 0.002784809796139598\n",
      "Batch 10972/14851, Loss: 0.0003867981431540102\n",
      "Batch 10973/14851, Loss: 0.0031378057319670916\n",
      "Batch 10974/14851, Loss: 0.010776790790259838\n",
      "Batch 10975/14851, Loss: 0.04701675474643707\n",
      "Batch 10976/14851, Loss: 0.0028465320356190205\n",
      "Batch 10977/14851, Loss: 0.0029233384411782026\n",
      "Batch 10978/14851, Loss: 0.01721806824207306\n",
      "Batch 10979/14851, Loss: 0.0011872367467731237\n",
      "Batch 10980/14851, Loss: 0.009966924786567688\n",
      "Batch 10981/14851, Loss: 0.0006755416397936642\n",
      "Batch 10982/14851, Loss: 0.056172046810388565\n",
      "Batch 10983/14851, Loss: 0.002467311918735504\n",
      "Batch 10984/14851, Loss: 0.013996556401252747\n",
      "Batch 10985/14851, Loss: 0.0013803219189867377\n",
      "Batch 10986/14851, Loss: 0.015350030735135078\n",
      "Batch 10987/14851, Loss: 0.005262605845928192\n",
      "Batch 10988/14851, Loss: 0.0013379605952650309\n",
      "Batch 10989/14851, Loss: 0.0017578034894540906\n",
      "Batch 10990/14851, Loss: 0.008093205280601978\n",
      "Batch 10991/14851, Loss: 0.00017953677161131054\n",
      "Batch 10992/14851, Loss: 0.001022140495479107\n",
      "Batch 10993/14851, Loss: 0.030821416527032852\n",
      "Batch 10994/14851, Loss: 0.00093061727238819\n",
      "Batch 10995/14851, Loss: 0.017135808244347572\n",
      "Batch 10996/14851, Loss: 0.00014922767877578735\n",
      "Batch 10997/14851, Loss: 0.036439500749111176\n",
      "Batch 10998/14851, Loss: 0.001969044329598546\n",
      "Batch 10999/14851, Loss: 0.008512931875884533\n",
      "Batch 11000/14851, Loss: 0.025653323158621788\n",
      "Batch 11001/14851, Loss: 0.001952790073119104\n",
      "Batch 11002/14851, Loss: 0.0025599210057407618\n",
      "Batch 11003/14851, Loss: 0.008546723984181881\n",
      "Batch 11004/14851, Loss: 0.006537728942930698\n",
      "Batch 11005/14851, Loss: 0.00010046487295767292\n",
      "Batch 11006/14851, Loss: 0.04847614839673042\n",
      "Batch 11007/14851, Loss: 0.013447210192680359\n",
      "Batch 11008/14851, Loss: 0.024675294756889343\n",
      "Batch 11009/14851, Loss: 0.006386969704180956\n",
      "Batch 11010/14851, Loss: 0.04830645024776459\n",
      "Batch 11011/14851, Loss: 0.0035448302514851093\n",
      "Batch 11012/14851, Loss: 0.001939966226927936\n",
      "Batch 11013/14851, Loss: 0.0017107712337747216\n",
      "Batch 11014/14851, Loss: 0.06208673492074013\n",
      "Batch 11015/14851, Loss: 0.0027614941354840994\n",
      "Batch 11016/14851, Loss: 0.014451909810304642\n",
      "Batch 11017/14851, Loss: 0.0008574898238293827\n",
      "Batch 11018/14851, Loss: 0.0019816395360976458\n",
      "Batch 11019/14851, Loss: 0.018175501376390457\n",
      "Batch 11020/14851, Loss: 0.0036289915442466736\n",
      "Batch 11021/14851, Loss: 0.014767255634069443\n",
      "Batch 11022/14851, Loss: 0.002993231173604727\n",
      "Batch 11023/14851, Loss: 0.00017524008580949157\n",
      "Batch 11024/14851, Loss: 0.005752965807914734\n",
      "Batch 11025/14851, Loss: 0.014399295672774315\n",
      "Batch 11026/14851, Loss: 0.0012875968823209405\n",
      "Batch 11027/14851, Loss: 0.027397867292165756\n",
      "Batch 11028/14851, Loss: 0.005072216037660837\n",
      "Batch 11029/14851, Loss: 0.063984215259552\n",
      "Batch 11030/14851, Loss: 0.00012138734018662944\n",
      "Batch 11031/14851, Loss: 0.0038138192612677813\n",
      "Batch 11032/14851, Loss: 0.00612754188477993\n",
      "Batch 11033/14851, Loss: 0.0011061594123020768\n",
      "Batch 11034/14851, Loss: 0.006934777833521366\n",
      "Batch 11035/14851, Loss: 0.002567620249465108\n",
      "Batch 11036/14851, Loss: 0.0017097905511036515\n",
      "Batch 11037/14851, Loss: 0.0008272056584246457\n",
      "Batch 11038/14851, Loss: 0.003647186327725649\n",
      "Batch 11039/14851, Loss: 0.023130692541599274\n",
      "Batch 11040/14851, Loss: 0.007923913188278675\n",
      "Batch 11041/14851, Loss: 0.018330708146095276\n",
      "Batch 11042/14851, Loss: 0.00012348096061032265\n",
      "Batch 11043/14851, Loss: 0.0009338011150248349\n",
      "Batch 11044/14851, Loss: 9.726484859129414e-05\n",
      "Batch 11045/14851, Loss: 0.018210219219326973\n",
      "Batch 11046/14851, Loss: 0.016440998762845993\n",
      "Batch 11047/14851, Loss: 0.007811432238668203\n",
      "Batch 11048/14851, Loss: 0.00549088791012764\n",
      "Batch 11049/14851, Loss: 0.06220724806189537\n",
      "Batch 11050/14851, Loss: 0.007602983620017767\n",
      "Batch 11051/14851, Loss: 0.0014536678791046143\n",
      "Batch 11052/14851, Loss: 0.007088618818670511\n",
      "Batch 11053/14851, Loss: 0.0053751119412481785\n",
      "Batch 11054/14851, Loss: 0.00023795168090146035\n",
      "Batch 11055/14851, Loss: 4.7769397497177124e-05\n",
      "Batch 11056/14851, Loss: 0.0021662539802491665\n",
      "Batch 11057/14851, Loss: 0.004590488504618406\n",
      "Batch 11058/14851, Loss: 0.004842275287955999\n",
      "Batch 11059/14851, Loss: 0.0028452768456190825\n",
      "Batch 11060/14851, Loss: 0.002500590169802308\n",
      "Batch 11061/14851, Loss: 0.0026144571602344513\n",
      "Batch 11062/14851, Loss: 0.002090703696012497\n",
      "Batch 11063/14851, Loss: 0.000790399790275842\n",
      "Batch 11064/14851, Loss: 0.0027055367827415466\n",
      "Batch 11065/14851, Loss: 0.0050771236419677734\n",
      "Batch 11066/14851, Loss: 0.00033150488161481917\n",
      "Batch 11067/14851, Loss: 0.0015548951923847198\n",
      "Batch 11068/14851, Loss: 0.004446121398359537\n",
      "Batch 11069/14851, Loss: 0.006992469076067209\n",
      "Batch 11070/14851, Loss: 0.11221802234649658\n",
      "Batch 11071/14851, Loss: 0.000482543051475659\n",
      "Batch 11072/14851, Loss: 0.0004061385989189148\n",
      "Batch 11073/14851, Loss: 0.005710854195058346\n",
      "Batch 11074/14851, Loss: 0.047013457864522934\n",
      "Batch 11075/14851, Loss: 0.00012446939945220947\n",
      "Batch 11076/14851, Loss: 0.008778323419392109\n",
      "Batch 11077/14851, Loss: 0.0004761094169225544\n",
      "Batch 11078/14851, Loss: 0.013775855302810669\n",
      "Batch 11079/14851, Loss: 0.01083932351320982\n",
      "Batch 11080/14851, Loss: 0.00039145475602708757\n",
      "Batch 11081/14851, Loss: 0.0006038881838321686\n",
      "Batch 11082/14851, Loss: 0.008221801370382309\n",
      "Batch 11083/14851, Loss: 0.011864605359733105\n",
      "Batch 11084/14851, Loss: 0.006275414023548365\n",
      "Batch 11085/14851, Loss: 0.014174436219036579\n",
      "Batch 11086/14851, Loss: 0.0003971979022026062\n",
      "Batch 11087/14851, Loss: 0.03415185213088989\n",
      "Batch 11088/14851, Loss: 0.008179751224815845\n",
      "Batch 11089/14851, Loss: 0.0001956621854333207\n",
      "Batch 11090/14851, Loss: 0.0002261735498905182\n",
      "Batch 11091/14851, Loss: 0.00015473738312721252\n",
      "Batch 11092/14851, Loss: 0.009777829051017761\n",
      "Batch 11093/14851, Loss: 0.0007764200563542545\n",
      "Batch 11094/14851, Loss: 0.007978464476764202\n",
      "Batch 11095/14851, Loss: 0.016957301646471024\n",
      "Batch 11096/14851, Loss: 0.00891411304473877\n",
      "Batch 11097/14851, Loss: 0.02320880815386772\n",
      "Batch 11098/14851, Loss: 0.017191149294376373\n",
      "Batch 11099/14851, Loss: 0.0011544799199327826\n",
      "Batch 11100/14851, Loss: 0.0019174081971868873\n",
      "Batch 11101/14851, Loss: 0.012254518456757069\n",
      "Batch 11102/14851, Loss: 0.00746126240119338\n",
      "Batch 11103/14851, Loss: 0.0013844259083271027\n",
      "Batch 11104/14851, Loss: 0.00013440351176541299\n",
      "Batch 11105/14851, Loss: 0.05224559083580971\n",
      "Batch 11106/14851, Loss: 0.0001569141895743087\n",
      "Batch 11107/14851, Loss: 0.008802073076367378\n",
      "Batch 11108/14851, Loss: 0.00015851475473027676\n",
      "Batch 11109/14851, Loss: 6.357828533509746e-05\n",
      "Batch 11110/14851, Loss: 0.007433208171278238\n",
      "Batch 11111/14851, Loss: 0.012549104169011116\n",
      "Batch 11112/14851, Loss: 0.07392168045043945\n",
      "Batch 11113/14851, Loss: 0.004473868291825056\n",
      "Batch 11114/14851, Loss: 0.0010696625104174018\n",
      "Batch 11115/14851, Loss: 0.0026609175838530064\n",
      "Batch 11116/14851, Loss: 0.0017046143766492605\n",
      "Batch 11117/14851, Loss: 0.04505312815308571\n",
      "Batch 11118/14851, Loss: 0.0027129340451210737\n",
      "Batch 11119/14851, Loss: 0.009965542703866959\n",
      "Batch 11120/14851, Loss: 0.0005133586819283664\n",
      "Batch 11121/14851, Loss: 0.0005765681271441281\n",
      "Batch 11122/14851, Loss: 0.00030181309557519853\n",
      "Batch 11123/14851, Loss: 0.004036957398056984\n",
      "Batch 11124/14851, Loss: 0.016523897647857666\n",
      "Batch 11125/14851, Loss: 0.0024258375633507967\n",
      "Batch 11126/14851, Loss: 0.0008049585740081966\n",
      "Batch 11127/14851, Loss: 0.003604983212426305\n",
      "Batch 11128/14851, Loss: 0.023776330053806305\n",
      "Batch 11129/14851, Loss: 0.00039967894554138184\n",
      "Batch 11130/14851, Loss: 8.942186832427979e-05\n",
      "Batch 11131/14851, Loss: 2.8438866138458252e-05\n",
      "Batch 11132/14851, Loss: 2.1666288375854492e-05\n",
      "Batch 11133/14851, Loss: 0.005987373646348715\n",
      "Batch 11134/14851, Loss: 0.01020702626556158\n",
      "Batch 11135/14851, Loss: 0.002030479023233056\n",
      "Batch 11136/14851, Loss: 0.001186611712910235\n",
      "Batch 11137/14851, Loss: 0.0003480402228888124\n",
      "Batch 11138/14851, Loss: 0.02455199882388115\n",
      "Batch 11139/14851, Loss: 0.0004322280583437532\n",
      "Batch 11140/14851, Loss: 0.0075008198618888855\n",
      "Batch 11141/14851, Loss: 0.032750751823186874\n",
      "Batch 11142/14851, Loss: 0.013813312165439129\n",
      "Batch 11143/14851, Loss: 0.002539261942729354\n",
      "Batch 11144/14851, Loss: 0.00018574048590380698\n",
      "Batch 11145/14851, Loss: 0.0006459584110416472\n",
      "Batch 11146/14851, Loss: 0.0036831621546298265\n",
      "Batch 11147/14851, Loss: 0.0007147665019147098\n",
      "Batch 11148/14851, Loss: 7.571528112748638e-05\n",
      "Batch 11149/14851, Loss: 0.010182556696236134\n",
      "Batch 11150/14851, Loss: 0.003446615068241954\n",
      "Batch 11151/14851, Loss: 0.0006198696792125702\n",
      "Batch 11152/14851, Loss: 0.017424682155251503\n",
      "Batch 11153/14851, Loss: 0.002520365873351693\n",
      "Batch 11154/14851, Loss: 0.020828185603022575\n",
      "Batch 11155/14851, Loss: 0.01071072556078434\n",
      "Batch 11156/14851, Loss: 0.0001296699047088623\n",
      "Batch 11157/14851, Loss: 0.005384164396673441\n",
      "Batch 11158/14851, Loss: 0.005743904504925013\n",
      "Batch 11159/14851, Loss: 2.360840699111577e-05\n",
      "Batch 11160/14851, Loss: 0.00816330499947071\n",
      "Batch 11161/14851, Loss: 0.015995223075151443\n",
      "Batch 11162/14851, Loss: 0.0022895238362252712\n",
      "Batch 11163/14851, Loss: 0.010624420829117298\n",
      "Batch 11164/14851, Loss: 0.0008434057235717773\n",
      "Batch 11165/14851, Loss: 0.0018346905708312988\n",
      "Batch 11166/14851, Loss: 0.00031812736415304244\n",
      "Batch 11167/14851, Loss: 0.00034114220761694014\n",
      "Batch 11168/14851, Loss: 0.0018411589553579688\n",
      "Batch 11169/14851, Loss: 0.0019863792695105076\n",
      "Batch 11170/14851, Loss: 0.014518287032842636\n",
      "Batch 11171/14851, Loss: 4.3019652366638184e-05\n",
      "Batch 11172/14851, Loss: 0.026666201651096344\n",
      "Batch 11173/14851, Loss: 0.07615985721349716\n",
      "Batch 11174/14851, Loss: 7.408732926705852e-05\n",
      "Batch 11175/14851, Loss: 3.54573130607605e-05\n",
      "Batch 11176/14851, Loss: 0.029523909091949463\n",
      "Batch 11177/14851, Loss: 0.014227880164980888\n",
      "Batch 11178/14851, Loss: 0.06039336696267128\n",
      "Batch 11179/14851, Loss: 0.034563515335321426\n",
      "Batch 11180/14851, Loss: 0.00015394827642012388\n",
      "Batch 11181/14851, Loss: 0.001140398788265884\n",
      "Batch 11182/14851, Loss: 0.028192611411213875\n",
      "Batch 11183/14851, Loss: 0.00035723671317100525\n",
      "Batch 11184/14851, Loss: 0.0010659495601430535\n",
      "Batch 11185/14851, Loss: 0.014681377448141575\n",
      "Batch 11186/14851, Loss: 0.00027241805219091475\n",
      "Batch 11187/14851, Loss: 0.0013069065753370523\n",
      "Batch 11188/14851, Loss: 0.0004099098441656679\n",
      "Batch 11189/14851, Loss: 0.002031483920291066\n",
      "Batch 11190/14851, Loss: 0.004443831741809845\n",
      "Batch 11191/14851, Loss: 4.2171526729362085e-05\n",
      "Batch 11192/14851, Loss: 0.04470325633883476\n",
      "Batch 11193/14851, Loss: 0.005925309844315052\n",
      "Batch 11194/14851, Loss: 0.0017997849499806762\n",
      "Batch 11195/14851, Loss: 2.405419945716858e-05\n",
      "Batch 11196/14851, Loss: 0.0010054173180833459\n",
      "Batch 11197/14851, Loss: 0.0007418381865136325\n",
      "Batch 11198/14851, Loss: 0.001716258586384356\n",
      "Batch 11199/14851, Loss: 0.0058538783341646194\n",
      "Batch 11200/14851, Loss: 0.0018233904847875237\n",
      "Batch 11201/14851, Loss: 0.035694003105163574\n",
      "Batch 11202/14851, Loss: 0.0007862287457101047\n",
      "Batch 11203/14851, Loss: 0.0006260474328882992\n",
      "Batch 11204/14851, Loss: 0.006468603853136301\n",
      "Batch 11205/14851, Loss: 0.00426705926656723\n",
      "Batch 11206/14851, Loss: 0.0012660423526540399\n",
      "Batch 11207/14851, Loss: 0.00023366759705822915\n",
      "Batch 11208/14851, Loss: 8.464107668260112e-05\n",
      "Batch 11209/14851, Loss: 0.0005036555230617523\n",
      "Batch 11210/14851, Loss: 0.002709345193579793\n",
      "Batch 11211/14851, Loss: 0.03264494612812996\n",
      "Batch 11212/14851, Loss: 0.05069560930132866\n",
      "Batch 11213/14851, Loss: 0.0015453877858817577\n",
      "Batch 11214/14851, Loss: 0.0018931369995698333\n",
      "Batch 11215/14851, Loss: 0.028026387095451355\n",
      "Batch 11216/14851, Loss: 0.00038857758045196533\n",
      "Batch 11217/14851, Loss: 0.005441737361252308\n",
      "Batch 11218/14851, Loss: 0.004259970039129257\n",
      "Batch 11219/14851, Loss: 0.0002806087431963533\n",
      "Batch 11220/14851, Loss: 0.029781518504023552\n",
      "Batch 11221/14851, Loss: 0.0365905724465847\n",
      "Batch 11222/14851, Loss: 0.0010967428097501397\n",
      "Batch 11223/14851, Loss: 0.005416474305093288\n",
      "Batch 11224/14851, Loss: 0.04046406224370003\n",
      "Batch 11225/14851, Loss: 0.012289291247725487\n",
      "Batch 11226/14851, Loss: 0.0008450088207609951\n",
      "Batch 11227/14851, Loss: 0.0003169798292219639\n",
      "Batch 11228/14851, Loss: 0.0016163153341040015\n",
      "Batch 11229/14851, Loss: 0.00266575301066041\n",
      "Batch 11230/14851, Loss: 0.002742764772847295\n",
      "Batch 11231/14851, Loss: 0.02393789030611515\n",
      "Batch 11232/14851, Loss: 0.0018354315543547273\n",
      "Batch 11233/14851, Loss: 0.018970629200339317\n",
      "Batch 11234/14851, Loss: 0.022431261837482452\n",
      "Batch 11235/14851, Loss: 0.00011585280299186707\n",
      "Batch 11236/14851, Loss: 0.021016795188188553\n",
      "Batch 11237/14851, Loss: 0.0004538999346550554\n",
      "Batch 11238/14851, Loss: 0.02286924608051777\n",
      "Batch 11239/14851, Loss: 0.0001443202345399186\n",
      "Batch 11240/14851, Loss: 0.0006854807143099606\n",
      "Batch 11241/14851, Loss: 0.0016186822904273868\n",
      "Batch 11242/14851, Loss: 0.01178144384175539\n",
      "Batch 11243/14851, Loss: 0.048851896077394485\n",
      "Batch 11244/14851, Loss: 0.0012581286719068885\n",
      "Batch 11245/14851, Loss: 0.023381877690553665\n",
      "Batch 11246/14851, Loss: 0.022428786382079124\n",
      "Batch 11247/14851, Loss: 0.0023124341387301683\n",
      "Batch 11248/14851, Loss: 0.011919932439923286\n",
      "Batch 11249/14851, Loss: 0.008750508539378643\n",
      "Batch 11250/14851, Loss: 0.0071841697208583355\n",
      "Batch 11251/14851, Loss: 0.001704129041172564\n",
      "Batch 11252/14851, Loss: 0.00225863722153008\n",
      "Batch 11253/14851, Loss: 0.00074746337486431\n",
      "Batch 11254/14851, Loss: 0.022794852033257484\n",
      "Batch 11255/14851, Loss: 9.16731878533028e-05\n",
      "Batch 11256/14851, Loss: 0.011316545307636261\n",
      "Batch 11257/14851, Loss: 0.0033915292005985975\n",
      "Batch 11258/14851, Loss: 0.00023169843188952655\n",
      "Batch 11259/14851, Loss: 0.009110911749303341\n",
      "Batch 11260/14851, Loss: 0.008148075081408024\n",
      "Batch 11261/14851, Loss: 0.06617596745491028\n",
      "Batch 11262/14851, Loss: 0.001004313468001783\n",
      "Batch 11263/14851, Loss: 0.0012743797851726413\n",
      "Batch 11264/14851, Loss: 0.003734693629667163\n",
      "Batch 11265/14851, Loss: 0.0030762942042201757\n",
      "Batch 11266/14851, Loss: 0.004063643980771303\n",
      "Batch 11267/14851, Loss: 0.00016060471534729004\n",
      "Batch 11268/14851, Loss: 0.0004840642213821411\n",
      "Batch 11269/14851, Loss: 0.007346857339143753\n",
      "Batch 11270/14851, Loss: 0.033555563539266586\n",
      "Batch 11271/14851, Loss: 0.006679524201899767\n",
      "Batch 11272/14851, Loss: 0.00822452548891306\n",
      "Batch 11273/14851, Loss: 0.042855679988861084\n",
      "Batch 11274/14851, Loss: 0.01608099974691868\n",
      "Batch 11275/14851, Loss: 0.01832357794046402\n",
      "Batch 11276/14851, Loss: 0.0012889467179775238\n",
      "Batch 11277/14851, Loss: 0.001146366004832089\n",
      "Batch 11278/14851, Loss: 0.00025444975472055376\n",
      "Batch 11279/14851, Loss: 0.01459649857133627\n",
      "Batch 11280/14851, Loss: 0.027968833222985268\n",
      "Batch 11281/14851, Loss: 0.0005214053089730442\n",
      "Batch 11282/14851, Loss: 0.019921891391277313\n",
      "Batch 11283/14851, Loss: 0.0877927616238594\n",
      "Batch 11284/14851, Loss: 0.004481498617678881\n",
      "Batch 11285/14851, Loss: 0.0002202851028414443\n",
      "Batch 11286/14851, Loss: 0.023759862408041954\n",
      "Batch 11287/14851, Loss: 0.00509623484686017\n",
      "Batch 11288/14851, Loss: 0.0031186838168650866\n",
      "Batch 11289/14851, Loss: 0.0007475204765796661\n",
      "Batch 11290/14851, Loss: 0.0005506592569872737\n",
      "Batch 11291/14851, Loss: 0.0004366921784821898\n",
      "Batch 11292/14851, Loss: 0.0001859957119449973\n",
      "Batch 11293/14851, Loss: 0.002308933762833476\n",
      "Batch 11294/14851, Loss: 0.03424625098705292\n",
      "Batch 11295/14851, Loss: 0.0009094054694287479\n",
      "Batch 11296/14851, Loss: 0.011557182297110558\n",
      "Batch 11297/14851, Loss: 0.003630591556429863\n",
      "Batch 11298/14851, Loss: 0.009892130270600319\n",
      "Batch 11299/14851, Loss: 0.06984390318393707\n",
      "Batch 11300/14851, Loss: 0.006114321295171976\n",
      "Batch 11301/14851, Loss: 0.007309476379305124\n",
      "Batch 11302/14851, Loss: 0.0014942536363378167\n",
      "Batch 11303/14851, Loss: 0.01851794682443142\n",
      "Batch 11304/14851, Loss: 0.021940341219305992\n",
      "Batch 11305/14851, Loss: 0.01635504700243473\n",
      "Batch 11306/14851, Loss: 0.0063308244571089745\n",
      "Batch 11307/14851, Loss: 0.004972029943019152\n",
      "Batch 11308/14851, Loss: 0.0005747266113758087\n",
      "Batch 11309/14851, Loss: 0.0025217931251972914\n",
      "Batch 11310/14851, Loss: 0.001785334199666977\n",
      "Batch 11311/14851, Loss: 0.016918417066335678\n",
      "Batch 11312/14851, Loss: 0.04555105045437813\n",
      "Batch 11313/14851, Loss: 0.04578098654747009\n",
      "Batch 11314/14851, Loss: 0.0001449895353289321\n",
      "Batch 11315/14851, Loss: 0.0045684208162128925\n",
      "Batch 11316/14851, Loss: 0.0005031483597122133\n",
      "Batch 11317/14851, Loss: 0.00010415042197564617\n",
      "Batch 11318/14851, Loss: 0.019531812518835068\n",
      "Batch 11319/14851, Loss: 0.00012174410949228331\n",
      "Batch 11320/14851, Loss: 0.025696735829114914\n",
      "Batch 11321/14851, Loss: 0.006941540632396936\n",
      "Batch 11322/14851, Loss: 0.00041039413190446794\n",
      "Batch 11323/14851, Loss: 0.00043030083179473877\n",
      "Batch 11324/14851, Loss: 0.0027525313198566437\n",
      "Batch 11325/14851, Loss: 0.0023096799850463867\n",
      "Batch 11326/14851, Loss: 0.006637310143560171\n",
      "Batch 11327/14851, Loss: 0.011852957308292389\n",
      "Batch 11328/14851, Loss: 0.011146520264446735\n",
      "Batch 11329/14851, Loss: 0.0015692246379330754\n",
      "Batch 11330/14851, Loss: 0.0019314810633659363\n",
      "Batch 11331/14851, Loss: 0.02961401641368866\n",
      "Batch 11332/14851, Loss: 0.006031456869095564\n",
      "Batch 11333/14851, Loss: 0.003733924124389887\n",
      "Batch 11334/14851, Loss: 0.0003737086954060942\n",
      "Batch 11335/14851, Loss: 0.013645641505718231\n",
      "Batch 11336/14851, Loss: 0.009355241432785988\n",
      "Batch 11337/14851, Loss: 0.020206665620207787\n",
      "Batch 11338/14851, Loss: 0.0017921491526067257\n",
      "Batch 11339/14851, Loss: 0.0028261980041861534\n",
      "Batch 11340/14851, Loss: 0.004486302379518747\n",
      "Batch 11341/14851, Loss: 0.010175390169024467\n",
      "Batch 11342/14851, Loss: 0.006613871548324823\n",
      "Batch 11343/14851, Loss: 0.0006342567503452301\n",
      "Batch 11344/14851, Loss: 5.884592610527761e-05\n",
      "Batch 11345/14851, Loss: 0.0002809514699038118\n",
      "Batch 11346/14851, Loss: 0.045645151287317276\n",
      "Batch 11347/14851, Loss: 0.004723792430013418\n",
      "Batch 11348/14851, Loss: 0.0021613810677081347\n",
      "Batch 11349/14851, Loss: 0.00210019014775753\n",
      "Batch 11350/14851, Loss: 0.013685091398656368\n",
      "Batch 11351/14851, Loss: 0.02564046159386635\n",
      "Batch 11352/14851, Loss: 0.006648709531873465\n",
      "Batch 11353/14851, Loss: 0.0018133159028366208\n",
      "Batch 11354/14851, Loss: 0.0039109461940824986\n",
      "Batch 11355/14851, Loss: 0.04889467731118202\n",
      "Batch 11356/14851, Loss: 0.01670629344880581\n",
      "Batch 11357/14851, Loss: 0.0065758503042161465\n",
      "Batch 11358/14851, Loss: 0.0039749108254909515\n",
      "Batch 11359/14851, Loss: 0.12489789724349976\n",
      "Batch 11360/14851, Loss: 0.001910292892716825\n",
      "Batch 11361/14851, Loss: 0.021480552852153778\n",
      "Batch 11362/14851, Loss: 0.004039068706333637\n",
      "Batch 11363/14851, Loss: 0.035186681896448135\n",
      "Batch 11364/14851, Loss: 0.008373656310141087\n",
      "Batch 11365/14851, Loss: 0.0010376637801527977\n",
      "Batch 11366/14851, Loss: 0.007366346661001444\n",
      "Batch 11367/14851, Loss: 5.131959915161133e-05\n",
      "Batch 11368/14851, Loss: 0.001317715854384005\n",
      "Batch 11369/14851, Loss: 0.00032419091439805925\n",
      "Batch 11370/14851, Loss: 0.022777821868658066\n",
      "Batch 11371/14851, Loss: 4.672135037253611e-05\n",
      "Batch 11372/14851, Loss: 0.000302143394947052\n",
      "Batch 11373/14851, Loss: 0.006881949957460165\n",
      "Batch 11374/14851, Loss: 0.0020946578588336706\n",
      "Batch 11375/14851, Loss: 0.0017887093126773834\n",
      "Batch 11376/14851, Loss: 0.006510946899652481\n",
      "Batch 11377/14851, Loss: 0.007224652916193008\n",
      "Batch 11378/14851, Loss: 0.059379249811172485\n",
      "Batch 11379/14851, Loss: 0.0007281911675818264\n",
      "Batch 11380/14851, Loss: 0.0003395547973923385\n",
      "Batch 11381/14851, Loss: 0.004476855043321848\n",
      "Batch 11382/14851, Loss: 0.004901323467493057\n",
      "Batch 11383/14851, Loss: 0.01607007347047329\n",
      "Batch 11384/14851, Loss: 0.004107584711164236\n",
      "Batch 11385/14851, Loss: 2.6660660296329297e-05\n",
      "Batch 11386/14851, Loss: 0.007199315819889307\n",
      "Batch 11387/14851, Loss: 0.0015129236271604896\n",
      "Batch 11388/14851, Loss: 0.0007865913212299347\n",
      "Batch 11389/14851, Loss: 0.0030043136794120073\n",
      "Batch 11390/14851, Loss: 0.0017830805154517293\n",
      "Batch 11391/14851, Loss: 7.938536145957187e-05\n",
      "Batch 11392/14851, Loss: 0.007490830030292273\n",
      "Batch 11393/14851, Loss: 0.03436851501464844\n",
      "Batch 11394/14851, Loss: 0.0007405654760077596\n",
      "Batch 11395/14851, Loss: 0.00020950628095306456\n",
      "Batch 11396/14851, Loss: 0.005482697859406471\n",
      "Batch 11397/14851, Loss: 0.05723006650805473\n",
      "Batch 11398/14851, Loss: 0.05357343330979347\n",
      "Batch 11399/14851, Loss: 0.00043460974120534956\n",
      "Batch 11400/14851, Loss: 0.006981464568525553\n",
      "Batch 11401/14851, Loss: 0.0025857752189040184\n",
      "Batch 11402/14851, Loss: 0.0004873722791671753\n",
      "Batch 11403/14851, Loss: 0.0029379345942288637\n",
      "Batch 11404/14851, Loss: 0.0003812859358731657\n",
      "Batch 11405/14851, Loss: 0.00037611700827255845\n",
      "Batch 11406/14851, Loss: 0.0017497017979621887\n",
      "Batch 11407/14851, Loss: 0.0007991741294972599\n",
      "Batch 11408/14851, Loss: 0.00010663270950317383\n",
      "Batch 11409/14851, Loss: 0.0018444968154653907\n",
      "Batch 11410/14851, Loss: 0.001293251640163362\n",
      "Batch 11411/14851, Loss: 0.0015244384994730353\n",
      "Batch 11412/14851, Loss: 0.00648634834215045\n",
      "Batch 11413/14851, Loss: 0.041738931089639664\n",
      "Batch 11414/14851, Loss: 0.009803836233913898\n",
      "Batch 11415/14851, Loss: 0.00014972563076298684\n",
      "Batch 11416/14851, Loss: 0.0002143209130736068\n",
      "Batch 11417/14851, Loss: 0.0008118085097521544\n",
      "Batch 11418/14851, Loss: 0.031727567315101624\n",
      "Batch 11419/14851, Loss: 0.0022806625347584486\n",
      "Batch 11420/14851, Loss: 0.015689827501773834\n",
      "Batch 11421/14851, Loss: 0.0014154886594042182\n",
      "Batch 11422/14851, Loss: 0.0026886560954153538\n",
      "Batch 11423/14851, Loss: 0.0003605013189371675\n",
      "Batch 11424/14851, Loss: 0.003455987898632884\n",
      "Batch 11425/14851, Loss: 0.011274830438196659\n",
      "Batch 11426/14851, Loss: 0.0007072979933582246\n",
      "Batch 11427/14851, Loss: 0.0009154503350146115\n",
      "Batch 11428/14851, Loss: 0.0006297191139310598\n",
      "Batch 11429/14851, Loss: 0.013287000358104706\n",
      "Batch 11430/14851, Loss: 0.0010582891991361976\n",
      "Batch 11431/14851, Loss: 0.0029102303087711334\n",
      "Batch 11432/14851, Loss: 0.0019282688153907657\n",
      "Batch 11433/14851, Loss: 0.05025201290845871\n",
      "Batch 11434/14851, Loss: 5.3160842071520165e-05\n",
      "Batch 11435/14851, Loss: 0.003931613638997078\n",
      "Batch 11436/14851, Loss: 0.0011575421085581183\n",
      "Batch 11437/14851, Loss: 7.429346442222595e-05\n",
      "Batch 11438/14851, Loss: 0.05182430520653725\n",
      "Batch 11439/14851, Loss: 0.007030439097434282\n",
      "Batch 11440/14851, Loss: 0.016340356320142746\n",
      "Batch 11441/14851, Loss: 0.029593797400593758\n",
      "Batch 11442/14851, Loss: 0.0004307888448238373\n",
      "Batch 11443/14851, Loss: 0.0014783305814489722\n",
      "Batch 11444/14851, Loss: 0.021559521555900574\n",
      "Batch 11445/14851, Loss: 0.05441971495747566\n",
      "Batch 11446/14851, Loss: 0.0002707901003304869\n",
      "Batch 11447/14851, Loss: 0.0007750826771371067\n",
      "Batch 11448/14851, Loss: 0.0023640666622668505\n",
      "Batch 11449/14851, Loss: 0.005033521447330713\n",
      "Batch 11450/14851, Loss: 0.0003968315722886473\n",
      "Batch 11451/14851, Loss: 0.008988278917968273\n",
      "Batch 11452/14851, Loss: 0.001241567195393145\n",
      "Batch 11453/14851, Loss: 0.0006673907046206295\n",
      "Batch 11454/14851, Loss: 0.002533210441470146\n",
      "Batch 11455/14851, Loss: 0.0008739034528844059\n",
      "Batch 11456/14851, Loss: 2.7686357498168945e-05\n",
      "Batch 11457/14851, Loss: 0.004128977656364441\n",
      "Batch 11458/14851, Loss: 0.002242499263957143\n",
      "Batch 11459/14851, Loss: 0.020781973376870155\n",
      "Batch 11460/14851, Loss: 0.0013241121778264642\n",
      "Batch 11461/14851, Loss: 0.0020816654432564974\n",
      "Batch 11462/14851, Loss: 0.021573588252067566\n",
      "Batch 11463/14851, Loss: 0.0012275666231289506\n",
      "Batch 11464/14851, Loss: 0.002270471304655075\n",
      "Batch 11465/14851, Loss: 0.027663199231028557\n",
      "Batch 11466/14851, Loss: 5.2136678277747706e-05\n",
      "Batch 11467/14851, Loss: 0.0005266914959065616\n",
      "Batch 11468/14851, Loss: 0.0027179846074432135\n",
      "Batch 11469/14851, Loss: 0.00044995546340942383\n",
      "Batch 11470/14851, Loss: 0.004399733152240515\n",
      "Batch 11471/14851, Loss: 0.00019468566461000592\n",
      "Batch 11472/14851, Loss: 0.05678044259548187\n",
      "Batch 11473/14851, Loss: 0.000363718718290329\n",
      "Batch 11474/14851, Loss: 0.0011580610880628228\n",
      "Batch 11475/14851, Loss: 0.0027771764434874058\n",
      "Batch 11476/14851, Loss: 0.00023723406775388867\n",
      "Batch 11477/14851, Loss: 0.00142213876824826\n",
      "Batch 11478/14851, Loss: 0.0010681162821128964\n",
      "Batch 11479/14851, Loss: 0.024854587391018867\n",
      "Batch 11480/14851, Loss: 0.0861063227057457\n",
      "Batch 11481/14851, Loss: 0.0009950449457392097\n",
      "Batch 11482/14851, Loss: 0.02206060104072094\n",
      "Batch 11483/14851, Loss: 0.0006267845747061074\n",
      "Batch 11484/14851, Loss: 9.745856368681416e-05\n",
      "Batch 11485/14851, Loss: 0.0007706930045969784\n",
      "Batch 11486/14851, Loss: 0.0074172853492200375\n",
      "Batch 11487/14851, Loss: 0.001375754363834858\n",
      "Batch 11488/14851, Loss: 0.011774846352636814\n",
      "Batch 11489/14851, Loss: 0.0003948236408177763\n",
      "Batch 11490/14851, Loss: 0.001975368708372116\n",
      "Batch 11491/14851, Loss: 0.0001683851151028648\n",
      "Batch 11492/14851, Loss: 0.03071398288011551\n",
      "Batch 11493/14851, Loss: 0.002155354944989085\n",
      "Batch 11494/14851, Loss: 0.01862652227282524\n",
      "Batch 11495/14851, Loss: 0.018114516511559486\n",
      "Batch 11496/14851, Loss: 0.019689124077558517\n",
      "Batch 11497/14851, Loss: 0.011035420000553131\n",
      "Batch 11498/14851, Loss: 0.0003800659906119108\n",
      "Batch 11499/14851, Loss: 0.0019206717843189836\n",
      "Batch 11500/14851, Loss: 0.003973762039095163\n",
      "Batch 11501/14851, Loss: 0.0004118730721529573\n",
      "Batch 11502/14851, Loss: 0.04144221916794777\n",
      "Batch 11503/14851, Loss: 0.0033369334414601326\n",
      "Batch 11504/14851, Loss: 0.0006173414294607937\n",
      "Batch 11505/14851, Loss: 0.06398498266935349\n",
      "Batch 11506/14851, Loss: 0.0005415852065198123\n",
      "Batch 11507/14851, Loss: 0.0016491987043991685\n",
      "Batch 11508/14851, Loss: 0.0037714592181146145\n",
      "Batch 11509/14851, Loss: 0.0013587562134489417\n",
      "Batch 11510/14851, Loss: 0.0011219966690987349\n",
      "Batch 11511/14851, Loss: 0.014025508426129818\n",
      "Batch 11512/14851, Loss: 0.034778643399477005\n",
      "Batch 11513/14851, Loss: 0.0029204990714788437\n",
      "Batch 11514/14851, Loss: 0.027445169165730476\n",
      "Batch 11515/14851, Loss: 0.00022339721908792853\n",
      "Batch 11516/14851, Loss: 0.0007050025160424411\n",
      "Batch 11517/14851, Loss: 0.04225214570760727\n",
      "Batch 11518/14851, Loss: 0.0011083161225542426\n",
      "Batch 11519/14851, Loss: 0.0010425448417663574\n",
      "Batch 11520/14851, Loss: 0.03359982371330261\n",
      "Batch 11521/14851, Loss: 0.0014225095510482788\n",
      "Batch 11522/14851, Loss: 0.008499226532876492\n",
      "Batch 11523/14851, Loss: 0.0009362415876239538\n",
      "Batch 11524/14851, Loss: 0.009061400778591633\n",
      "Batch 11525/14851, Loss: 0.00031924867653287947\n",
      "Batch 11526/14851, Loss: 0.007657965645194054\n",
      "Batch 11527/14851, Loss: 0.0006246647681109607\n",
      "Batch 11528/14851, Loss: 0.0019627686124294996\n",
      "Batch 11529/14851, Loss: 0.0017418389907106757\n",
      "Batch 11530/14851, Loss: 0.00303089851513505\n",
      "Batch 11531/14851, Loss: 0.0029393271543085575\n",
      "Batch 11532/14851, Loss: 0.008065793663263321\n",
      "Batch 11533/14851, Loss: 0.003320256946608424\n",
      "Batch 11534/14851, Loss: 0.050876226276159286\n",
      "Batch 11535/14851, Loss: 0.02880716696381569\n",
      "Batch 11536/14851, Loss: 0.0006099917809478939\n",
      "Batch 11537/14851, Loss: 0.0017082629492506385\n",
      "Batch 11538/14851, Loss: 0.0037929986137896776\n",
      "Batch 11539/14851, Loss: 0.05625353753566742\n",
      "Batch 11540/14851, Loss: 9.269340080209076e-05\n",
      "Batch 11541/14851, Loss: 0.0005848594009876251\n",
      "Batch 11542/14851, Loss: 0.021323787048459053\n",
      "Batch 11543/14851, Loss: 0.0017050852766260505\n",
      "Batch 11544/14851, Loss: 0.018757114186882973\n",
      "Batch 11545/14851, Loss: 0.0012166175292804837\n",
      "Batch 11546/14851, Loss: 0.02353682368993759\n",
      "Batch 11547/14851, Loss: 0.0008999395067803562\n",
      "Batch 11548/14851, Loss: 0.03508308529853821\n",
      "Batch 11549/14851, Loss: 0.043127380311489105\n",
      "Batch 11550/14851, Loss: 0.00047576925135217607\n",
      "Batch 11551/14851, Loss: 0.0006809781189076602\n",
      "Batch 11552/14851, Loss: 0.019719529896974564\n",
      "Batch 11553/14851, Loss: 0.00021720056247431785\n",
      "Batch 11554/14851, Loss: 0.09131962060928345\n",
      "Batch 11555/14851, Loss: 0.0003787043096963316\n",
      "Batch 11556/14851, Loss: 0.0020579223055392504\n",
      "Batch 11557/14851, Loss: 0.003836162621155381\n",
      "Batch 11558/14851, Loss: 0.01236399170011282\n",
      "Batch 11559/14851, Loss: 0.002482111332938075\n",
      "Batch 11560/14851, Loss: 0.001364040421321988\n",
      "Batch 11561/14851, Loss: 0.002897123573347926\n",
      "Batch 11562/14851, Loss: 0.00932678859680891\n",
      "Batch 11563/14851, Loss: 0.02430851012468338\n",
      "Batch 11564/14851, Loss: 0.018887562677264214\n",
      "Batch 11565/14851, Loss: 0.0021359745878726244\n",
      "Batch 11566/14851, Loss: 0.006751962937414646\n",
      "Batch 11567/14851, Loss: 0.004329999443143606\n",
      "Batch 11568/14851, Loss: 0.0025255854707211256\n",
      "Batch 11569/14851, Loss: 0.04237072169780731\n",
      "Batch 11570/14851, Loss: 0.014737785793840885\n",
      "Batch 11571/14851, Loss: 0.003751860000193119\n",
      "Batch 11572/14851, Loss: 0.04982299730181694\n",
      "Batch 11573/14851, Loss: 0.01686684601008892\n",
      "Batch 11574/14851, Loss: 0.000688621134031564\n",
      "Batch 11575/14851, Loss: 0.0006044730544090271\n",
      "Batch 11576/14851, Loss: 0.07452013343572617\n",
      "Batch 11577/14851, Loss: 0.0038243376184254885\n",
      "Batch 11578/14851, Loss: 0.003467884846031666\n",
      "Batch 11579/14851, Loss: 0.038534898310899734\n",
      "Batch 11580/14851, Loss: 0.0004822822811547667\n",
      "Batch 11581/14851, Loss: 0.01054193265736103\n",
      "Batch 11582/14851, Loss: 0.00044599300599657\n",
      "Batch 11583/14851, Loss: 0.007013670168817043\n",
      "Batch 11584/14851, Loss: 0.021448055282235146\n",
      "Batch 11585/14851, Loss: 0.0014800442149862647\n",
      "Batch 11586/14851, Loss: 0.005381992552429438\n",
      "Batch 11587/14851, Loss: 0.003695125225931406\n",
      "Batch 11588/14851, Loss: 0.003265363397076726\n",
      "Batch 11589/14851, Loss: 0.01513499952852726\n",
      "Batch 11590/14851, Loss: 0.012075185775756836\n",
      "Batch 11591/14851, Loss: 0.017381029203534126\n",
      "Batch 11592/14851, Loss: 0.004372571129351854\n",
      "Batch 11593/14851, Loss: 0.01905207149684429\n",
      "Batch 11594/14851, Loss: 0.0010320419678464532\n",
      "Batch 11595/14851, Loss: 0.00493852561339736\n",
      "Batch 11596/14851, Loss: 0.014572474174201488\n",
      "Batch 11597/14851, Loss: 0.01460938435047865\n",
      "Batch 11598/14851, Loss: 0.019073065370321274\n",
      "Batch 11599/14851, Loss: 0.00825225468724966\n",
      "Batch 11600/14851, Loss: 0.008875993080437183\n",
      "Batch 11601/14851, Loss: 0.0021179430186748505\n",
      "Batch 11602/14851, Loss: 0.010430320166051388\n",
      "Batch 11603/14851, Loss: 0.003846827894449234\n",
      "Batch 11604/14851, Loss: 0.03497584909200668\n",
      "Batch 11605/14851, Loss: 0.0015212256694212556\n",
      "Batch 11606/14851, Loss: 0.0018165111541748047\n",
      "Batch 11607/14851, Loss: 0.028153112158179283\n",
      "Batch 11608/14851, Loss: 0.0038188055623322725\n",
      "Batch 11609/14851, Loss: 0.00036544352769851685\n",
      "Batch 11610/14851, Loss: 0.0004574395716190338\n",
      "Batch 11611/14851, Loss: 0.005264017730951309\n",
      "Batch 11612/14851, Loss: 0.003123857779428363\n",
      "Batch 11613/14851, Loss: 0.00792156532406807\n",
      "Batch 11614/14851, Loss: 0.0015035649994388223\n",
      "Batch 11615/14851, Loss: 0.0009605498635210097\n",
      "Batch 11616/14851, Loss: 0.03735887259244919\n",
      "Batch 11617/14851, Loss: 0.007510934956371784\n",
      "Batch 11618/14851, Loss: 0.0006512254476547241\n",
      "Batch 11619/14851, Loss: 0.000436258822446689\n",
      "Batch 11620/14851, Loss: 0.003127514151856303\n",
      "Batch 11621/14851, Loss: 0.014878802932798862\n",
      "Batch 11622/14851, Loss: 0.003875798312947154\n",
      "Batch 11623/14851, Loss: 0.0013931147987022996\n",
      "Batch 11624/14851, Loss: 0.0030021248385310173\n",
      "Batch 11625/14851, Loss: 0.004884738475084305\n",
      "Batch 11626/14851, Loss: 0.008831270970404148\n",
      "Batch 11627/14851, Loss: 0.022265540435910225\n",
      "Batch 11628/14851, Loss: 0.03464759141206741\n",
      "Batch 11629/14851, Loss: 0.0010978529462590814\n",
      "Batch 11630/14851, Loss: 0.0040482464246451855\n",
      "Batch 11631/14851, Loss: 0.05994797497987747\n",
      "Batch 11632/14851, Loss: 0.0009736220235936344\n",
      "Batch 11633/14851, Loss: 0.011130423285067081\n",
      "Batch 11634/14851, Loss: 0.009347888641059399\n",
      "Batch 11635/14851, Loss: 0.0025051417760550976\n",
      "Batch 11636/14851, Loss: 0.0005666539072990417\n",
      "Batch 11637/14851, Loss: 0.0028834801632910967\n",
      "Batch 11638/14851, Loss: 0.006932130549103022\n",
      "Batch 11639/14851, Loss: 0.0022082068026065826\n",
      "Batch 11640/14851, Loss: 0.006246224045753479\n",
      "Batch 11641/14851, Loss: 0.0024875986855477095\n",
      "Batch 11642/14851, Loss: 0.0010910158744081855\n",
      "Batch 11643/14851, Loss: 0.005526683293282986\n",
      "Batch 11644/14851, Loss: 0.0038714047987014055\n",
      "Batch 11645/14851, Loss: 0.01301930844783783\n",
      "Batch 11646/14851, Loss: 0.031011834740638733\n",
      "Batch 11647/14851, Loss: 0.001972764730453491\n",
      "Batch 11648/14851, Loss: 0.01699303835630417\n",
      "Batch 11649/14851, Loss: 0.00011704365169862285\n",
      "Batch 11650/14851, Loss: 0.0014633905375376344\n",
      "Batch 11651/14851, Loss: 0.004749990999698639\n",
      "Batch 11652/14851, Loss: 0.01522558368742466\n",
      "Batch 11653/14851, Loss: 0.0031386788468807936\n",
      "Batch 11654/14851, Loss: 0.013415209017693996\n",
      "Batch 11655/14851, Loss: 0.022543402388691902\n",
      "Batch 11656/14851, Loss: 0.0030634913127869368\n",
      "Batch 11657/14851, Loss: 0.013573170639574528\n",
      "Batch 11658/14851, Loss: 0.00022313867521006614\n",
      "Batch 11659/14851, Loss: 0.0010103607783094049\n",
      "Batch 11660/14851, Loss: 0.003763442160561681\n",
      "Batch 11661/14851, Loss: 0.00709774112328887\n",
      "Batch 11662/14851, Loss: 0.0001610654144315049\n",
      "Batch 11663/14851, Loss: 8.628641808172688e-05\n",
      "Batch 11664/14851, Loss: 0.008544056676328182\n",
      "Batch 11665/14851, Loss: 0.0007547798450104892\n",
      "Batch 11666/14851, Loss: 0.015188004821538925\n",
      "Batch 11667/14851, Loss: 0.0768512710928917\n",
      "Batch 11668/14851, Loss: 0.0016461759805679321\n",
      "Batch 11669/14851, Loss: 0.010437852703034878\n",
      "Batch 11670/14851, Loss: 0.00026983642601408064\n",
      "Batch 11671/14851, Loss: 0.00011050825560232624\n",
      "Batch 11672/14851, Loss: 0.01052834838628769\n",
      "Batch 11673/14851, Loss: 0.004885369446128607\n",
      "Batch 11674/14851, Loss: 0.0002461634576320648\n",
      "Batch 11675/14851, Loss: 0.005631280597299337\n",
      "Batch 11676/14851, Loss: 0.0028377994894981384\n",
      "Batch 11677/14851, Loss: 0.047696903347969055\n",
      "Batch 11678/14851, Loss: 0.00011802464723587036\n",
      "Batch 11679/14851, Loss: 0.0336838997900486\n",
      "Batch 11680/14851, Loss: 0.0002503464638721198\n",
      "Batch 11681/14851, Loss: 0.001815957366488874\n",
      "Batch 11682/14851, Loss: 0.0012621989008039236\n",
      "Batch 11683/14851, Loss: 0.011540909297764301\n",
      "Batch 11684/14851, Loss: 0.003073684172704816\n",
      "Batch 11685/14851, Loss: 0.00022531177091877908\n",
      "Batch 11686/14851, Loss: 0.0001857938914326951\n",
      "Batch 11687/14851, Loss: 0.008391818031668663\n",
      "Batch 11688/14851, Loss: 0.0015331879258155823\n",
      "Batch 11689/14851, Loss: 0.0007014122675172985\n",
      "Batch 11690/14851, Loss: 0.033323243260383606\n",
      "Batch 11691/14851, Loss: 0.02290494367480278\n",
      "Batch 11692/14851, Loss: 0.008819719776511192\n",
      "Batch 11693/14851, Loss: 0.00039368742727674544\n",
      "Batch 11694/14851, Loss: 0.002502244431525469\n",
      "Batch 11695/14851, Loss: 0.00046522170305252075\n",
      "Batch 11696/14851, Loss: 0.0026964545249938965\n",
      "Batch 11697/14851, Loss: 0.011002430692315102\n",
      "Batch 11698/14851, Loss: 0.00018239766359329224\n",
      "Batch 11699/14851, Loss: 0.0005487265880219638\n",
      "Batch 11700/14851, Loss: 0.0008016924257390201\n",
      "Batch 11701/14851, Loss: 0.03327730670571327\n",
      "Batch 11702/14851, Loss: 0.024467824026942253\n",
      "Batch 11703/14851, Loss: 0.011083840392529964\n",
      "Batch 11704/14851, Loss: 0.0012152442941442132\n",
      "Batch 11705/14851, Loss: 0.050620026886463165\n",
      "Batch 11706/14851, Loss: 0.005524075124412775\n",
      "Batch 11707/14851, Loss: 0.01650198921561241\n",
      "Batch 11708/14851, Loss: 2.2359192371368408e-05\n",
      "Batch 11709/14851, Loss: 0.04317491501569748\n",
      "Batch 11710/14851, Loss: 4.918500781059265e-05\n",
      "Batch 11711/14851, Loss: 0.010816240683197975\n",
      "Batch 11712/14851, Loss: 0.00024359913368243724\n",
      "Batch 11713/14851, Loss: 0.004934148397296667\n",
      "Batch 11714/14851, Loss: 0.015173121355473995\n",
      "Batch 11715/14851, Loss: 0.0035430167336016893\n",
      "Batch 11716/14851, Loss: 0.03154706209897995\n",
      "Batch 11717/14851, Loss: 0.0008090685005299747\n",
      "Batch 11718/14851, Loss: 0.004107922315597534\n",
      "Batch 11719/14851, Loss: 0.0023001867812126875\n",
      "Batch 11720/14851, Loss: 0.007114332169294357\n",
      "Batch 11721/14851, Loss: 0.01621878892183304\n",
      "Batch 11722/14851, Loss: 0.018915705382823944\n",
      "Batch 11723/14851, Loss: 0.0004207479360047728\n",
      "Batch 11724/14851, Loss: 0.0004587682487908751\n",
      "Batch 11725/14851, Loss: 5.8983761846320704e-05\n",
      "Batch 11726/14851, Loss: 0.00020040199160575867\n",
      "Batch 11727/14851, Loss: 4.894286394119263e-05\n",
      "Batch 11728/14851, Loss: 0.005462215282022953\n",
      "Batch 11729/14851, Loss: 8.221840107580647e-05\n",
      "Batch 11730/14851, Loss: 0.019127728417515755\n",
      "Batch 11731/14851, Loss: 0.009227953851222992\n",
      "Batch 11732/14851, Loss: 0.011202441528439522\n",
      "Batch 11733/14851, Loss: 0.02408468909561634\n",
      "Batch 11734/14851, Loss: 0.007189398165792227\n",
      "Batch 11735/14851, Loss: 0.01449498999863863\n",
      "Batch 11736/14851, Loss: 0.01397350151091814\n",
      "Batch 11737/14851, Loss: 0.007898525334894657\n",
      "Batch 11738/14851, Loss: 0.006455517839640379\n",
      "Batch 11739/14851, Loss: 0.0028286229353398085\n",
      "Batch 11740/14851, Loss: 0.00017523269343655556\n",
      "Batch 11741/14851, Loss: 0.018498621881008148\n",
      "Batch 11742/14851, Loss: 0.0013403197517618537\n",
      "Batch 11743/14851, Loss: 0.00015819880354683846\n",
      "Batch 11744/14851, Loss: 0.0009497019345872104\n",
      "Batch 11745/14851, Loss: 0.0005723622743971646\n",
      "Batch 11746/14851, Loss: 0.009617572650313377\n",
      "Batch 11747/14851, Loss: 0.029131455346941948\n",
      "Batch 11748/14851, Loss: 0.024707358330488205\n",
      "Batch 11749/14851, Loss: 0.03622046858072281\n",
      "Batch 11750/14851, Loss: 0.00023126229643821716\n",
      "Batch 11751/14851, Loss: 0.007708633318543434\n",
      "Batch 11752/14851, Loss: 0.001947060227394104\n",
      "Batch 11753/14851, Loss: 0.0005237427540123463\n",
      "Batch 11754/14851, Loss: 0.005544103216379881\n",
      "Batch 11755/14851, Loss: 0.000291538453893736\n",
      "Batch 11756/14851, Loss: 0.0003583405923563987\n",
      "Batch 11757/14851, Loss: 0.034198883920907974\n",
      "Batch 11758/14851, Loss: 0.004665117710828781\n",
      "Batch 11759/14851, Loss: 0.003821066115051508\n",
      "Batch 11760/14851, Loss: 0.028041942045092583\n",
      "Batch 11761/14851, Loss: 0.0038936417549848557\n",
      "Batch 11762/14851, Loss: 0.0012509512016549706\n",
      "Batch 11763/14851, Loss: 0.0064828600734472275\n",
      "Batch 11764/14851, Loss: 0.001551323919557035\n",
      "Batch 11765/14851, Loss: 0.01499668974429369\n",
      "Batch 11766/14851, Loss: 0.02925431728363037\n",
      "Batch 11767/14851, Loss: 0.0008046887814998627\n",
      "Batch 11768/14851, Loss: 0.01397415529936552\n",
      "Batch 11769/14851, Loss: 8.344416710315272e-05\n",
      "Batch 11770/14851, Loss: 0.00231319572776556\n",
      "Batch 11771/14851, Loss: 0.001914752065204084\n",
      "Batch 11772/14851, Loss: 0.0009581335471011698\n",
      "Batch 11773/14851, Loss: 0.011827963404357433\n",
      "Batch 11774/14851, Loss: 0.0021456461399793625\n",
      "Batch 11775/14851, Loss: 0.011001556180417538\n",
      "Batch 11776/14851, Loss: 0.11178697645664215\n",
      "Batch 11777/14851, Loss: 0.0003232319431845099\n",
      "Batch 11778/14851, Loss: 0.011464911513030529\n",
      "Batch 11779/14851, Loss: 0.0015536422142758965\n",
      "Batch 11780/14851, Loss: 0.0008736364543437958\n",
      "Batch 11781/14851, Loss: 0.0227145254611969\n",
      "Batch 11782/14851, Loss: 0.016960639506578445\n",
      "Batch 11783/14851, Loss: 0.0036094936076551676\n",
      "Batch 11784/14851, Loss: 0.008010541088879108\n",
      "Batch 11785/14851, Loss: 0.0014533748617395759\n",
      "Batch 11786/14851, Loss: 0.00021258245396893471\n",
      "Batch 11787/14851, Loss: 0.02300352230668068\n",
      "Batch 11788/14851, Loss: 0.004596064332872629\n",
      "Batch 11789/14851, Loss: 0.0007997353677637875\n",
      "Batch 11790/14851, Loss: 0.0004057516634929925\n",
      "Batch 11791/14851, Loss: 0.001024338067509234\n",
      "Batch 11792/14851, Loss: 0.00915983971208334\n",
      "Batch 11793/14851, Loss: 0.007654009852558374\n",
      "Batch 11794/14851, Loss: 0.0076316422782838345\n",
      "Batch 11795/14851, Loss: 0.0013423952041193843\n",
      "Batch 11796/14851, Loss: 0.026072869077324867\n",
      "Batch 11797/14851, Loss: 0.007127104792743921\n",
      "Batch 11798/14851, Loss: 0.011066574603319168\n",
      "Batch 11799/14851, Loss: 0.008628223091363907\n",
      "Batch 11800/14851, Loss: 0.004885135684162378\n",
      "Batch 11801/14851, Loss: 0.0025487542152404785\n",
      "Batch 11802/14851, Loss: 0.00021344018750824034\n",
      "Batch 11803/14851, Loss: 0.0001389831304550171\n",
      "Batch 11804/14851, Loss: 0.009519146755337715\n",
      "Batch 11805/14851, Loss: 0.009529505856335163\n",
      "Batch 11806/14851, Loss: 0.006330137141048908\n",
      "Batch 11807/14851, Loss: 0.039365172386169434\n",
      "Batch 11808/14851, Loss: 0.000662313133943826\n",
      "Batch 11809/14851, Loss: 0.0010348893702030182\n",
      "Batch 11810/14851, Loss: 0.005433531943708658\n",
      "Batch 11811/14851, Loss: 0.02956044301390648\n",
      "Batch 11812/14851, Loss: 0.03570176661014557\n",
      "Batch 11813/14851, Loss: 0.004082662519067526\n",
      "Batch 11814/14851, Loss: 0.021218786016106606\n",
      "Batch 11815/14851, Loss: 0.000382348895072937\n",
      "Batch 11816/14851, Loss: 0.036146800965070724\n",
      "Batch 11817/14851, Loss: 0.023621978238224983\n",
      "Batch 11818/14851, Loss: 0.006907584611326456\n",
      "Batch 11819/14851, Loss: 6.943941116333008e-05\n",
      "Batch 11820/14851, Loss: 0.014136314392089844\n",
      "Batch 11821/14851, Loss: 0.023961607366800308\n",
      "Batch 11822/14851, Loss: 0.001192087889648974\n",
      "Batch 11823/14851, Loss: 0.04328696429729462\n",
      "Batch 11824/14851, Loss: 0.006822200957685709\n",
      "Batch 11825/14851, Loss: 0.058065205812454224\n",
      "Batch 11826/14851, Loss: 0.011660180985927582\n",
      "Batch 11827/14851, Loss: 0.005177063401788473\n",
      "Batch 11828/14851, Loss: 0.00022421032190322876\n",
      "Batch 11829/14851, Loss: 0.0029186883475631475\n",
      "Batch 11830/14851, Loss: 0.0869247317314148\n",
      "Batch 11831/14851, Loss: 0.04052115976810455\n",
      "Batch 11832/14851, Loss: 0.03812122344970703\n",
      "Batch 11833/14851, Loss: 0.028403034433722496\n",
      "Batch 11834/14851, Loss: 0.00011876846110681072\n",
      "Batch 11835/14851, Loss: 0.022495649755001068\n",
      "Batch 11836/14851, Loss: 0.00913544837385416\n",
      "Batch 11837/14851, Loss: 8.881961548468098e-05\n",
      "Batch 11838/14851, Loss: 0.00010314086830476299\n",
      "Batch 11839/14851, Loss: 0.0002651450631674379\n",
      "Batch 11840/14851, Loss: 0.02993401326239109\n",
      "Batch 11841/14851, Loss: 0.0006397771066986024\n",
      "Batch 11842/14851, Loss: 0.0011831633746623993\n",
      "Batch 11843/14851, Loss: 0.04131337255239487\n",
      "Batch 11844/14851, Loss: 9.354576468467712e-05\n",
      "Batch 11845/14851, Loss: 0.03728595748543739\n",
      "Batch 11846/14851, Loss: 0.0021537940483540297\n",
      "Batch 11847/14851, Loss: 0.008105190470814705\n",
      "Batch 11848/14851, Loss: 0.0036519430577754974\n",
      "Batch 11849/14851, Loss: 0.008763366378843784\n",
      "Batch 11850/14851, Loss: 0.0030706387478858232\n",
      "Batch 11851/14851, Loss: 0.10195573419332504\n",
      "Batch 11852/14851, Loss: 0.004709030967205763\n",
      "Batch 11853/14851, Loss: 0.0022562944795936346\n",
      "Batch 11854/14851, Loss: 0.06078457832336426\n",
      "Batch 11855/14851, Loss: 0.0038361859042197466\n",
      "Batch 11856/14851, Loss: 0.03090529330074787\n",
      "Batch 11857/14851, Loss: 0.0001552202447783202\n",
      "Batch 11858/14851, Loss: 0.0013142134994268417\n",
      "Batch 11859/14851, Loss: 0.02710055559873581\n",
      "Batch 11860/14851, Loss: 0.001936390995979309\n",
      "Batch 11861/14851, Loss: 0.0013805069029331207\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "# Define a dataset class (same as before)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Construct input text using all columns except 'field'\n",
    "        input_columns = [col for col in self.data.columns if col != 'field']\n",
    "        input_text = ' '.join([f\"{col}: {row[col]}\" for col in input_columns])  # Example input construction\n",
    "\n",
    "        # Target text is the 'field' column\n",
    "        target_text = row['field']  # Output is the 'field' column\n",
    "\n",
    "        # Tokenize inputs and targets\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        targets = self.tokenizer.encode_plus(\n",
    "            target_text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "        labels = targets[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(\"/kaggle/input/bhagawat-training-data-ploy-features/training_data_ploy_features.csv\")\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/kaggle/input/model_4a/transformers/default/1\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/kaggle/input/model_4a/transformers/default/1\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = CustomDataset(tokenizer, data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Same batch size\n",
    "\n",
    "# Set up a new optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)  # Same learning rate as before\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Mixed precision training setup\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Gradient accumulation settings\n",
    "gradient_accumulation_steps = 2\n",
    "accumulated_loss = 0\n",
    "\n",
    "# Continue training for 1 more epoch\n",
    "num_epochs = 1  # One additional epoch\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Ignore padding tokens in the labels\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        with autocast():  # Mixed precision context\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / gradient_accumulation_steps  # Divide by accumulation steps\n",
    "            accumulated_loss += loss.item()\n",
    "\n",
    "        # Check for NaN loss and skip the batch if encountered\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Skipping batch {batch_idx + 1} due to NaN loss\")\n",
    "            continue\n",
    "\n",
    "        # Scale the loss and call backward\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscale the gradients and apply gradient clipping after accumulation\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # Clear the CUDA cache periodically to free up memory\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Batch {batch_idx + 1}/{num_batches}, Loss: {accumulated_loss}\")\n",
    "        accumulated_loss = 0\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1} Average Loss: {avg_loss}\")\n",
    "\n",
    "    # Save the model and optimizer state after this epoch\n",
    "    model.save_pretrained(f\"trained_t5_model_epoch_{epoch + 3}\")  # Save as epoch 3 (continue from previous)\n",
    "    tokenizer.save_pretrained(f\"trained_t5_model_epoch_{epoch + 3}\")\n",
    "    torch.save(optimizer.state_dict(), f\"optimizer_state_epoch_{epoch + 3}.pth\")\n",
    "    print(f\"Model and optimizer state saved after epoch {epoch + 3}\")\n",
    "\n",
    "# Final save after the additional epoch\n",
    "model.save_pretrained(\"trained_t5_model_final_v2\")\n",
    "tokenizer.save_pretrained(\"trained_t5_model_final_v2\")\n",
    "torch.save(optimizer.state_dict(), \"optimizer_state_final_v2.pth\")\n",
    "print(\"Final model and optimizer state saved.\")\n",
    "end = time.time()\n",
    "print(f\"Training time: {end - st} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result of this varient of model is in eval_metrics_after_runing_3_epoch.csv this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T12:13:04.666048Z",
     "iopub.status.busy": "2024-10-21T12:13:04.665655Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Set up timing\n",
    "st = time.time()\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/kaggle/input/final/transformers/default/1\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/kaggle/input/final/transformers/default/1\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(\"/kaggle/input/prediction/data_for_prediction_ploy_features.csv\")\n",
    "\n",
    "# Prepare the input text\n",
    "def prepare_input_text(row, input_columns):\n",
    "    return ' '.join([f\"{col}: {row[col]}\" for col in input_columns])\n",
    "\n",
    "# Batch predictions for efficiency\n",
    "def predict_batch(input_texts, tokenizer, model, max_len=128):\n",
    "    inputs = tokenizer(\n",
    "        input_texts,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=max_len)\n",
    "    \n",
    "    # Decode predictions\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# Define the columns to use for input text\n",
    "input_columns = [col for col in test_data.columns if col != 'field']\n",
    "\n",
    "# Batch size for predictions\n",
    "batch_size = 1024\n",
    "predictions = []\n",
    "\n",
    "for i in range(0, len(test_data), batch_size):\n",
    "    batch_data = test_data.iloc[i:i+batch_size]\n",
    "    input_texts = [prepare_input_text(row, input_columns) for _, row in batch_data.iterrows()]\n",
    "    \n",
    "    # Predict for the current batch\n",
    "    batch_predictions = predict_batch(input_texts, tokenizer, model)\n",
    "    predictions.extend(batch_predictions)\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "test_data['predicted_field'] = predictions\n",
    "\n",
    "# Save the result to a new CSV file (optional)\n",
    "test_data.to_csv(\"test_data_with_ploy_predictions.csv\", index=False)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - st} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due internet disconnect output of cell in this file got erased"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5916618,
     "sourceId": 9680061,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5918891,
     "sourceId": 9682969,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 143558,
     "modelInstanceId": 120346,
     "sourceId": 142066,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
